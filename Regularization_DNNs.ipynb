{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization_DNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOP37F3FDGZhZm4kYDm+B+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Regularization_DNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEAjrg5oj1Qg",
        "colab_type": "text"
      },
      "source": [
        "# Training Deep Neural Networks\n",
        "\n",
        "[Here](https://github.com/victorviro/Deep_learning_python/blob/master/Introduction_artificial_neural_networks_keras.ipynb) we introduced artificial neural networks and trained our first deep neural networks. But they were shallow nets, with just a few hidden layers. What if you need to tackle a complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN, perhaps with 10 layers or many more, each containing hundreds of neurons, linked by hundreds of thousands of connections. Training a deep DNN isn’t a walk in the park. Here are some of the problems you could run into:\n",
        "\n",
        "- You may be faced with the tricky *vanishing gradients* problem or the related *exploding gradients* problem. This is when the gradients grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training. Both of these problems make lower layers very hard to train.\n",
        "\n",
        "- You might not have enough training data for such a large network, or it might be too costly to label.\n",
        "\n",
        "- Training may be extremely slow.\n",
        "\n",
        "- A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances or if they are too noisy.\n",
        "\n",
        "In this notebook we will go through a few popular regularization techniques for large neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0oQT-Zaj1ij",
        "colab_type": "text"
      },
      "source": [
        "## Avoiding overfitting through regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is1Vmxowj1gT",
        "colab_type": "text"
      },
      "source": [
        "Deep neural networks typically have tens of thousands of parameters, sometimes even millions. With so many parameters, the network has an incredible amount of freedom and can fit a huge variety of complex datasets. But this great flexibility also means that it is prone to overfitting the training set. We need regularization.\n",
        "\n",
        "Even though [Batch Normalization](https://github.com/victorviro/Deep_learning_python/blob/master/Vanishing_Exploding_gradients_problem_DNNs.ipynb) was designed to solve the vanishing/exploding gradients problems, is also acts like a pretty good regularizer. In this section we will present other popular regularization techniques for neural networks: $l_1$ and $l_2$ regularization, dropout, max-norm regularization and early stopping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYyysrV_ks5O",
        "colab_type": "text"
      },
      "source": [
        "### $l_1$ and $l_2$ regularization\n",
        "\n",
        "Just like for simple [linear models](https://github.com/victorviro/ML_algorithms_python/blob/master/Introduction_generalized_linear_models.ipynb), you can use $l_1$ and $l_2$ regularization to constrain a neural network’s connection weights (but typically not its biases). Here is how to apply $l_2$ regularization to a Keras layer’s connection weights, using a regularization factor of 0.01:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE-o130Ylouz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f893a11f-c688-440e-85ee-756281f9a0fb"
      },
      "source": [
        "import keras\n",
        "layer = keras.layers.Dense(100, activation=\"elu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhybSzgLj1eM",
        "colab_type": "text"
      },
      "source": [
        "The `l2()` function returns a regularizer that will be called to compute the regularization loss, at each step during training. This regularization loss is then added to the final loss. As you might expect, you can just use `keras.regularizers.l1()` if you want $l_1$ regularization, and if you want both $l_1$ and $l_2$ regularization, use `keras.regularizers.l1_l2()` (specifying both regularization factors).\n",
        "\n",
        "Let's train a classifier for the [MNIST fashion](https://www.kaggle.com/zalando-research/fashionmnist) problem using $l_2$ regularization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VuaDAKmNqUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and prepare data\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split data data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, shuffle= True)\n",
        "\n",
        "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpExYSEtJ0Hw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "de90b1dc-8540-46bf-d153-f29a1001c10d"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"elu\",\n",
        "                       kernel_initializer=\"he_normal\",\n",
        "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dense(100, activation=\"elu\",\n",
        "                       kernel_initializer=\"he_normal\",\n",
        "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dense(10, activation=\"softmax\",\n",
        "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "n_epochs = 2\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/2\n",
            "54000/54000 [==============================] - 8s 142us/step - loss: 1.2988 - accuracy: 0.7975 - val_loss: 0.7143 - val_accuracy: 0.8285\n",
            "Epoch 2/2\n",
            "54000/54000 [==============================] - 6s 107us/step - loss: 0.7434 - accuracy: 0.8189 - val_loss: 0.7403 - val_accuracy: 0.8207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cuR0tjNJnpI",
        "colab_type": "text"
      },
      "source": [
        "Since you will typically want to apply the same regularizer to all layers in your network, as well as the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments over and over. This makes it ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is to use Python’s `functools.partial()` function: it lets you create a thin wrapper for any callable, with some default argument values. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAPXoLGtmPtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "ecf47bb6-fca8-4beb-ca37-7a8380445120"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                           activation=\"elu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "n_epochs = 2\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/2\n",
            "54000/54000 [==============================] - 6s 110us/step - loss: 1.3192 - accuracy: 0.7975 - val_loss: 0.7765 - val_accuracy: 0.8080\n",
            "Epoch 2/2\n",
            "54000/54000 [==============================] - 6s 106us/step - loss: 0.7420 - accuracy: 0.8176 - val_loss: 0.7796 - val_accuracy: 0.7937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E0mrnwpmaeB",
        "colab_type": "text"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP2nr2EYj1b-",
        "colab_type": "text"
      },
      "source": [
        "[Dropout](https://arxiv.org/abs/1207.0580) is one of the most popular regularization techniques for deep neural networks. It was proposed by Geoffrey Hinton in 2012 and further detailed in a [paper](http://jmlr.org/papers/v15/srivastava14a.html) by Nitish Srivastava, and it has proven to be highly successful: even the state-of-the-art neural networks got a 1–2% accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).\n",
        "\n",
        "It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability $p$ of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-9). The hyperparameter $p$ is called the *dropout rate*, and it is typically set to 50%. After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).\n",
        "\n",
        " ![texto alternativo](https://i.ibb.co/TBZSpqV/dropout.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaRk1Cc1j1Z9",
        "colab_type": "text"
      },
      "source": [
        "It is quite surprising at first that this rather brutal technique works at all. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes better.\n",
        "\n",
        "Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of $2^N$ possible networks (where $N$ is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run a $10000$ training steps, you have essentially trained $10000$ different neural networks (each with just one training instance). These neural networks are obviously not independent since they share many of their weights, but they are nevertheless all different. The resulting neural\n",
        "network can be seen as an averaging ensemble of all these smaller neural networks.\n",
        "\n",
        "\n",
        "There is one small but important technical detail. Suppose $p = 50\\%$, in which case during testing a neuron will be connected to twice as many input neurons as it was (on average) during training. To compensate for this fact, we need to multiply each neuron’s input connection weights by $0.5$ after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on, and it is unlikely to perform well. More generally, we need to multiply each input connection weight by the *keep probability* $(1 – p)$ after training. Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well).\n",
        "\n",
        "\n",
        "To implement dropout using Keras, you can use the `keras.layers.Dropout` layer.\n",
        "During training, it randomly drops some inputs (setting them to $0$) and divides the remaining inputs by the keep probability. After training, it does nothing at all, it just passes the inputs to the next layer. For example, the following code applies dropout regularization before every `Dense` layer, using a dropout rate of $0.2$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXqDWQstn9gA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "3102c1c1-e1d5-412a-f519-443a12f30206"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "n_epochs = 2\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/2\n",
            "54000/54000 [==============================] - 6s 116us/step - loss: 0.5784 - accuracy: 0.7987 - val_loss: 0.4098 - val_accuracy: 0.8485\n",
            "Epoch 2/2\n",
            "54000/54000 [==============================] - 6s 115us/step - loss: 0.4542 - accuracy: 0.8347 - val_loss: 0.3986 - val_accuracy: 0.8503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbY07-aZn9pk",
        "colab_type": "text"
      },
      "source": [
        "**Note**: Since dropout is only active during training, the training loss is\n",
        "penalized compared to the validation loss, so comparing the two can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training).\n",
        "\n",
        "If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong.\n",
        "\n",
        "\n",
        "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.\n",
        "\n",
        "**Note**: If you want to regularize a self-normalizing network based on the\n",
        "SELU [activation function](https://github.com/victorviro/Deep_learning_python/blob/master/Vanishing_Exploding_gradients_problem_DNNs.ipynb), you should use `AlphaDropout`: this is a variant of dropout that preserves the mean and standard deviation of its inputs (it was introduced in the same paper as SELU, as regular dropout would break self-normalization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh2V5lmOOCGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "224eb687-3e24-457d-db52-08b632c3410a"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.AlphaDropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.AlphaDropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.AlphaDropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "n_epochs = 20\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/20\n",
            "54000/54000 [==============================] - 5s 97us/step - loss: 0.6697 - accuracy: 0.7566 - val_loss: 0.5752 - val_accuracy: 0.8375\n",
            "Epoch 2/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.5587 - accuracy: 0.7945 - val_loss: 0.5864 - val_accuracy: 0.8395\n",
            "Epoch 3/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.5288 - accuracy: 0.8041 - val_loss: 0.5363 - val_accuracy: 0.8420\n",
            "Epoch 4/20\n",
            "54000/54000 [==============================] - 5s 96us/step - loss: 0.5084 - accuracy: 0.8115 - val_loss: 0.4493 - val_accuracy: 0.8628\n",
            "Epoch 5/20\n",
            "54000/54000 [==============================] - 5s 96us/step - loss: 0.4925 - accuracy: 0.8174 - val_loss: 0.4339 - val_accuracy: 0.8693\n",
            "Epoch 6/20\n",
            "54000/54000 [==============================] - 5s 96us/step - loss: 0.4843 - accuracy: 0.8196 - val_loss: 0.4144 - val_accuracy: 0.8688\n",
            "Epoch 7/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.4749 - accuracy: 0.8238 - val_loss: 0.4485 - val_accuracy: 0.8615\n",
            "Epoch 8/20\n",
            "54000/54000 [==============================] - 5s 97us/step - loss: 0.4692 - accuracy: 0.8269 - val_loss: 0.3983 - val_accuracy: 0.8685\n",
            "Epoch 9/20\n",
            "54000/54000 [==============================] - 5s 96us/step - loss: 0.4636 - accuracy: 0.8281 - val_loss: 0.4527 - val_accuracy: 0.8613\n",
            "Epoch 10/20\n",
            "54000/54000 [==============================] - 5s 98us/step - loss: 0.4595 - accuracy: 0.8307 - val_loss: 0.3945 - val_accuracy: 0.8690\n",
            "Epoch 11/20\n",
            "54000/54000 [==============================] - 5s 96us/step - loss: 0.4522 - accuracy: 0.8325 - val_loss: 0.4430 - val_accuracy: 0.8663\n",
            "Epoch 12/20\n",
            "54000/54000 [==============================] - 5s 96us/step - loss: 0.4482 - accuracy: 0.8351 - val_loss: 0.4117 - val_accuracy: 0.8727\n",
            "Epoch 13/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.4421 - accuracy: 0.8369 - val_loss: 0.4419 - val_accuracy: 0.8665\n",
            "Epoch 14/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.4358 - accuracy: 0.8374 - val_loss: 0.4217 - val_accuracy: 0.8690\n",
            "Epoch 15/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.4383 - accuracy: 0.8387 - val_loss: 0.4124 - val_accuracy: 0.8767\n",
            "Epoch 16/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.4296 - accuracy: 0.8405 - val_loss: 0.4123 - val_accuracy: 0.8765\n",
            "Epoch 17/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.4318 - accuracy: 0.8402 - val_loss: 0.3874 - val_accuracy: 0.8740\n",
            "Epoch 18/20\n",
            "54000/54000 [==============================] - 5s 94us/step - loss: 0.4247 - accuracy: 0.8421 - val_loss: 0.3928 - val_accuracy: 0.8688\n",
            "Epoch 19/20\n",
            "54000/54000 [==============================] - 5s 94us/step - loss: 0.4286 - accuracy: 0.8413 - val_loss: 0.3832 - val_accuracy: 0.8850\n",
            "Epoch 20/20\n",
            "54000/54000 [==============================] - 5s 95us/step - loss: 0.4216 - accuracy: 0.8440 - val_loss: 0.3990 - val_accuracy: 0.8740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTw4APrhOhac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "afbb47a1-8507-492d-82bd-c759b51c5aac"
      },
      "source": [
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 52us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4830064167499542, 0.8601999878883362]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVhN28aPoMDN",
        "colab_type": "text"
      },
      "source": [
        "### Max-Norm regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPev2mFAoLFe",
        "colab_type": "text"
      },
      "source": [
        "Another regularization technique that is quite popular for neural networks is called *max-norm regularization*: for each neuron, it constrains the weights $\\boldsymbol{w}$ of the incoming connections such that $||\\boldsymbol{w}||_2 ≤ r$, where $r$ is the max-norm hyperparameter and $||·||_2$ is the $l_2$ norm.\n",
        "Max-norm regularization does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing $||\\boldsymbol{w}||_2$ after each training step and clipping $\\boldsymbol{w}$ if needed  ($\\boldsymbol{w}=\\boldsymbol{w}\\frac{r}{||\\boldsymbol{w}||_2}$).\n",
        "\n",
        "\n",
        "Reducing $r$ increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the vanishing/exploding gradients problems (if you are not using Batch Normalization). To implement max-norm regularization in Keras, just set every hidden layer’s `kernel_constraint` argument to a `max_norm()` constraint, with the appropriate max value, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJvfKUmEYF4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "c7aaa350-556e-46f5-b099-d69c5a7b4cc3"
      },
      "source": [
        "MaxNormDense = partial(keras.layers.Dense,\n",
        "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
        "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    MaxNormDense(300),\n",
        "    MaxNormDense(100),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "n_epochs = 2\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/2\n",
            "54000/54000 [==============================] - 6s 120us/step - loss: 0.5139 - accuracy: 0.8208 - val_loss: 0.3763 - val_accuracy: 0.8612\n",
            "Epoch 2/2\n",
            "54000/54000 [==============================] - 6s 116us/step - loss: 0.4022 - accuracy: 0.8538 - val_loss: 0.3982 - val_accuracy: 0.8547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6SyzmiyW9VB",
        "colab_type": "text"
      },
      "source": [
        "After each training iteration, the model’s `fit()` method will call the object returned by `max_norm()` , passing it the layer’s weights and getting clipped weights in return, which then replace the layer’s weights. You can define your own custom constraint function if you ever need to, and use it as the `kernel_constraint` . You can also constrain the bias terms by setting the `bias_constraint` argument.\n",
        "\n",
        "\n",
        "The `max_norm()` function has an `axis` argument that defaults to $0$. A `Dense` layer usually has weights of shape [number of inputs, number of neurons], so using `axis=0` means that the max norm constraint will apply independently to each neuron’s weight vector. If you want to use max-norm with convolutional layers, make sure to set the `max_norm()` constraint’s `axis` argument appropriately (usually `axis=[0, 1, 2]`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyew3PIRfX1k",
        "colab_type": "text"
      },
      "source": [
        "### Early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHr-uXFNr4EA",
        "colab_type": "text"
      },
      "source": [
        "A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called *early stopping*.\n",
        "\n",
        "To implement early stopping simply use the `EarlyStopping` callback. It will interrupt training when it measures no progress on the validation set for\n",
        "a number of epochs (defined by the `patience` argument). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgLpJK5Ir4M9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "41c6dfd1-73f3-40e5-cbcb-6a2f220a6685"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=10)\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, epochs=2,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[early_stopping])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/2\n",
            "54000/54000 [==============================] - 6s 111us/step - loss: 0.5793 - accuracy: 0.8001 - val_loss: 0.7936 - val_accuracy: 0.7053\n",
            "Epoch 2/2\n",
            "54000/54000 [==============================] - 6s 108us/step - loss: 0.4515 - accuracy: 0.8360 - val_loss: 0.7053 - val_accuracy: 0.7357\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}