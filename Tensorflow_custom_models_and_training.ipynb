{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow custom models and training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP9eX/Ip9MXBx7VMVmuK4Lm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Tensorflow_custom_models_and_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jrqNaZ3s4J8"
      },
      "source": [
        "# Custom models and training in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUgSg-9WVMtd"
      },
      "source": [
        "# Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_erhQefI2df"
      },
      "source": [
        "1. [Introduction](#1)\n",
        "2. [Dataset](#2)\n",
        "3. [Custom loss functions](#3)\n",
        "    1. [Saving/Loading models with custom objects](#3.1)\n",
        "4. [Custom activation functions, initializers, regularizers and constraints](#4)\n",
        "5. [Custom metrics](#5)\n",
        "6. [Custom layers](#6)\n",
        "7. [Custom models](#7)\n",
        "8. [Custom training loops](#8)\n",
        "    1. [Speeding up our training steo with `tf.function`](#8.1)\n",
        "9. [References and further reading](#9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eUAtuVlVM1s"
      },
      "source": [
        "# Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXqi9uW1XdNC"
      },
      "source": [
        "When we want to train a model with TensorFlow, the **95% of cases we** will not likely **require** anything else than TensorFlow’s high-level API, **`tf.keras`**. In other notebooks we have built various neural network architectures, including regression and classification nets, deep nets, and self-normalizing nets, using all sorts of techniques, such as Batch Normalization, dropout, learning rate schedules, and more. We also took a quick tour of TensorFlow and we saw the basic operations around tensors and other data structures. **In this notebook** we dive deeper into TensorFlow and take a look at its **lower-level Python API**.  This will be **useful when we need extra control**, to write **custom loss functions, custom metrics, layers, models, initializers, regularizers**, weight constraints, and more. We may even need to fully **control the training loop** itself, for example, to apply special transformations or constraints to the gradients, or to use multiple optimizers for different parts of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABPOPzBRuMUf"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l7lxxm7EBiL"
      },
      "source": [
        "# Dataset <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlC1phkeB4VS"
      },
      "source": [
        "In this notebook, we will be using the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYtznTIrJwx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0674ea8f-2faf-45e2-8960-4bf02eb0b487"
      },
      "source": [
        "# Load and prepare the California housing dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_b-hp0qbtix"
      },
      "source": [
        "# Custom loss functions <a name=\"3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvmN77AIbut7"
      },
      "source": [
        "Suppose we want to train a regression model, but our training set is a bit noisy. Of course, we cleaned up our dataset by fixing the outliers, but it turns out to be insufficient. Which loss function should we use? The mean squared error might penalize large errors too much. We can use the Huber loss (introduced in this [notebook](https://nbviewer.jupyter.org/github/victorviro/Machine-Learning-Python/blob/master/ML_metrics_and_cost_functions.ipynb)). The Huber loss is available in tf.keras ([`keras.losses.Huber`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)), but let’s pretend it’s not there. Let's create a function that takes the labels and predictions as arguments, and use TensorFlow operations to compute every instance’s loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyIbqFu7cL1N"
      },
      "source": [
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOI7GB4Tvq9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "cellView": "form",
        "outputId": "4a65db65-759d-40bc-8194-ac19f59ed5ea"
      },
      "source": [
        "#@title\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.figure(figsize=(9, 4))\n",
        "z = np.linspace(-4, 4, 200)\n",
        "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
        "plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
        "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
        "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
        "plt.gca().axhline(y=0, color='k')\n",
        "plt.gca().axvline(x=0, color='k')\n",
        "plt.axis([-4, 4, 0, 4])\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"$z$\")\n",
        "plt.legend(fontsize=14)\n",
        "plt.title(\"Huber loss\", fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kas2BJhGcbYS"
      },
      "source": [
        "**Note**: For better performance, we should use a **vectorized implementation**, as in this example. Moreover, if we want to benefit from TensorFlow’s graph features, we should **use only TensorFlow operations**.\n",
        "\n",
        "It is also preferable to **return a tensor containing one loss per instance**, rather than returning the mean loss. This way, Keras can apply class weights or sample weights when requested.\n",
        "\n",
        "\n",
        "Next, we can just use this loss when we compile the Keras model, then train our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2JYPbPbc3wK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71751223-4ee5-4c72-edaf-201d33d33d0e"
      },
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 2s 2ms/step - loss: 0.6360 - mae: 1.0092 - val_loss: 0.3114 - val_mae: 0.6276\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2318 - mae: 0.5307 - val_loss: 0.2226 - val_mae: 0.5062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6806e8cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB8f746YdlG3"
      },
      "source": [
        "For each batch during training, Keras will call the `huber_fn()` function to compute the loss, and use it to perform a Gradient Descent step. Moreover, it will keep track of the total loss since the beginning of the epoch, and it will display the mean loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnpnaNhjdHYJ"
      },
      "source": [
        "## Saving/Loading models that contain custom components <a name=\"3.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1JGecBNdLXU"
      },
      "source": [
        "Saving a model containing a custom loss function actually works fine, as Keras just saves the name of the function. However, whenever we load it, we need to provide a dictionary that maps the function name to the actual function. More generally, when we load a model containing custom objects, we need to map the names to the objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tckqT8H6w4H_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0051ca6-35b8-4f02-bdbb-fea7c8a8ff84"
      },
      "source": [
        "model.save(\"my_model_with_a_custom_loss\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyPpOl9ndQwV"
      },
      "source": [
        "model = keras.models.load_model(\"my_model_with_a_custom_loss\",\n",
        "                                custom_objects={\"huber_fn\": huber_fn})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrV16V9Vlg0q",
        "outputId": "07dc22d3-4f6e-4a9a-f563-8dc39d43806f"
      },
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1900 - mae: 0.4693 - val_loss: 0.2033 - val_mae: 0.4709\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.1756 - mae: 0.4471 - val_loss: 0.1893 - val_mae: 0.4551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6705b3bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb8_rbUdbAv"
      },
      "source": [
        "With the current implementation of the huber loss, any error between -1 and 1 is considered “small”. But what if we want a different threshold? One solution is to create a function that creates a configured loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcTncK8Zddgb"
      },
      "source": [
        "def create_huber(threshold=1.0):\n",
        "    def huber_fn(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    return huber_fn\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pchxAJWxDl5"
      },
      "source": [
        "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djNyjg8NxE3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22485c5e-3bca-4c85-e0b6-c82cb7f9acc7"
      },
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1913 - mae: 0.4415 - val_loss: 0.2253 - val_mae: 0.4489\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.1860 - mae: 0.4345 - val_loss: 0.2004 - val_mae: 0.4483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6702804d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Syo6l8ddsvF"
      },
      "source": [
        "Unfortunately, when we save the model, the threshold will not be saved. This means that we will have to specify the threshold value when loading the model (the name to use is \"`huber_fn`\", which is the name of the function we gave Keras, not the name of the function that created it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn_oUV1AxLuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372a0c28-da93-4cfb-8e8f-4cdfd9822e9b"
      },
      "source": [
        "model.save(\"my_model_with_a_custom_loss_threshold_2\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss_threshold_2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mU4sMy-d0V5"
      },
      "source": [
        "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2\",\n",
        "                                custom_objects={\"huber_fn\": create_huber(2.0)})"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34NCHLwxN-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fbf995-f3e3-4c63-812c-68c532cbc8a7"
      },
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1824 - mae: 0.4317 - val_loss: 0.2031 - val_mae: 0.4350\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.1795 - mae: 0.4263 - val_loss: 0.1837 - val_mae: 0.4216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6705b3a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxb5-pFad5eY"
      },
      "source": [
        "We can solve this by creating a subclass of the `keras.losses.Loss` class, and implement its `get_config()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxJR6AdUd_Xj"
      },
      "source": [
        "class HuberLoss(keras.losses.Loss):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        self.name = \"HuberLoss\"\n",
        "        super().__init__(**kwargs)\n",
        "    def call(self, y_true, y_pred):\n",
        "        tf.name_scope(\"name\")\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k9kLcXUfFsm"
      },
      "source": [
        "- The constructor accepts `**kwargs` and passes them to the parent constructor, which handles standard arguments of the ``keras.losses.Loss` class: the name of the loss and the reduction algorithm to use to aggregate the individual instance losses. By default, it is \"`sum_over_batch_size`\", which means that the loss will be the sum of the instance losses, possibly weighted by the sample weights, if any, and then divide the result by the batch size. Other possible values are `\"sum\"` and `None`.\n",
        "\n",
        "- The `call()` method takes the labels and predictions, computes all the instance losses, and returns them.\n",
        "\n",
        "- The `get_config()` method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class’s `get_config()` method, then adds the new arguments to this dictionary.\n",
        "\n",
        "We can then use any instance of this class when we compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dd_bzPBxdtV"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(1),\n",
        "])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HECxWDvUfyuT"
      },
      "source": [
        "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-J4s-YTxjh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71903289-567d-483c-b0e6-c0b30264fa5d"
      },
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5518 - mae: 0.7794 - val_loss: 0.3422 - val_mae: 0.5804\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2517 - mae: 0.5193 - val_loss: 0.2300 - val_mae: 0.4817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa66ceae390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dipUHNTHf0Cp"
      },
      "source": [
        "When we save the model, the threshold will be saved along with it, and when we load the model we just need to map the class name to the class itself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9CxRJpBxmWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bb0d20-7e89-41f8-b8da-daad0e0b38d3"
      },
      "source": [
        "model.save(\"my_model_with_a_custom_loss_class\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss_class/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ADVtNQf4N_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc2ec7a-4607-47a7-e0d9-b7f3ae6528cc"
      },
      "source": [
        "model = keras.models.load_model(\"my_model_with_a_custom_loss_class\",\n",
        "                                custom_objects={\"HuberLoss\": HuberLoss})\n",
        "print(model.loss.threshold)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVaXRvuzxsZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a69301b-f5ca-4fac-e36e-a910645e882d"
      },
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2111 - mae: 0.4721 - val_loss: 0.2042 - val_mae: 0.4490\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1963 - mae: 0.4521 - val_loss: 0.1860 - val_mae: 0.4368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa66ced4f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfO6fQalf8zg"
      },
      "source": [
        "When we save a model, Keras calls the loss instance’s `get_config()` method and saves the config. When we load the model, it calls the `from_config()` class method on the `HuberLoss` class: this method is implemented by the base class (`keras.losses.Loss`) and just creates an instance of the class, passing `**config` to the constructor.\n",
        "\n",
        "**Note**: The Keras API only specifies how to use subclassing to define layers, models, callbacks, and regularizers. If we build other components (such as losses, metrics, initializers or constraints) using subclassing, they may not be portable to other Keras implementations.\n",
        "\n",
        "More information about how to save and load custom objects and functions is available in the [TensorFlow guide](https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piQZq_OogSSf"
      },
      "source": [
        "# Custom activation functions, initializers, regularizers, and constraints <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyKYr8CxgZDO"
      },
      "source": [
        "Most Keras functionalities, such as losses, regularizers, constraints, initializers, metrics, activation functions, and layers can be customized in very much the same way. Most of the time, we will just need to write a simple function, with the appropriate inputs and outputs. For example, here are examples of a custom activation function (equivalent to `keras.activations.softplus`), a custom Glorot initializer (equivalent to `keras.initializers.glorot_normal`), a custom $l_1$ regularizer (equivalent to `keras.regularizers.l1(0.01)`) and a custom constraint that ensures weights are all positive (equivalent to `keras.constraints.nonneg()`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar19foEKx-F9"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GnEhVGPg4nl"
      },
      "source": [
        "def my_softplus(z):\n",
        "    return tf.math.log(tf.exp(z) + 1.0)\n",
        "\n",
        "def my_glorot_initializer(shape, dtype=tf.float32):\n",
        "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
        "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
        "\n",
        "def my_l1_regularizer(weights):\n",
        "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
        "\n",
        "def my_positive_weights(weights):\n",
        "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z42bwJVQhBQ4"
      },
      "source": [
        "As we can see, the arguments depend on the type of custom function. These custom functions can then be used normally, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy9wRtDUhCpL"
      },
      "source": [
        "layer = keras.layers.Dense(30, activation=my_softplus,\n",
        "                           kernel_initializer=my_glorot_initializer,\n",
        "                           kernel_regularizer=my_l1_regularizer,\n",
        "                           kernel_constraint=my_positive_weights)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw03Sn7cyJak"
      },
      "source": [
        "- The activation function will be applied to the output of this `Dense` layer, and its result will be passed on to the next layer. \n",
        "\n",
        "- The layer’s weights will be initialized using the value returned by the initializer. \n",
        "\n",
        "- At each training step, the weights will be passed to the regularization function to compute the regularization loss, which will be added to the main loss to get the final loss used for training. \n",
        "\n",
        "- Finally, the constraint function will be called after each training step, and the layer’s weights will be replaced by the constrained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjEcZCEwyJ1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cda5dfa-cda9-4e67-fb9b-e987a5462fc2"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(1, activation=my_softplus,\n",
        "                       kernel_regularizer=my_l1_regularizer,\n",
        "                       kernel_constraint=my_positive_weights,\n",
        "                       kernel_initializer=my_glorot_initializer),\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.2683 - mae: 0.7647 - val_loss: inf - val_mae: inf\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.6426 - mae: 0.5444 - val_loss: inf - val_mae: inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa673c445d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvpXCzDMyTKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477917e7-4838-4ab2-ea1c-71e6f3d29c68"
      },
      "source": [
        "model.save(\"my_model_with_many_custom_parts\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_with_many_custom_parts/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjBAqzmHyToy"
      },
      "source": [
        "model = keras.models.load_model(\n",
        "    \"my_model_with_many_custom_parts\",\n",
        "    custom_objects={\n",
        "       \"my_l1_regularizer\": my_l1_regularizer,\n",
        "       \"my_positive_weights\": my_positive_weights,\n",
        "       \"my_glorot_initializer\": my_glorot_initializer,\n",
        "       \"my_softplus\": my_softplus,\n",
        "    })"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krqkUufZhYUx"
      },
      "source": [
        "**If a function has some hyperparameters that need to be saved** along with the model, then we will want to **subclass the appropriate class**, such as `keras.regularizers.Regularizer`, `keras.constraints.Constraint`, `keras.initializers.Initializer` or `keras.layers.Layer` (for any layer, including activation functions). For example, much like we did for the custom loss, here is a simple class for $l_1$ regularization, that saves its `factor` hyperparameter (this time we do not need to call the parent constructor or the `get_config()` method, as they are not defined by the parent class):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h98vbTy8iDUA"
      },
      "source": [
        "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "    def __call__(self, weights):\n",
        "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVMoEDXmiINf"
      },
      "source": [
        "Note that we must implement the `call()` method for losses, layers (including activation functions), and models, or the `__call__()` method for regularizers, initializers, and constraints. For metrics, things are a bit different, as we will see now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_QD6S5oyjmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f214b49-4dad-4ec7-85a8-2a1ad29315dd"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(1, activation=my_softplus,\n",
        "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
        "                       kernel_constraint=my_positive_weights,\n",
        "                       kernel_initializer=my_glorot_initializer),\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.1123 - mae: 0.7253 - val_loss: inf - val_mae: inf\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6057 - mae: 0.5370 - val_loss: 2.3830 - val_mae: 0.5363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa66e101490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAn5IKMYywkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a833174e-6454-4107-ed6f-f905c9a41f4d"
      },
      "source": [
        "model.save(\"my_model_with_many_custom_parts\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_with_many_custom_parts/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0LMN-gXyzKW"
      },
      "source": [
        "model = keras.models.load_model(\n",
        "    \"my_model_with_many_custom_parts\",\n",
        "    custom_objects={\n",
        "       \"MyL1Regularizer\": MyL1Regularizer,\n",
        "       \"my_positive_weights\": my_positive_weights,\n",
        "       \"my_glorot_initializer\": my_glorot_initializer,\n",
        "       \"my_softplus\": my_softplus,\n",
        "    })"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2wtP1qeiQQJ"
      },
      "source": [
        "# Custom metrics <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXn6s4zGiRdq"
      },
      "source": [
        "Losses and metrics are conceptually not the same things: losses are used by Gradient Descent to train a model, so they must be differentiable (at least where they are evaluated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not easily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to evaluate a model, they must be more easily interpretable, and they can be non-differentiable or have 0 gradients everywhere (e.g., accuracy).\n",
        "\n",
        "That said, in most cases, defining a custom metric function is exactly the same as defining a custom loss function. In fact, we could even use the Huber loss function we created earlier as a metric (the Huber loss is seldom used as a metric, the MAE or MSE are preferred), it would work just fine (and persistence would also work the same way, in this case only saving the name of the function, \"`huber_fn`\" ):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZS_WO_VXGNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2a0f14-06ee-41d9-bfe5-2266ed866e62"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
        "# model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, epochs=2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.0826 - huber_fn: 0.9243\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6068 - huber_fn: 0.2965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffk3I9nck3_J"
      },
      "source": [
        "For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch. Most of the time, this is exactly what we want. But not always! \n",
        "\n",
        "Consider a binary classifier’s precision, for example. The precision is the number of true positives divided by the number of positive predictions (including both true positives and false positives). Suppose the model made 5 positive predictions in the first batch, 4 of which were correct: that’s 80% precision. Then suppose the model made 3 positive predictions in the second batch, but they were all incorrect: that’s 0% precision for the second batch. If we just compute the mean of these two precisions, we get 40%. But wait for a second, this is not the model’s precision over these two batches! Indeed, there were a total of 4 true positives (4 + 0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%.\n",
        "\n",
        "What we need is an object that can keep track of the number of true positives and the number of false positives, and compute their ratio when requested. This is precisely what the `keras.metrics.Precision` class does:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf35Tj3zlRpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29842bd9-51d2-4f0a-8dd6-1c2a7760a421"
      },
      "source": [
        "precision = tf.keras.metrics.Precision()\n",
        "print(precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]))\n",
        "print(precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.8, shape=(), dtype=float32)\n",
            "tf.Tensor(0.5, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hsAIrFlnDg"
      },
      "source": [
        "In this example, we created a `Precision` object, then we used it as a function, passing it the labels and predictions for the first batch, then for the second batch (note that we could also have passed sample weights). We used the same number of true and false positives as in the example we just discussed. After the first batch, it returns the precision of 80%, then after the second batch, it returns 50% (which is the overall precision so far, not the second batch’s precision). This is called a streaming metric (or stateful metric), as it is gradually updated, batch after batch.\n",
        "\n",
        "At any point, we can call the `result()` method to get the current value of the metric. We can also look at its variables (tracking the number of true and false positives) using the `variables` attribute, and reset these variables using the `reset_states()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vScyDr4Unvhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c636365-2fc1-4ce5-8377-50ec26a8667b"
      },
      "source": [
        "print(precision.result().numpy())\n",
        "print(precision.variables)\n",
        "precision.reset_states()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>, <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApEVawXon99J"
      },
      "source": [
        "If we need to create such a streaming metric, we can just create a subclass of the `keras.metrics.Metric` class. Here is a simple example that keeps track of the total Huber loss and the number of instances seen so far. When asked for the result, it returns the ratio, which is simply the mean Huber loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UpOIukIh_y_"
      },
      "source": [
        "class HuberMetric(keras.metrics.Metric):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.huber_fn = create_huber(threshold)\n",
        "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        metric = self.huber_fn(y_true, y_pred)\n",
        "        self.total.assign_add(tf.reduce_sum(metric))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFV3_IaGoUYi"
      },
      "source": [
        "- The constructor uses the `add_weight()` method to create the variables needed to keep track of the metric’s state over multiple batches, in this case, the sum of all Huber losses (`total`) and the number of instances seen so far (`count`). We could just create variables manually if we preferred. Keras tracks any `tf.Variable` that is set as an attribute (and more generally, any \"trackable\" object, such as layers or models).\n",
        "\n",
        "- The `update_state()` method is called when we use an instance of this class as a function (as we did with the `Precision` object). It updates the variables given the labels and predictions for one batch (and sample weights, but in this case we just ignore them).\n",
        "\n",
        "- The `result()` method computes and returns the final result, in this case just the mean Huber metric over all instances. When we use the metric as a function, the `update_state()` method gets called first, then the `result()` method is called, and its output is returned.\n",
        "\n",
        "- We also implement the `get_config()` method to ensure the `threshold` gets saved along with the model.\n",
        "\n",
        "- The default implementation of the `reset_states()` method just resets all variables to 0.0 (but we can override it if needed).\n",
        "\n",
        "When we define a metric using a simple function, Keras automatically calls it for each batch, and it keeps track of the mean during each epoch, just like we did manually. So the only benefit of our `HuberMetric` class is that the `threshold` will be saved. But of course, some metrics, like precision, cannot simply be averaged over batches: in those cases, there’s no other option than to implement a streaming metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16hEc8xqaLCi"
      },
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(1),\n",
        "])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1QeVNMOaN-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab47f00b-8acc-4080-dfbe-d569c0dd205f"
      },
      "source": [
        "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[HuberMetric(2.0)])\n",
        "\n",
        "history = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),\n",
        "                    epochs=2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.7665 - huber_metric: 0.7665\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2811 - huber_metric: 0.2811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npOS0a_laPYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2215ff4-5ee7-40f9-e5f0-ac931fb9dbff"
      },
      "source": [
        "model.save(\"my_model_with_a_custom_metric\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_with_a_custom_metric/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwnjLnhrtpkH"
      },
      "source": [
        "model = keras.models.load_model(\"/content/my_model_with_a_custom_metric\",\n",
        "                               custom_objects={\"huber_fn\": create_huber(2.0),\n",
        "                                               \"HuberMetric\": HuberMetric})"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZOWTMCSt2Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de53a7b-9700-48e2-e6d1-0888669c731d"
      },
      "source": [
        "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.2220 - huber_metric: 0.2220\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2013 - huber_metric: 0.2013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa66f8b1450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHhatZr6t62q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50bb610-877a-44a3-d860-9271d6766df1"
      },
      "source": [
        "model.metrics[-1].threshold"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0sPbIzvvxhm"
      },
      "source": [
        "Alternatively, we could have created the class in this way, which also supports sample weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTYB-2Aqv7J4"
      },
      "source": [
        "class HuberMetric(keras.metrics.Mean):\n",
        "    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n",
        "        self.threshold = threshold\n",
        "        self.huber_fn = create_huber(threshold)\n",
        "        super().__init__(name=name, dtype=dtype)\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        metric = self.huber_fn(y_true, y_pred)\n",
        "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8RMLKUApHUt"
      },
      "source": [
        "# Custom layers <a name=\"6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ3ZNJN9pMMC"
      },
      "source": [
        "We may occasionally want to build an architecture that contains an exotic layer for which TensorFlow does not provide a default implementation. In this case, we will need to create a custom layer. Or sometimes we may simply want to build a very repetitive architecture, containing identical blocks of layers repeated many times, and it would be convenient to treat each block of layers as a single layer. For instance, in a ResNet50 model, we would have several ResNet blocks subclassing `Layer`, and a single `Model` encompassing the entire ResNet50 network.\n",
        "\n",
        "First, some layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If we want to create a custom layer without any weights, the simplest option is to write a function and wrap it in a `keras.layers.Lambda layer`. For example, the following layer will apply the exponential function to its inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evZTX6gpmE-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0fef1f-248b-4350-d186-8e057f456971"
      },
      "source": [
        "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
        "exponential_layer([-1., 0., 1.])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC6eFANQmb-4"
      },
      "source": [
        "This custom layer can then be used like any other layer, using the sequential API, the functional API, or the subclassing API. We can also use it as an activation function (or we could just use `activation=tf.exp`, or `activation=keras.activations.exponential`, or simply `activation=\"exponential\"` ). The exponential layer is sometimes used in the output layer of a regression model when the values to predict have very different scales (e.g., $0.001$, $10.$, $1000.$).\n",
        "\n",
        "To build a custom stateful layer (i.e., a layer with weights), we need to create a subclass of the `keras.layers.Layer` class. For example, the following class implements a simplified version of the Dense layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xuM5O-jm4ti"
      },
      "source": [
        "class MyDense(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\", \n",
        "            shape=[batch_input_shape[-1], self.units],\n",
        "            initializer=\"glorot_normal\", \n",
        "            trainable=True\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            name=\"bias\", \n",
        "            shape=[self.units], \n",
        "            initializer=\"zeros\", \n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.activation(X @ self.kernel + self.bias)\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"units\": self.units,\n",
        "                \"activation\": keras.activations.serialize(self.activation)}"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DduKzvv9u0cJ"
      },
      "source": [
        "- The constructor takes all the hyperparameters as arguments (in this example just `units` and `activation`), and it also takes a `**kwargs` argument. It calls the parent constructor, passing it the `kwargs`: this takes care of standard arguments such as `input_shape`, `trainable`, `name`, and so on. Then it saves the hyperparameters as attributes, converting the `activation` argument to the appropriate activation function using the `keras.activations.get()` function (it accepts functions, standard strings like `\"relu\"` or `\"selu\"` , or simply `None`) (this function is specific to `tf.keras`, we could use `keras.activations.Activation` instead).\n",
        "\n",
        "- The `build()` method’s role is to create the layer’s variables, by calling the `add_weight()` method for each weight. The `build()` method is called the first time the layer is called. At that point, Keras will know the shape of this layer’s inputs, and it will pass it to the `build()` method (the Keras API calls this argument `input_shape`, but since it also includes the batch dimension, we prefer to call it `batch_input_shape`), which is often necessary to create some of the weights. For example, we need to know the number of neurons in the previous layer in order to create the connection weights matrix (i.e., the `\"kernel\"`): this corresponds to the size of the last dimension of the inputs.\n",
        "\n",
        "- The `call()` method actually performs the desired operations. In this case, we compute the matrix multiplication of the inputs `X` and the layer’s kernel, we add the bias vector, we apply the activation function to the result, and this gives us the output of the layer.\n",
        "\n",
        "- The `compute_output_shape()` method simply returns the shape of this layer’s outputs. In this case, it is the same shape as the inputs, except the last dimension is replaced with the number of neurons in the layer. Note that in `tf.keras`, shapes are instances of the `tf.TensorShape` class, which we can convert to Python lists using `as_list()`.\n",
        "\n",
        "- The `get_config()` method is just like earlier. Note that we save the activation function’s full configuration by calling `keras.activations.serialize()` . \n",
        "\n",
        "We can now use a `MyDense` layer just like any other layer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCnQxV_0m7UQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea65f8f-12bb-463f-b243-66ca22eba4cc"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    MyDense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))\n",
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.1882 - val_loss: 2.1264\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5879 - val_loss: 0.9615\n",
            "162/162 [==============================] - 0s 811us/step - loss: 0.4988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4987967908382416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgXyrvHcwkHR"
      },
      "source": [
        "**Note**: We can generally omit the `compute_output_shape()` method, as `tf.keras` automatically infers the output shape, except when the layer is dynamic.\n",
        "\n",
        "To create a layer with multiple inputs (e.g., `Concatenate`), the argument to the `call()` method should be a tuple containing all the inputs, and similarly, the argument to the `compute_output_shape()` method should be a tuple containing each input’s batch shape. To create a layer with multiple outputs, the `call()` method should return the list of outputs, and the `compute_output_shape()` should return the list of batch output shapes (one per output). For example, the following toy layer takes two inputs and returns two outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbONCkpKyP4t"
      },
      "source": [
        "class MyMultiLayer(keras.layers.Layer):\n",
        "    def call(self, X):\n",
        "        X1, X2 = X\n",
        "        return X1 + X2, X1 * X2\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
        "        return [batch_input_shape1, batch_input_shape2]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoenicroxD3N"
      },
      "source": [
        "This layer may now be used like any other layer, but of course only using the functional and subclassing APIs, not the sequential API (which only accepts layers with one input and one output)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-tP7r7qxESA"
      },
      "source": [
        "inputs1 = keras.layers.Input(shape=[2])\n",
        "inputs2 = keras.layers.Input(shape=[2])\n",
        "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb3vmfNuxKVR"
      },
      "source": [
        "If our layer needs to have a different behavior during training and during testing (e.g., if it uses `Dropout` or `BatchNormalization` layers), then we must add a `training` argument to the `call()` method and use this argument to decide what to do. For example, let’s create a layer that adds Gaussian noise during training (for regularization), but does nothing during testing (Keras actually has a layer that does the same thing: `keras.layers.GaussianNoise`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6agXABQxUTG"
      },
      "source": [
        "class AddGaussianNoise(keras.layers.Layer):\n",
        "    def __init__(self, stddev, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, X, training=None):\n",
        "        if training:\n",
        "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
        "            return X + noise\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return batch_input_shape"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otrq5AGnxY8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16c3b38-d047-4779-a3d8-effef7e77ac8"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "model.fit(X_train_scaled, y_train, epochs=2,\n",
        "          validation_data=(X_valid_scaled, y_valid))\n",
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4809 - val_loss: 1.4488\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4220 - val_loss: 0.8684\n",
            "162/162 [==============================] - 0s 853us/step - loss: 0.3980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39801305532455444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEf-7KCIy0TP"
      },
      "source": [
        "More information about building custom layers is available [here](https://www.tensorflow.org/guide/keras/custom_layers_and_models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ1guxUopMVm"
      },
      "source": [
        "# Custom models <a name=\"7\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBV3gfYvpPtx"
      },
      "source": [
        "We already looked at custom model classes when we discussed the subclassing API (see [notebook](https://nbviewer.jupyter.org/github/victorviro/Deep_learning_python/blob/master/Keras_Functional_API.ipynb#The-Subclassing-API)). It is actually quite straightforward, just subclass the `keras.models.Model` class, create layers and variables in the constructor, and implement the `call()` method to do whatever you want the model to do. For example, suppose we want to build the model represented in the next figure:\n",
        "\n",
        "![](https://i.ibb.co/NTvMVbt/custom-model.png)\n",
        "\n",
        "The inputs go through a first dense layer, then through a residual block composed of two dense layers and an addition operation (a residual block adds its inputs to its outputs), then through this same residual block 3 more times, then through a second residual block, and the final result goes through a dense output layer. This model does not make much sense, it’s just an example to illustrate the fact that we can easily build any kind of model we want, even containing loops and skip connections. To implement this model, it is best to first create a `ResidualBlock` layer, since we are going to create a couple of identical blocks (and we might want to reuse it in another model):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSC3sWwLp2zu"
      },
      "source": [
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"relu\")\n",
        "                       for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBuOdK1Ep4sn"
      },
      "source": [
        "This layer is a bit special since it contains other layers. This is handled transparently by Keras: it automatically detects that the hidden attribute contains trackable objects (layers in this case), so their variables are automatically added to this layer’s list of variables. Next, let’s use the subclassing API to define the model itself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTztNal4qCIf"
      },
      "source": [
        "class ResidualRegressor(keras.models.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(30, activation=\"relu\")\n",
        "        self.block1 = ResidualBlock(2, 30)\n",
        "        self.block2 = ResidualBlock(2, 30)\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = self.hidden1(inputs)\n",
        "        for _ in range(1 + 3):\n",
        "            Z = self.block1(Z)\n",
        "        Z = self.block2(Z)\n",
        "        return self.out(Z)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOI749ZXqNLm"
      },
      "source": [
        "We create the layers in the constructor and use them in the `call()` method. This model can then be used like any other model (compile it, fit it, evaluate it, and use it to make predictions). If we also want to be able to save the model using the `save()` method, and load it using the `keras.models.load_model()` function, we must implement the `get_config()` method (as we did earlier) in both the `ResidualBlock` class and the `ResidualRegressor` class. Alternatively, we can just save and load the weights using the `save_weights()` and `load_weights()` methods.\n",
        "\n",
        "The `Model` class is actually a subclass of the `Layer` class, so models can be defined and used exactly like layers. But a model also has some extra functionalities, including of course its `compile()`, `fit()`, `evaluate()` and `predict()` methods (and a few variants, such as `train_on_batch()` or `fit_generator()`), plus the `get_layers()` method (which can return any of the model’s layers by name or by index), and the `save()` method (and support for `keras.models.load_model()` and `keras.models.clone_model()`). So if models provide more functionalities than layers, why not just define every layer as a model? Well, technically we could, but it is probably cleaner to distinguish the internal components of our model (layers or reusable blocks of layers) from the model itself. The former should subclass the `Layer` class, while the latter should subclass the `Model` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6btFM0DDqDyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a84a52-7078-4296-ddf9-16ca78148e46"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "X_new_scaled = X_test_scaled\n",
        "\n",
        "model = ResidualRegressor(1)\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = model.fit(X_train_scaled, y_train, epochs=3)\n",
        "score = model.evaluate(X_test_scaled, y_test)\n",
        "y_pred = model.predict(X_new_scaled)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "363/363 [==============================] - 2s 2ms/step - loss: 0.6565\n",
            "Epoch 2/3\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4053\n",
            "Epoch 3/3\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4049\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.3433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HtIqCTc0UUf"
      },
      "source": [
        "We could have defined the model using the sequential API instead:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9WUv6U0We3"
      },
      "source": [
        "block1 = ResidualBlock(2, 30)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    block1, block1, block1, block1,\n",
        "    ResidualBlock(2, 30),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6kvQOT1pPqc"
      },
      "source": [
        "With that, we can quite naturally and concisely build almost any model that we want, either using the sequential API, the functional API, the subclassing API, or even a mix of these. More information about building custom models is available [here](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZv-DGHfpYfX"
      },
      "source": [
        "# Custom training loops <a name=\"8\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeja-KT4pYbJ"
      },
      "source": [
        "In some rare cases, the `fit()` method may not be flexible enough for what we need to do. For example, if we want to use two different optimizers. Since the `fit()` method only uses one optimizer (the one that we specify when compiling the model), implementing this requires writing our own custom loop.\n",
        "\n",
        "We may also like to write our own custom training loops simply to feel more confident that it does precisely what we intended it to do. However, writing a custom training loop will make our code longer, more error-prone, and harder to maintain. Unless we really need the extra flexibility, we should prefer using the `fit()` method rather than implementing our own training loop.\n",
        "\n",
        "First, let’s build a simple model. No need to compile it, since we will handle the training loop manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChWIRdO7s_X4"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "l2_reg = keras.regularizers.l2(0.05)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", kernel_regularizer=l2_reg),\n",
        "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
        "])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMME6pUVtDMy"
      },
      "source": [
        "Next, let's prepare the datasets for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZOlCpvCtDkT"
      },
      "source": [
        "batch_size = 32\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "# Prepare the validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_valid_scaled, y_valid))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMK0tK_AtGlK"
      },
      "source": [
        "Let’s define some hyperparameters, choose the optimizer, the loss function, and the metrics (just the MAE in this example):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ5h0UrstTxZ"
      },
      "source": [
        "n_epochs = 2\n",
        "n_steps = len(X_train) // batch_size\n",
        "progress_bar_values_names = ['loss', 'mse']\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.MeanSquaredError()\n",
        "train_metric = keras.metrics.MeanAbsoluteError()\n",
        "val_metric = keras.metrics.MeanAbsoluteError()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxvStbACsrZ3"
      },
      "source": [
        "And now we are ready to build the custom loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP39p10Xx-wR",
        "outputId": "5b08ecaf-5b07-470c-da99-0b11c828fdcc"
      },
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "    progbar = tf.keras.utils.Progbar(\n",
        "        target=n_steps, \n",
        "        stateful_metrics=progress_bar_values_names\n",
        "    )\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        # Open a GradientTape to record the ops run during the forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(x_batch_train, training=True)\n",
        "            # Compute the loss for this minibatch\n",
        "            main_loss = loss_fn(y_batch_train, y_pred)\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "\n",
        "        # Compute gradients of the trainable variables with respect to the loss\n",
        "        gradients = tape.gradient(loss, model.trainable_weights)\n",
        "        # Gradient descent step to update the weights to minimize the loss\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "        # Update training metric\n",
        "        train_metric.update_state(y_batch_train, y_pred)\n",
        "        values = [('loss', loss), ('mse', train_metric.result())]\n",
        "        progbar.update(step, values)\n",
        "    # Display metrics at the end of each epoch\n",
        "    train_mae = train_metric.result()\n",
        "    print(f\"Training mse over epoch: {float(train_mae):.4f}\")\n",
        "    # Reset training metric at the end of each epoch\n",
        "    train_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        y_pred_val = model(x_batch_val, training=False)\n",
        "        # Update val metric\n",
        "        val_metric.update_state(y_batch_val, y_pred_val)\n",
        "    val_mae = val_metric.result()\n",
        "    val_metric.reset_states()\n",
        "    print(f\"Validation mse: {float(val_mae):.4f}\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "362/362 [==============================] - 4s 12ms/step - loss: 0.5262 - mse: 0.5764\n",
            "Training mse over epoch: 0.5764\n",
            "Validation mse: 0.5220\n",
            "Epoch 2/2\n",
            "362/362 [==============================] - 4s 11ms/step - loss: 0.3898 - mse: 0.5093\n",
            "Training mse over epoch: 0.5093\n",
            "Validation mse: 0.4736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TL5Xk5rG77W"
      },
      "source": [
        "- We create two nested loops: one for the epochs, the other for the batches within an epoch. We use `tf.keras.utils.Progbar` to display a progress bar during each epoch. \n",
        "\n",
        "- Inside the `tf.GradientTape()` block, we make a prediction for one batch (using the model as a function), and we compute the loss: it is equal to the main loss (mse) plus the other losses (in this model, there is one regularization loss per layer). The `tf.add_n()` sums multiple tensors of the same shape and data type.\n",
        "\n",
        "- Next, we ask the tape to compute the gradient of the loss with regards to each trainable variable, and we apply them to the optimizer to perform a Gradient Descent step. An explanation about how TensorFlow's autodiff works is available in the notebook [Tensorflow's autodifferentiation](https://nbviewer.jupyter.org/github/victorviro/Deep_learning_python/blob/master/TensorFlow%E2%80%99s_autodifferentiation.ipynb). If we want to apply any other transformation to the gradients, we can simply do so before calling the `apply_gradients()` method.\n",
        "\n",
        "- Next, we update the training metrics (over the current step of the epoch), and we updated the progress bar.\n",
        "\n",
        "- At the end of each epoch, we print the metric value in the training dataset, and we reset its state. We also compute the metric in the validation dataset and we print it.\n",
        "\n",
        "As we can see, there are a lot of things we need to get right, it is easy to make a mistake. But on the bright side, we get full control.\n",
        "\n",
        "**Note**: If we add weight constraints to your model (e.g., by setting `kernel_constraint` or `bias_constraint` when creating a layer), we should update the training loop to apply these constraints just after `apply_gradients()`:\n",
        "\n",
        "```\n",
        "for variable in model.variables:\n",
        "    if variable.constraint is not None:\n",
        "        variable.assign(variable.constraint(variable))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7_ZFOdVHBaM"
      },
      "source": [
        "## Speeding-up our training step with `tf.function` <a name=\"8.1\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YuqD2u9tJ2"
      },
      "source": [
        "The default runtime in TensorFlow 2.0 is eager execution. This is great for debugging, but **graph compilation has a performance advantage** (more information about eager and graph modes in TensorFlow is available in this [notebook](https://nbviewer.jupyter.org/github/victorviro/Deep_learning_python/blob/master/TensorFlow_Eager_vs_Graph_execution.ipynb)). \n",
        "\n",
        "We can compile into a static graph any function that takes tensors as input. Just we have to add a `@tf.function` decorator on it. So, let's create a function that performs the training step and decorate it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-cH1xDV_qeD"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x, training=True)\n",
        "        main_loss = loss_fn(y, y_pred)\n",
        "        loss = tf.add_n([main_loss] + model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    train_metric.update_state(y, y_pred)\n",
        "    return loss"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYgN0Qzc-4mP"
      },
      "source": [
        "Now, let's re-run our training loop with this compiled training step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAAmn1zQALHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d025e9-02e1-456c-8b99-97fa3ddb08ed"
      },
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "    progbar = tf.keras.utils.Progbar(\n",
        "        target=n_steps, \n",
        "        stateful_metrics=progress_bar_values_names\n",
        "    )\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch_train, y_batch_train)\n",
        "        values = [('loss', loss), ('mse', train_metric.result())]\n",
        "        progbar.update(step, values)\n",
        "    \n",
        "    train_mae = train_metric.result()\n",
        "    print(f\"Training mse over epoch: {float(train_mae):.4f}\")\n",
        "    train_metric.reset_states()\n",
        "\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        y_pred_val = model(x_batch_val, training=False)\n",
        "        val_metric.update_state(y_batch_val, y_pred_val)\n",
        "    val_mae = val_metric.result()\n",
        "    val_metric.reset_states()\n",
        "    print(f\"Validation mse: {float(val_mae):.4f}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.7104 - mse: 0.5096\n",
            "Training mse over epoch: 0.5096\n",
            "Validation mse: 0.4891\n",
            "Epoch 2/2\n",
            "362/362 [==============================] - 1s 1ms/step - loss: 0.5323 - mse: 0.5094\n",
            "Training mse over epoch: 0.5094\n",
            "Validation mse: 0.4897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Ca0kky0G19"
      },
      "source": [
        "# References and further reading <a name=\"9\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db_xVvoP0JHq"
      },
      "source": [
        "- [TensorFlow: Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
        "\n",
        "- [TensorFlow: Save and load Keras models](https://www.tensorflow.org/guide/keras/save_and_serialize)\n",
        "\n",
        "- [TensorFlow: Writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/)\n",
        "\n",
        "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)"
      ]
    }
  ]
}