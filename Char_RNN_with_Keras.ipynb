{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char RNN with Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN5BXkW7h2m+EGXplfrS1SE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Char_RNN_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NZ3J0OEobd_",
        "colab_type": "text"
      },
      "source": [
        "# NLP with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hOisI-duPKZ",
        "colab_type": "text"
      },
      "source": [
        "Can we build a machine that can read and write natural language?\n",
        "\n",
        "A common approach for natural language tasks is to use recurrent neural networks. We will continue to explore RNNs (introduced in previous notebooks [Introduction to RNNs](https://github.com/victorviro/Deep_learning_python/blob/master/Introduction_to_RNNs.ipynb), [RNNs for long sequences](https://github.com/victorviro/Deep_learning_python/blob/master/RNNs_long_sequences.ipynb)), with a Character-Level Language Model which we call *character RNN*, trained to predict the next character in a sentence. This will allow us to generate some original text, and in the process, we will see how to build a TensorFlow Dataset on a very long sequence. We will first use a *stateless RNN* (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a stateful RNN (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V5lYH15BGEy",
        "colab_type": "text"
      },
      "source": [
        "# Lenguage model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9X_UL5zBL2r",
        "colab_type": "text"
      },
      "source": [
        "Given any sentence or sequence of words/characters $w_1,w_2, …,w_T$, a language model tries to predict what is the probability of that sentence. In other words, the language model tries to learn the joint probability distribution of sequences of characters/words in a training text, i.e. trying to learn the joint probability function. For example, if we are trying to predict a sequence of $T$ words, we try to get the joint probability $P(w_1,w_2,...,w_T)$\n",
        " as big as we can. Applying the [general product rule](https://en.wikipedia.org/wiki/Chain_rule_(probability)) we can express this joint probability in terms of conditional probabilities $P(w_1,w_2,...,w_T)=\\prod_{t=1}^{T}P(w_t|w_{t-1},...,w_1)$. \n",
        "\n",
        "For a further explanation of how language models work, check this [video](https://www.coursera.org/lecture/nlp-sequence-models/language-model-and-sequence-generation-gw1Xw).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zagQXGwuuuO6",
        "colab_type": "text"
      },
      "source": [
        "## Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ghf8UYtu2b7",
        "colab_type": "text"
      },
      "source": [
        "The term \"char-rnn\" is short for \"character recurrent neural network\", and is effectively a recurrent neural network trained to predict the next character given a sequence of previous characters. In this way, we can think of a char-rnn as a classification model. In the same way that we output a probability distribution over classes when doing image classification, for a char-rnn we wish to output a probability distribution over character classes, i.e., a vocabulary of characters. Unlike image classification where we are given a single image and expected to predict an output immediately, however, in this setting, we are given the characters one at a time and only expected to predict an output after the last character.\n",
        "\n",
        "\n",
        "In a famous 2015 [blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) titled \"The Unreasonable Effectiveness of Recurrent Neural Networks\", Andrej Karpathy showed how to train an RNN to predict the next character in a sentence. This *Char-RNN* can then be used to generate novel text, one character at a time.\n",
        "\n",
        "Let’s look at how to build a Char-RNN, step by step, starting with the creation of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKJFUDSyu_H-",
        "colab_type": "text"
      },
      "source": [
        "### Creating the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZasiG2hbvDIr",
        "colab_type": "text"
      },
      "source": [
        "First, let’s download all of Shakespeare’s work, using Keras’s handy `get_file()` function and downloading the data from Andrej Karpathy’s [Char-RNN project](https://github.com/karpathy/char-rnn):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs66BUPiwV-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70NfwaLcvIQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fad44e0f-e8a9-427a-df5b-5612f90f6d3c"
      },
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut URL\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uurMv9bNoxj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "06394f1d-2531-47b0-cbfa-8d7d9565277c"
      },
      "source": [
        "print(f'Lenght of text: {len(shakespeare_text)} characters')\n",
        "print(f'First 100 characters of the text:\\n{shakespeare_text[:250]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lenght of text: 1115394 characters\n",
            "First 100 characters of the text:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRBPnJI_vdV3",
        "colab_type": "text"
      },
      "source": [
        "Next, we must encode every character as an integer. We use Keras’s [`Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer?hl=en) class. First, we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters (it does not start at 0, so we can use that value for masking):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZUE7q-9vyHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNdYJgfEv55P",
        "colab_type": "text"
      },
      "source": [
        "We set `char_level=True` to get character-level encoding rather than the default word-level encoding. Note that this tokenizer converts the text to lowercase by default (but we can set `lower=False` if we do not want that). Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "749RBN9fvcHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "0d2d8839-c5de-4d51-8a9d-41c9fff62701"
      },
      "source": [
        "print(f'Sentence \"{shakespeare_text[:5]}\" encoded as {tokenizer.texts_to_sequences([shakespeare_text[:5]])}')\n",
        "print(f'[[20, 6, 9, 8, 3]] -----> {tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])}')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) #max_id\n",
        "print(f'Number of distinct characters in text: {vocab_size}')\n",
        "\n",
        "dataset_size = tokenizer.document_count \n",
        "print(f'Total number of characters in text: {dataset_size}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence \"First\" encoded as [[20, 6, 9, 8, 3]]\n",
            "[[20, 6, 9, 8, 3]] -----> ['f i r s t']\n",
            "Number of distinct characters in text: 39\n",
            "Total number of characters in text: 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRs26O8zwRV4",
        "colab_type": "text"
      },
      "source": [
        "Let’s encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLt_hy9MwTzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6f914a05-f10d-4c43-9a7d-3ff8b96971a1"
      },
      "source": [
        "[text_as_int] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "print(f'Full text encoded: {text_as_int}')\n",
        "print(f'Total number of characters in text: {len(text_as_int)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full text encoded: [19  5  8 ... 20 26 10]\n",
            "Total number of characters in text: 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_saFE_qw_JX",
        "colab_type": "text"
      },
      "source": [
        "Let’s create a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will return each character one by one from this set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JzZ2ygNxA0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61bbs6ADBHBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8c299934-a6a7-471b-9ebe-14ba4808c8f4"
      },
      "source": [
        "for element in dataset.take(5):\n",
        "    print(element)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(19, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdI3i7k2B_GI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69365032-df8a-4201-9376-38658d9f3945"
      },
      "source": [
        "print(dataset.cardinality().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGfNpXdOxLkR",
        "colab_type": "text"
      },
      "source": [
        "### Chopping the Sequential Dataset into Multiple Windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d760APesxMdq",
        "colab_type": "text"
      },
      "source": [
        "The training set now consists of a single sequence of over a million characters, so we can’t just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will convert this long sequence of characters into many smaller windows or subsequences of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called *truncated backpropagation through time* (see notebook [Introduction to RNNs](https://github.com/victorviro/Deep_learning_python/blob/master/Introduction_to_RNNs.ipynb)). Let’s call the `batch()` method to create a dataset of short text windows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wItZ9cYndB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps+1\n",
        "windows = dataset.batch(window_length, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFXohesTniyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "5442ddbd-b266-4189-a92c-d0d9b4098370"
      },
      "source": [
        "for window_tensor in windows.take(2):\n",
        "    print(window_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
            "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
            "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
            " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
            " 10 15  3 13  0], shape=(101,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[ 4  8  1  0  4 11 11  0  8  1  7  3 11 25  1 12  0  8  4  2  6  1  8  0\n",
            "  2  3  0 12  5  1  0  2  6  4  9  0  2  3  0 19  4 14  5  7  6 29 10 10\n",
            "  4 11 11 23 10  8  1  7  3 11 25  1 12 26  0  8  1  7  3 11 25  1 12 26\n",
            " 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 19  5  8  7  2 17  0\n",
            " 15  3 13  0 24], shape=(101,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wuGi7kPxhzF",
        "colab_type": "text"
      },
      "source": [
        "**Note**: We can try tuning `window_length`: it is easier to train RNNs on shorter input sequences, but of course the RNN will not be able to learn any pattern longer than `window_length`, so we don’t make it too small.\n",
        "\n",
        "By default, the `batch()` method creates nonoverlapping windows. To ensure that all windows are exactly 101 characters long (which will allow us to create batches without having to do any padding), we set `drop_remainder=True` (otherwise the last window can contain less than 101 characters). Notice that we call `batch(window_length)` so all windows have exactly that length and we will get a single tensor for each of them. Now the dataset contains consecutive windows of 101 characters each.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzSSF1nqu72k",
        "colab_type": "text"
      },
      "source": [
        "We create a function to separate the inputs (the first 100 characters) from the target (the last 100 characters):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rle5uj5HosZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(windows):\n",
        "  input_text = windows[:-1]\n",
        "  target_text = windows[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = windows.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2MSw59Co7eH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f483ad9e-8eb4-4317-e1b1-2cf0fd83e444"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print(input_example)\n",
        "  print(target_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
            "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
            "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
            " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
            " 10 15  3 13], shape=(100,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[ 5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1  0\n",
            " 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1  4\n",
            "  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24 17\n",
            "  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10\n",
            " 15  3 13  0], shape=(100,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSny43I-pFpc",
        "colab_type": "text"
      },
      "source": [
        "### Create training batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNA3u9q7xsMT",
        "colab_type": "text"
      },
      "source": [
        "Since Gradient Descent works best when the instances in the training set are independent and identically distributed, we need to shuffle these windows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT7YkgVZXnF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMUrn-zAX2Ut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "3d15e96e-16d2-4d1b-95f0-9bff8fac2e75"
      },
      "source": [
        "for batch in dataset.take(1):\n",
        "    print(batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
            "array([[14,  3,  7, ...,  4,  5,  9],\n",
            "       [ 9, 20,  0, ...,  1,  7,  0],\n",
            "       [ 3,  8,  0, ..., 18, 15,  0],\n",
            "       ...,\n",
            "       [ 2,  6,  4, ...,  9, 20, 27],\n",
            "       [ 7,  2,  3, ...,  1,  4,  8],\n",
            "       [ 9,  7, 28, ..., 14,  3,  8]])>, <tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
            "array([[ 3,  7,  2, ...,  5,  9,  0],\n",
            "       [20,  0,  2, ...,  7,  0,  2],\n",
            "       [ 8,  0,  5, ..., 15,  0, 21],\n",
            "       ...,\n",
            "       [ 6,  4,  2, ..., 20, 27,  7],\n",
            "       [ 2,  3,  8, ...,  4,  8,  7],\n",
            "       [ 7, 28,  0, ...,  3,  8,  1]])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhHzO5vxq0E1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8391dcc8-d7d3-477d-e602-bafc309c5582"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIiOslMix384",
        "colab_type": "text"
      },
      "source": [
        "Categorical input features should generally be encoded, usually as one-hot vectors or as embeddings. We could encode each character using a one-hot vector because there are fairly few distinct characters (only 39), but we will encode them as embeddings when defining the model.\n",
        "\n",
        "That’s it! Preparing the dataset was the hardest part. Now let’s create the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-eBxsnfx4L1",
        "colab_type": "text"
      },
      "source": [
        "### Building and Training the Char-RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Vu0m82x9LH",
        "colab_type": "text"
      },
      "source": [
        "To predict the next character based on the previous 100 characters, we can use an RNN with 1 `GRU` layer of 1024 units each and 20% dropout on the inputs (`dropout`). We can tweak these hyperparameters later if needed. The output layer is a `Dense` layer which must have 39 units (`vocab_size`) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). We choose to Keras sequential model here since all the layers in the model only have a single input and produce a single output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aE1pr6MqBLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, stateful):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=stateful,\n",
        "                            dropout=0.2),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHrWCj0fi8FG",
        "colab_type": "text"
      },
      "source": [
        "Let's create the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv5BBfh-sPxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of RNN units\n",
        "rnn_units = 1024 \n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "model = build_model(vocab_size = vocab_size, embedding_dim=embedding_dim,\n",
        "                    rnn_units=rnn_units, batch_size=BATCH_SIZE, stateful=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuiMMjDz0ZLg",
        "colab_type": "text"
      },
      "source": [
        "We set `stateful=False` because we are building a stateless model, that is the internal state of the `GRU` layer is reset every time it sees a new batch. This is useful when every sample is assumed to be independent of the past. Note that this is not our case since we have broken the whole text into shorter sequences (windows of length 101), although we are going to feed these shorter sequences shuffled into the RNN layer. It would be appropriate to build a stateful model and feed the windows sequentially onto the RNN layer as we will see it in the next section.\n",
        "\n",
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![](https://i.ibb.co/fpn6qR1/stateless-char-rnn.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsZ2AGQraMmY",
        "colab_type": "text"
      },
      "source": [
        "In our example the window length of the input is 101 but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5t2PR_-aQoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "8aa02b56-cb52-45ad-ada4-4e8c48f13d3e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (32, None, 256)           9984      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (32, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (32, None, 39)            39975     \n",
            "=================================================================\n",
            "Total params: 3,988,263\n",
            "Trainable params: 3,988,263\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOZz0RTCaRHe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "5fa44959-5f98-48c8-b4c6-1ea016b16d4d"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szacny4E03by",
        "colab_type": "text"
      },
      "source": [
        "Before training the model, we run the model to see that it behaves as expected. First, we check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Ir72rM03m-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "119fd861-83e5-473d-a66e-4b30a9cbadf7"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "   example_batch_predictions = model(input_example_batch)\n",
        "   print(example_batch_predictions.shape, \"# (batch_size, window_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 39) # (batch_size, window_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeviqBzx1Vpe",
        "colab_type": "text"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary. \n",
        "\n",
        "**Note**: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop as we will see when we generate fake Shakespearean text.\n",
        "\n",
        "Let's try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJi_4b3f1WGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "32cbfbc6-8815-4103-8efb-db953f1144f1"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22,  5, 18, 34,  6, 24, 27, 33, 33, 27, 38, 12, 26,  4, 30, 18,  1,\n",
              "       31, 32, 26, 30, 27, 19,  0, 18, 16, 12, 30, 11, 22, 29, 24, 22, 28,\n",
              "       29, 26, 10, 30,  1,  8, 11, 33,  6,  0, 11, 19, 30, 19, 24, 23,  8,\n",
              "       33,  5, 29, 16, 28, 20,  7, 10, 10, 22, 11,  4, 12, 20, 32, 19, 30,\n",
              "       11, 25, 38, 27, 13, 10, 35, 22, 24,  2, 18, 14, 33, 18, 38, 18, 22,\n",
              "       14, 17, 32, 22, 37,  0, 11, 16, 32, 34, 20, 26,  9,  3, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNtUvzjm1n0z",
        "colab_type": "text"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index.\n",
        "\n",
        "As we said, the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character. The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions. Because our model returns logits (we did not use the activation function in the last `Dense` layer), we need to set the `from_logits` flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa5HQxaMrpd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv1oon6NLRTK",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the loss for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmDUb69YK1jZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "aa39e60b-9f97-478e-fc48-fa6d661a41c8"
      },
      "source": [
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, window_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (32, 100, 39)  # (batch_size, window_length, vocab_size)\n",
            "scalar_loss:       3.663464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQLmZcNXjYem",
        "colab_type": "text"
      },
      "source": [
        "We can then compile this model, using the loss previously defined and an Adam optimizer. Finally, we are ready to train the model for several epochs (this may take many hours, depending on our hardware):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntu_4DHsqtFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=loss, optimizer=\"adam\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfwKszsJLoSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b085e15-d400-4eb4-a4f1-7e3a0f7e523c"
      },
      "source": [
        "print(f'Number of batches in the dataset: {dataset_size // window_length // BATCH_SIZE}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of batches in the dataset: 345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xfvAYZUn89f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fab43b2-6a7c-4397-cd75-cdc07242b6be"
      },
      "source": [
        "history = model.fit(dataset, epochs=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "345/345 [==============================] - 12s 34ms/step - loss: 2.3382\n",
            "Epoch 2/30\n",
            "345/345 [==============================] - 12s 34ms/step - loss: 1.7076\n",
            "Epoch 3/30\n",
            "345/345 [==============================] - 12s 34ms/step - loss: 1.4986\n",
            "Epoch 4/30\n",
            "345/345 [==============================] - 12s 34ms/step - loss: 1.3980\n",
            "Epoch 5/30\n",
            "345/345 [==============================] - 12s 34ms/step - loss: 1.3321\n",
            "Epoch 6/30\n",
            "345/345 [==============================] - 12s 34ms/step - loss: 1.2789\n",
            "Epoch 7/30\n",
            "345/345 [==============================] - 12s 35ms/step - loss: 1.2311\n",
            "Epoch 8/30\n",
            "345/345 [==============================] - 12s 35ms/step - loss: 1.1850\n",
            "Epoch 9/30\n",
            "345/345 [==============================] - 12s 35ms/step - loss: 1.1394\n",
            "Epoch 10/30\n",
            "345/345 [==============================] - 12s 35ms/step - loss: 1.0930\n",
            "Epoch 11/30\n",
            "345/345 [==============================] - 12s 35ms/step - loss: 1.0465\n",
            "Epoch 12/30\n",
            "345/345 [==============================] - 12s 35ms/step - loss: 1.0018\n",
            "Epoch 13/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.9590\n",
            "Epoch 14/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.9200\n",
            "Epoch 15/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.8851\n",
            "Epoch 16/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.8565\n",
            "Epoch 17/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.8301\n",
            "Epoch 18/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.8105\n",
            "Epoch 19/30\n",
            "345/345 [==============================] - 13s 36ms/step - loss: 0.7918\n",
            "Epoch 20/30\n",
            "345/345 [==============================] - 13s 37ms/step - loss: 0.7782\n",
            "Epoch 21/30\n",
            "345/345 [==============================] - 13s 36ms/step - loss: 0.7681\n",
            "Epoch 22/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7572\n",
            "Epoch 23/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7505\n",
            "Epoch 24/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7461\n",
            "Epoch 25/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7429\n",
            "Epoch 26/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7391\n",
            "Epoch 27/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7395\n",
            "Epoch 28/30\n",
            "345/345 [==============================] - 13s 36ms/step - loss: 0.7411\n",
            "Epoch 29/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7404\n",
            "Epoch 30/30\n",
            "345/345 [==============================] - 12s 36ms/step - loss: 0.7402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAMmpZALyAoP",
        "colab_type": "text"
      },
      "source": [
        "### Using the Char-RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ-ZR5s1Ucq4",
        "colab_type": "text"
      },
      "source": [
        "Now we have a model that can predict the next character in the text written by Shakespeare. After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, we create an identical model but using batch size 1, and copy the model’s weights to this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhCqwTwBWDMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weigths_model = model.get_weights()\n",
        "\n",
        "model_pred = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1, stateful=False)\n",
        "\n",
        "model_pred.set_weights(model.get_weights())\n",
        "\n",
        "model= model_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3cspfl7WIfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "502306fc-b214-4440-9162-1eef964efa18"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            9984      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 39)             39975     \n",
            "=================================================================\n",
            "Total params: 3,988,263\n",
            "Trainable params: 3,988,263\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA8T2eAryDt6",
        "colab_type": "text"
      },
      "source": [
        "To feed it some text to the model, we first need to preprocess it like we did earlier, so let’s create a little function for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yekvEuneWnvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_text_to_integers(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HilasdpmW3HX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5263817-ad22-40b2-fd5c-1bd58202abee"
      },
      "source": [
        "X_new = encode_text_to_integers([\"How are yo\"])\n",
        "print(f'Input text vectorized: {X_new}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text vectorized: [[ 6  3 16  0  4  8  1  0 15  3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8BbjqPcyInv",
        "colab_type": "text"
      },
      "source": [
        "Now let’s use the model to predict the next letter in some text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTGxkKB-AYcH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8509d294-f3ee-4b80-9280-23c77554dce6"
      },
      "source": [
        "Y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
        "print(tokenizer.sequences_to_texts(Y_pred + 1)[0][-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "u\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sbxTXemyMm3",
        "colab_type": "text"
      },
      "source": [
        "The model guessed right. Now let’s use this model to generate new text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWtyswyjyOM2",
        "colab_type": "text"
      },
      "source": [
        "### Generating Fake Shakespearean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ckBCsy8yQSq",
        "colab_type": "text"
      },
      "source": [
        "To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice, this often leads to the same words being repeated over and over again. Instead, we can pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s `tf.random.categorical()` function. This will generate a more diverse and interesting text. The `categorical()` function samples random class indices, given the class log probabilities (logits). To have more control over the diversity of the generated text, we can divide the logits by a number called the *temperature*, which we can tweak as we wish: a *temperature* close to 0 will favor the high-probability characters, while a very high temperature will give all characters an equal probability. The following `next_char()` function uses this approach to pick the next character to add to the input text:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaqVib2eZ94S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_char(start_string, temperature=1):\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    X_new = encode_text_to_integers([start_string])\n",
        "    predictions = model.predict(X_new)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    temperature = 1.0\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    # Converting our vector prediction to string\n",
        "    return tokenizer.sequences_to_texts([[predicted_id + 1]])[0]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blgug1oXVrxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7b1611b-b659-4458-e84d-560e17269f7c"
      },
      "source": [
        "print(next_char(\"How are yo\", temperature=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "u\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM13EnZoySPC",
        "colab_type": "text"
      },
      "source": [
        "Next, we can write a small function that will repeatedly call `next_char()` to get the next character and append it to the given text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skPR50U9yUJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_text(text, n_chars=200, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO0ihsKByWoK",
        "colab_type": "text"
      },
      "source": [
        "The next figure depicts the procedure for generating text.\n",
        "\n",
        "![](https://i.ibb.co/jJv8B2N/char-RNN-genberating-text.png)\n",
        "\n",
        "We are now ready to generate some text! Let’s try with different temperatures:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICh50_1KyYEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "d904a2b7-bfed-43a1-eb93-b1aa52c6369b"
      },
      "source": [
        "print(complete_text(\"ROMEO: \", temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: no man she does.\n",
            "\n",
            "gloucester:\n",
            "carth gonzalm, go but a higher banished;\n",
            "for 'tis crack'd in a dishest person:\n",
            "there was wait yours.ly, old grain,\n",
            "never come now death is doubt.\n",
            "\n",
            "second citizen:\n",
            "once; f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye4Q_ZITFwdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "4e0f4be1-6765-4384-fdbd-4a7e27e49363"
      },
      "source": [
        "print(complete_text(\"ROMEO: \", temperature=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: gentlemen, with the profition\n",
            "let them heep: 'tis love it will.\n",
            "i make you kindly.\n",
            "\n",
            "marcius:\n",
            "gentle sir, most high-behembed,\n",
            "shall in the other. therefore, not well:\n",
            "regetter you, my sovereign kings, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UviAR4IyZgz",
        "colab_type": "text"
      },
      "source": [
        "Note that the model is character-based, so when training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "Apparently, our Shakespeare model works best at a temperature close to 1. To generate more convincing text, we could try using more GRU layers, train for longer, and add some regularization (for example, we could set `recurrent_dropout=0.3` in the `GRU` layers). Moreover, the model is currently incapable of learning patterns longer than `n_steps`, which is just 100 characters. We could try making this window larger, but it will also make training harder, and even LSTM and GRU cells cannot handle very long sequences. Alternatively, we could use a stateful RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nM9pI2KyesR",
        "colab_type": "text"
      },
      "source": [
        "### Stateful RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lez4c_RYyfgK",
        "colab_type": "text"
      },
      "source": [
        "Until now, we have used only stateless RNNs: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. What if we told the RNN to preserve this final state after processing one training batch and use it as the initial state for the next training batch? This way the model can learn long-term patterns despite only backpropagating through short sequences. This is called a *stateful RNN*. Let’s see how to build one.\n",
        "\n",
        "First, note that a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thing we need to do to build a stateful RNN is to use sequential and nonoverlapping input sequences. We will use the dataset’s `window()` method to convert this long sequence of characters into many smaller windows of text. We set the parameter `shift=n_steps` when calling the `window()` method. Note that the `window()` method creates a dataset that contains windows, each of which is also represented as a dataset. It’s a *nested dataset*, analogous to a list of lists. This is useful when we want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them). However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. So, we must call the `flat_map()` method which converts a nested dataset into a flat dataset (one that does not contain datasets). \n",
        "\n",
        "Moreover, we must obviously not call the `shuffle()` method. Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN. Indeed, if we were to call `batch(32)`, then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these window where it left off. The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64, so if you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest solution to this problem is to just use “batches” containing a single window.\n",
        "\n",
        "Figure 16-2 summarizes the dataset preparation steps discussed so far (showing windows of length 11 rather than 101, and a batch size of 1).\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/xSm5z3F/sequence-fragments-stateful-RNN.png)\n",
        "\n",
        "Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use `tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))` to create proper consecutive batches, where the $n^{\\text{th}}$ input sequence in a batch starts off exactly where the $n^{\\text{th}}$ input sequence ended in the previous batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4ffy4whd3es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(text_as_int, batch_size)#[:train_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKQQHIv0d_yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "274d666e-f72e-4a39-a9a8-e9ffc98c2d33"
      },
      "source": [
        "encoded_part = encoded_parts[0]\n",
        "print(encoded_part.shape)\n",
        "print(encoded_part)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34857,)\n",
            "[19  5  8 ...  7  0 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmM0UCTIeORL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "9490b6d8-b54f-4e3c-9ff7-ea9b2f63e279"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "for element in dataset.take(5):\n",
        "    print(element)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(19, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC3jgJhveaNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "b11d0b7c-656a-4b6d-c133-79691b79641e"
      },
      "source": [
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "for window in dataset.take(2):\n",
        "    print(window)\n",
        "    for element in window.take(2):\n",
        "        print(element)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_VariantDataset shapes: (), types: tf.int64>\n",
            "tf.Tensor(19, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "<_VariantDataset shapes: (), types: tf.int64>\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQv00lJe6Ll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "4077a753-1fde-4b6e-be4d-e43c889ebada"
      },
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "for window_tensor in dataset.take(2):\n",
        "    print(window_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
            "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
            "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
            " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
            " 10 15  3 13  0], shape=(101,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[ 0  4  8  1  0  4 11 11  0  8  1  7  3 11 25  1 12  0  8  4  2  6  1  8\n",
            "  0  2  3  0 12  5  1  0  2  6  4  9  0  2  3  0 19  4 14  5  7  6 29 10\n",
            " 10  4 11 11 23 10  8  1  7  3 11 25  1 12 26  0  8  1  7  3 11 25  1 12\n",
            " 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 19  5  8  7  2 17\n",
            "  0 15  3 13  0], shape=(101,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHvihsx7d-Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = []\n",
        "for encoded_part in encoded_parts:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "    datasets.append(dataset)\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX3gnSSRyj2a",
        "colab_type": "text"
      },
      "source": [
        "Now let’s create the stateful RNN. First, we need to set `stateful=True` when creating every recurrent layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1dxnlTojNTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size = vocab_size, embedding_dim=embedding_dim,\n",
        "                    rnn_units=rnn_units, batch_size=32, stateful=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axWRYYw8k1_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "304dc5f9-4156-4bb6-a473-a555cc380a95"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (32, None, 256)           9984      \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (32, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (32, None, 39)            39975     \n",
            "=================================================================\n",
            "Total params: 3,988,263\n",
            "Trainable params: 3,988,263\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwe74P38Jvkc",
        "colab_type": "text"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![](https://i.ibb.co/6ykYWq6/char-rnn.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HUua7Npyp7s",
        "colab_type": "text"
      },
      "source": [
        "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small callback:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME-AvM06yqOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUs5XvTAysF7",
        "colab_type": "text"
      },
      "source": [
        "And now we can compile and fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2OhuNwIT46y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=loss, optimizer=\"adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Pqa_jPT8F8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9086265-3881-4b22-c2f4-0f8eef912b60"
      },
      "source": [
        "print(f'Number of batches in the dataset: {dataset_size // batch_size // n_steps}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of batches in the dataset: 348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp2mYhEpytb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e03735d-e405-462a-a5f5-476e0fa2f694"
      },
      "source": [
        "history = model.fit(dataset, epochs=30, callbacks=[ResetStatesCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "348/348 [==============================] - 14s 39ms/step - loss: 2.2843\n",
            "Epoch 2/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.6461\n",
            "Epoch 3/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.4416\n",
            "Epoch 4/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.3428\n",
            "Epoch 5/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.2757\n",
            "Epoch 6/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.2202\n",
            "Epoch 7/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.1700\n",
            "Epoch 8/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.1196\n",
            "Epoch 9/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.0696\n",
            "Epoch 10/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 1.0225\n",
            "Epoch 11/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.9819\n",
            "Epoch 12/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.9509\n",
            "Epoch 13/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.9206\n",
            "Epoch 14/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8965\n",
            "Epoch 15/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8797\n",
            "Epoch 16/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8675\n",
            "Epoch 17/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8583\n",
            "Epoch 18/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8506\n",
            "Epoch 19/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8427\n",
            "Epoch 20/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8388\n",
            "Epoch 21/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8377\n",
            "Epoch 22/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8339\n",
            "Epoch 23/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8348\n",
            "Epoch 24/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8345\n",
            "Epoch 25/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8361\n",
            "Epoch 26/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8350\n",
            "Epoch 27/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8399\n",
            "Epoch 28/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8399\n",
            "Epoch 29/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8426\n",
            "Epoch 30/30\n",
            "348/348 [==============================] - 14s 40ms/step - loss: 0.8502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9fPBi7nyv-_",
        "colab_type": "text"
      },
      "source": [
        "As with the stateless model, after training, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, we create an identical model with batch_size 1, and copy the model’s weights to this model. To set the weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMwL00ekZDH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weigths_model = model.get_weights()\n",
        "\n",
        "model_pred = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1, stateful=False)\n",
        "\n",
        "model_pred.set_weights(model.get_weights())\n",
        "\n",
        "model= model_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hegSJD_Aorlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "b6a9c996-10b8-4318-951b-6cab6d06a5f3"
      },
      "source": [
        "print(complete_text(\"ROMEO: \", temperature=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: is thy\n",
            "boneth he shall, my mounding successed?\n",
            "\n",
            "friar laurence:\n",
            "of so die in this cup blast send\n",
            "from my order appetite, by this disguise,\n",
            "fall not a wonder, i through my life\n",
            "and tell him what your q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAUej-msrk1a",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYzxXnkgrmSv",
        "colab_type": "text"
      },
      "source": [
        "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
        "\n",
        "- https://github.com/ageron/handson-ml2\n",
        "\n",
        "- [Text generation TensorFlow](https://www.tensorflow.org/tutorials/text/text_generation)\n",
        "\n",
        "- [RNNs MIT lecture](https://www.youtube.com/watch?v=SEnXr6v2ifU)\n",
        "\n",
        "- [Character-level Recurrent Neural Networks in Practice: Comparing Training and Sampling Schemes](https://arxiv.org/abs/1801.00632)\n",
        "\n",
        "- [Keras examples: Character-level text generation with LSTM](https://keras.io/examples/generative/lstm_character_level_text_generation/)\n",
        "\n",
        "- [Sequence Models course](https://www.coursera.org/lecture/nlp-sequence-models/language-model-and-sequence-generation-gw1Xw)\n",
        "\n",
        "- [Character level language model](https://towardsdatascience.com/character-level-language-model-1439f5dd87fe)\n",
        "\n"
      ]
    }
  ]
}