{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO explained.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0HvmV3ijXXBD",
        "PMwnS1wOn-rf"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNkrJuzMDcXnmf/PvM1JdRj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/YOLO_explained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cAVzPgetSW",
        "colab_type": "text"
      },
      "source": [
        "In previous notebooks we [introduced the CNNs](https://github.com/victorviro/Deep_learning_python/blob/master/Introduction_to_CNNs.ipynb) and we study different [architectures of CNNs](https://github.com/victorviro/Deep_learning_python/blob/master/CNN_Architectures.ipynb) for classification tasks. Later we saw an [introduction to object detection](https://github.com/victorviro/Deep_learning_python/blob/master/Introduction_to_object_detection.ipynb) and some techniques to [improve object detection](https://github.com/victorviro/Deep_learning_python/blob/master/Improving_object_detection.ipynb). Here we are going to go deep into a famous architecture to perform object detection: YOLO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DyYAfQGN9vt",
        "colab_type": "text"
      },
      "source": [
        "# You Only Look Once (YOLO)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnS1PR2rXSO_",
        "colab_type": "text"
      },
      "source": [
        "YOLO is an extremely fast and accurate object detection architecture proposed by Joseph Redmon in a [paper](https://arxiv.org/abs/1506.02640) in 2015, and subsequently improved in 2016 ([YOLOv2](https://arxiv.org/abs/1612.08242)), in 2018 ([YOLOv3](https://arxiv.org/abs/1804.02767)), and recently in 2020 ([YOLOv4](https://arxiv.org/abs/2004.10934)). It is so fast that it can run in realtime on a video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HvmV3ijXXBD",
        "colab_type": "text"
      },
      "source": [
        "## YOLOv1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21C8lV4kXbD0",
        "colab_type": "text"
      },
      "source": [
        "[YOLO](https://arxiv.org/abs/1506.02640) was first introduced in 2016 as a different approach to treat generic object detection problems. The approach is based on training only one CNN. YOLOv1 is composed of 24 convolutional layers for feature extraction and 2 fully connected layers for generating the final output. The total architecture is detailed in Figure 1.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/3zxgn4Y/YOLOv1-arquitecture.png)\n",
        "\n",
        "The input image is divided into a grid of $S \\times S$. Every grid cell can be associated with only one object. The grid cell has also a fixed number B of boundary boxes to be predicted for this object (called anchor boxes or bounding box priors)(for a visual explanation of anchor boxes check this [video](https://www.youtube.com/watch?v=RTlwl2bv0Tg&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=29)). The anchor box dimensions are hand picked. YOLO calculates for every grid cell a vector of class probabilities among the C classes we are targeting. YOLO calculates also for every bounding box of the cell a vector containing 5 parameters: $(x, y, w, h$, box_confidence_score).The box_confidence_score reflects how likely the box contains an object (objectness) and represents the IOU between the predicted box and any ground truth box. The $(x, y)$ coordinates represent the center of the box relative to the bounds of the grid cell (offsets). Once the image is divided into a $S \\times S$ grid, for each object, the grid cell $(g_x,g_y)$ containing the object’s center is located. Having assigned the \"responsibility\" of predicting the object to a grid cell, the center of the bounding box is described as offsets from the cell as shown in the next figure.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/bmk0kmc/object-center-as-offsets.png)\n",
        "\n",
        "\n",
        "\n",
        "The width and height are predicted relative to the whole image (normalized). Therefore, $x, y, w$ and $h$ are all between 0 and 1. Hence, the YOLO network generates for every input image a tensor of the form: $S \\times S \\times (B \\ast 5 + C)$ where:\n",
        "\n",
        "- $S \\times S$: corresponds to the number of grid cells.\n",
        "- B: corresponds to the number of bounding boxes (anchor boxes).\n",
        "- C: corresponds to the number of targeted classes.\n",
        "\n",
        "For evaluating YOLO on dataset PASCAL VOC, authors used S = 7, B = 2. PASCAL VOC has 20 labelled classes so C = 20. The final prediction is a 7 × 7 × 30 tensor.\n",
        "\n",
        "Having assigned the object to a grid cell, the truth vector $y_{(g_x,g_y)}$ requires the predictions $\\hat{y}_{(g_x,g_y)}$ are located at the grid cell in the tensor outputted from the Yolo CNN. As seen in the next figure, each grid cell predicts two bounding boxes with their respective object existence probabilities $P(\\text{Object})$ and a class probability distribution, so each cell only predicts one object and, at prediction time, we select the bounding box with the highest value of $P(\\text{Object})$, which is the probability the box contains an object.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/s25ds3x/output-vector-yolo.png)\n",
        "\n",
        "In the figure, the true object bounding box is denoted as $\\boldsymbol{b}$, the predicted bounding boxes as $\\boldsymbol{\\hat{b}}_1$ and $\\boldsymbol{\\hat{b}}_2$, the true class probability vector as $\\boldsymbol{p}$, the predicted class probability vector as $\\boldsymbol{\\hat{p}}$. The \"confidence\" that box1 and box2, respectively, contain an object (P(Object) for the respective boxes) are denoted by $\\hat{c}_1$ and $\\hat{c}_2$.\n",
        "\n",
        "The true object bounding box $\\boldsymbol{b}$ is assigned to one of box1 or box2 based on which predicted bounding box has the highest Intersection over Union with $\\boldsymbol{b}$. The value c is set to be the maximum IoU. This process results in the truth vector $y_{(g_x,g_y)}$, an example of which is depicted in the previous figure.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To train the YOLO network, they used a combination of three loss functions. The first is the classification loss (loss of the conditional probabilities for every class). The second is the localization loss (the position of the estimated bounding box compared to the ground truth). The third is the confidence loss (the box confidence score compared to the ground truth).\n",
        "The expression of the loss is detailed in the paper. To measure the error between the predicted value and the ground truth, YOLO uses sum-squared error as a metric. \n",
        "\n",
        "Just like in training, predicting detections for a test image only requires one network evaluation. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections.\n",
        "\n",
        "When it was introduced, YOLO surpassed other object detection algorithms in terms of speed. Its mAP (mean Average Precision) was comparable or exceeded the mAP of other state-of-the-art algorithms. However the model had some limitations. For example, each grid cell only predicts two boxes and can only have one class. This constraint limits the number of nearby objects that the model can predict. Hence, the model struggles with small objects that appear in groups, such as flocks of birds. Other limitation is that the loss function treats errors the same in small bounding boxes versus large bounding boxes.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/N7XyzWq/yolov1-ex.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjA9XMbLXZCN",
        "colab_type": "text"
      },
      "source": [
        "## YOLOv2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtQis_FFa8Qp",
        "colab_type": "text"
      },
      "source": [
        "In [YOLOv2](https://arxiv.org/abs/1612.08242), many improvements had been introduced to increase the accuracy and decrease the processing time. Among them, we can note:\n",
        "\n",
        "- [**Batch Normalization**](https://github.com/victorviro/Deep_learning_python/blob/master/Vanishing_Exploding_gradients_problem_DNNs.ipynb) (BN): BN was added to all convolutional layers, which improved the mAP by 2%. Dropout was removed from the model without overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **Convolution with anchor boxes**:\n",
        "\n",
        " - YOLOv1 predicts the coordinates of bounding boxes directly using fully connected layers on top of the convolutional feature extractor. In YOLOv2 the fully connected layers are removed and anchor boxes are used to predict bounding boxes. In Yolo v1, each grid cell in the last layer can predict just one object because, while each grid cell gives us a choice between two bounding boxes, we only have one class probability vector. In Yolo v2, the class prediction is moved from the level of the grid cell to the level of the boundary box (the model predicts class for every anchor box for each grid cell). Thus, the YOLOv2 network generates for every input image a tensor of the form: $S \\times S \\times (B \\ast (5 + C)$). Each prediction includes 4 parameters for the boundary box, 1 box confidence score (objectness) and C class probabilities. If we use C = 20 and B = 5, we will have $25\\ast25=125$ parameters per grid cell. Same as YOLOv1, the objectness prediction still predicts the IOU of the ground truth and the proposed box, and the class predictions predict the conditional probability of that class given that there is an object. YOLOv2 uses 5 anchor boxes, but that makes creating diagrams and notation painful, so it is limited to 2 in the next figure and a $7 \\times 7$ grid is used for the same reason.\n",
        "\n",
        "    ![texto alternativo](https://i.ibb.co/Ws9VtH2/yolov2-prediction.png)\n",
        "\n",
        "\n",
        "\n",
        " - The input image size is changed from $448 \\times 448$ to $416 \\times 416$. This creates an odd number spatial dimension ($7 \\times 7$ v.s. $8 \\times 8$ grid cell). The center of a picture is often occupied by a large object. With an odd number grid cell, it is more certain on where the object belongs.\n",
        "\n",
        "\n",
        " - One pooling layer is removed to make the output of the network’s convolutional layers higher resolution (to $13 \\times 13$ instead of $7 \\times 7$).\n",
        "\n",
        " - **High-resolution classifier**: The YOLOv1 training composes of 2 phases. It trains the classifier network at $224 \\times 224$ and increases the resolution to 448 for detection. YOLOv2 starts with $224 \\times 224$ pictures for the classifier training but then retune the classifier again with $448 \\times 448$ pictures using 10 epochs. This makes the detector training easier and moves mAP up by 4%. To fine tune the resulting network for detection, they replaced the last convolution layer with three $3 \\times 3$ convolutional layers each outputting 1024 output channels. Then a final $1 \\times 1$ convolutional layer is added to convert the $7 \\times 7 \\times 1024$ output into $7 \\times 7 \\times 125$.\n",
        "\n",
        "- **Dimension Clusters**:The use of K-means clustering algorithm to select the best anchor box from the training set of truth bounding boxes. Since we are dealing with boundary boxes rather than points, we cannot use the regular spatial distance to measure datapoint distances like Euclidian distance. No surprise, they used IoU.\n",
        "\n",
        "- **Direct location prediction**: YOLOv1 predicts the width and height of a bounding box directly. Yolo v2 predicts width and height offsets relative to an anchor box. The center coordinates for each bounding box prediction remain the same as in YOLOv1. YOLOv1 does not have constraints on location prediction which makes the model unstable at early iterations. The predicted bounding box can be far from the original grid location. Thus, YOLOv2 predicts $(t_x, t_y, t_w, t_h, t_o)$ and applies the sigma function to constraint its possible offset range. This makes the network converge better.\n",
        "\n",
        "\n",
        "  ![texto alternativo](https://i.ibb.co/bBm7TDj/constrained-BBox-prediction.png)\n",
        "\n",
        "- \n",
        " - $(c_x, c_y)$ is the location of the grid.\n",
        "\n",
        " - $(b_x, b_y)$ is the location of bounding box.\n",
        "\n",
        " - $(p_w, p_h)$ is the anchor box prior got from clustering.\n",
        "\n",
        " - $(b_w, b_h)$ is the bounding box dimensions: $(p_w, p_h)$ scaled by $(t_w, t_h)$.\n",
        "\n",
        "- **Fine-grained features**: Similarly to the identity mapping in ResNet, YOLOv2 concatenates low resolution features to high resolution features to improve the ability to detect small objects. It reshapes the $26 \\times 26 \\times 512$ layer to $13 \\times 13 \\times 2048$. Then it concatenates with the original $13 \\times 13 \\times 1024$ output layer. Now they apply convolution filters on the new $13 \\times 13 \\times 3072$ layer to make predictions. This improves the mAP by 1%.\n",
        "\n",
        "- **Multi-Scale Training**: The original YOLO uses an input resolution of $448 \\times 448$. With the addition of anchor boxes they changed the resolution to $416 \\times 416$. However, since the model only uses convolutional and pooling layers it can be resized on the fly. Instead of fixing the input image size they change the network every few iterations. Every 10 batches the network randomly chooses a new image dimension size. Since the model downsamples by a factor of 32, we pull from the following multiples of 32: {320, 352, ..., 608}. They resize the network to that dimension and continue training. This acts as data augmentation and forces the network to predict well for different input image dimension and scale. \n",
        "\n",
        "\n",
        "- The **loss function** used in YOLOv2 is not explicitly described in the paper, but we can infer from the YOLOv1 loss function. While there are now multiple object predictions per grid cell, YOLOv2 still performs a max-IOU matching of truth to predicted bounding box. It is expected that the bounding box coordinate loss is still a weight linear regression loss. However, the Yolo v3 tech report mentions using binary cross entropy loss for the class predictions and Yolo v2 mentions classification loss, which we infer to mean not regression loss, so Yolo v2 probably uses binary cross entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0fUU_5m13q5",
        "colab_type": "text"
      },
      "source": [
        "Some improvements had been introduced to speed performance. Most detection frameworks rely on VGG-16 as the base feature extractor. VGG-16 is a powerful, accurate classification network (see notebook [CNNs architectures](https://github.com/victorviro/Deep_learning_python/blob/master/CNN_Architectures.ipynb)) but it is needlessly complex. VGG16 requires 30.69 billion floating point operations for a single pass over a $224 \\times 224$ image versus 8.52 billion operations for a customized GoogLeNet. We can replace the VGG16 with the customized GoogLeNet. YOLOv1 leverages GoogeLeNet architecture but pays a price on the top-5 accuracy for ImageNet: accuracy drops from 90.0% to 88.0%.\n",
        "\n",
        "- **DarkNet**: Similar to the VGG models, DarkNet uses mostly $3 \\times 3$ filters and double the number of channels after every pooling step. Following the work on Network in Network (NIN) it uses global average pooling to make predictions as well as $1 \\times 1$ to reduce output channels. It uses batch normalization to stabilize training, speed up convergence, and regularize the model. The final model, called *Darknet-19*, has 19 convolutional layers and 5 maxpooling layers.  Darknet-19 only requires 5.58 billion operations to process an image yet achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet.\n",
        "\n",
        " ![texto alternativo](https://i.ibb.co/6Rp1zmJ/darknet-19-arquitecture-png.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TtfijyF3hN1",
        "colab_type": "text"
      },
      "source": [
        "**YOLO9000**: In the paper, authors also trained a model using this object detector on more than 9000 classes from ImageNet as well as detection data from COCO. This model uses hierarchical classification: the model predicts a probability for each node in a visual hierarchy called Word‐Tree. This makes it possible for the network to predict with high confidence that an image represents, say, a dog, even though it is unsure what specific type of dog it is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKE-NBYUbHGi",
        "colab_type": "text"
      },
      "source": [
        "## YOLOv3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs4bGJlKbKN9",
        "colab_type": "text"
      },
      "source": [
        "In April 2018, the [YOLOv3](https://arxiv.org/abs/1804.02767) was introduced as an incremental improvement to the previous versions. Among the improvements made, we can note:\n",
        "\n",
        "- **Class prediction**: The use of the multi-label classification. Instead of the mutual exclusive labeling in the previous versions, YOLOv3 uses a logistic classifier to estimate the likeliness of the object being of a specific label (using a softmax imposes the assumption that each box has exactly one class which is often not the case, for example, if we have classes like person and women). During training, the binary cross-entropy loss is used for the class predictions.\n",
        "\n",
        "- **Bounding box prediction & cost function calculation**: Following YOLOv2 this network predicts bounding boxes using dimension clusters as anchor boxes. The network predicts 4 coordinates for each bounding box, $t_x, t_y, t_w, t_h$ using the sum of squared error loss during training. The objectness score is also predited for each bounding box using logistic regression. YOLOv3 changes the way in calculating the cost function. During the training, the objectness score 1 is associated with the bounding box prior (anchor) that best overlaps the ground truth object. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold (default 0.5) the prediction is ignored. Each ground truth object is associated with only one boundary box prior. If a bounding box prior is not assigned to a ground truth object, it incurs no classification and localization lost, just confidence loss on objectness.\n",
        "\n",
        "- **Feature extractor**: As we saw, YOLOv2 used a custom deep architecture darknet-19, an originally 19-layer network supplemented with 11 more layers for object detection. With a 30-layer architecture, YOLOv2 often struggled with small object detections. This was attributed to loss of fine-grained features as the layers downsampled the input. To remedy this, YOLOv2 used an identity mapping, concatenating feature maps from a previous layer to capture low level features. However, YOLO v2’s architecture was still lacking some of the most important elements that are now staple in most of state-of-the art algorithms. No residual blocks, no skip connections and no upsampling. YOLOv3 incorporates all of these. First, YOLOv3 uses a variant of Darknet as the feature extractor, which originally has 53 layer network trained on Imagenet. For the task of detection, 53 more layers are stacked onto it, giving us a 106 layer fully convolutional underlying architecture for YOLOv3. This is the reason behind the slowness of YOLO v3 compared to YOLOv2.  The new feature extractor, called Darknet-53, has 53 layers and uses skip connection similarly to ResNet. It uses both $3 \\times 3$ and $1 \\times 1$ convolutions. It gave the state of the art accuracy but with better speed and fewer computations. Here is how the architecture of YOLOv3 looks like.\n",
        "\n",
        "![texto alternativo](https://www.researchgate.net/profile/Wen_Liu121/publication/335228064/figure/fig2/AS:793166898819072@1566117133057/The-framework-of-YOLOv3-neural-network-for-ship-detection.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRoHrrO6W0Pf",
        "colab_type": "text"
      },
      "source": [
        "- **Predictions across scales**: In YOLOv3, the prediction is done for one grid cell at 3 different scales (the detection is done by applying $1 \\times 1$ detection kernels on feature maps of three different sizes at three different places in the network). This was inspired by the Feature Pyramid Networks (FPN) (see notebook [improving object detection](https://github.com/victorviro/Deep_learning_python/blob/master/Improving_object_detection.ipynb)). From the base feature extractor we add several convolutional layers. \n",
        "The last of these layers predicts a 3-d tensor encoding bounding box, objectness, and class predictions. We predict $B=3$ boxes (anchor boxes) at each scale for each grid cell so the tensor is then: $S \\times S \\times (B \\ast (5 + C))$ where:\n",
        "\n",
        " - $S \\times S$: corresponds to the number of grid cells.\n",
        " - $B$: corresponds to the number of anchor boxes at each scale ($B=3$)\n",
        " - $C$: corresponds to the number of targeted classes ($C=80$ for COCO).\n",
        "\n",
        " ![texto alternativo](https://i.ibb.co/v4Hf9SB/yolov3-prediction.png)\n",
        "\n",
        " Yolo v3 merges earlier layers in the feature extractor network with later layers (the extra CNN layers), which is essentially what FPNs do. Intuitively, small objects are more easily detected in high resolution early layers than in the significantly, subsampled low resolution later layers, but the early layers of a CNN contain semantically weak features, so rather than use them directly, FPNs merge them with upsampled later layers that contain semantically strong features.\n",
        "\n",
        " ![texto alternativo](https://i.ibb.co/T0HQLs8/fpn-yolov3.png)\n",
        "\n",
        "\n",
        " We can see from the previous figure that Yolov3, unlike FPNs, uses concatenation instead of summation to merge layers and, while not mentioned, Yolov3 probably upsamples the same way as FPN (using nearest neighbor). In addition the Yolov3 structure isn’t quite the same as the FPN, since Yolov3 doesn’t use the result of previous merges to produce the next detection tensor. To be more clear on the differences, we produced a diagram of the interpretation of what Yolov3 would have looked like if it followed FPN structure more closely.\n",
        "\n",
        " ![texto alternativo](https://i.ibb.co/LNz2tPM/fpn2-yolov3.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGtTUvdqaOaL",
        "colab_type": "text"
      },
      "source": [
        "## YOLOv4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iHDf7_XaQ_q",
        "colab_type": "text"
      },
      "source": [
        "Modern object detectors are usually composed of two parts, a backbone which is usually pre-trained on ImageNet (or in a similar dataset) and a head which is used to predict classes and bounding boxes of objects. The backbone refers to the network which takes as input the image and extracts the features maps. It could be VGG, ResNet, ResNeXt, etc. The head part is usually categorized into two kinds, i.e., one-stage object detector and two-stage object detector. One-stage object detectors takes only one forward propagation pass through the network to make the predictions and they are usually faster than two-stage detectors but they usually have less accuracy. The most representative one-stage object detectors are *You Only Look Once* ([YOLOv1](https://arxiv.org/abs/1506.02640), [YOLOv2](https://arxiv.org/abs/1612.08242), [YOLOv3](https://arxiv.org/abs/1804.02767), [YOLOv4](https://arxiv.org/abs/2004.10934)), *Single Shot MultiBox Detector* ([SSD](https://arxiv.org/abs/1512.02325)) ,and [RetinaNet](https://arxiv.org/abs/1708.02002). As for two-stage object detector, the most representative models are the [R-CNN](https://arxiv.org/abs/1311.2524) series (including [fast R-CNN](https://arxiv.org/abs/1504.08083), [faster R-CNN](https://arxiv.org/abs/1506.01497) and [R-FCN](https://arxiv.org/abs/1605.06409)). Object detectors developed in recent years often insert some layers between backbone and head, and these layers are usually used to collect feature maps from different stages. We can call it the *neck* of an object detector. Usually, a neck is composed of several bottom-up paths and several top-down paths. Networks equipped with this mechanism include *Feature Pyramid Network* ([FPN](https://arxiv.org/abs/1612.03144)) or *Path Aggregation Network* ([PAN](https://arxiv.org/abs/1803.01534)).\n",
        "\n",
        "\n",
        "To sum up, object detectors tipically are compose of several components:\n",
        "\n",
        "\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/LN2vn7q/object-detection-flow.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065hpPZbOPmK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "[YOLOv4](https://arxiv.org/abs/2004.10934) was introduced in 2020. Several architectural design candidates were shortlisted for the YOLOv4 model generation.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/Vw3sLBQ/object-detector-arquitecture.png)\n",
        "\n",
        "The baseline architecture used in YOLOv4 is shown in the next figure:\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/n1sFfvX/baseline-architecture-YOLOv4.png)\n",
        "\n",
        "Apart from different approaches in architecture designs, the authors also added two new “Bags” or optimization procedures to be used at the time of training and inference. Improvements made in the training process to advance accuracy without impact on inference speed (like data augmentation, class imbalance, cost function, soft labeling, etc)  are called *Bag of Freebies* (BoG). Another methods, called *Bag of Specials* (BoS), impacts the inference time slightly with a good return in performance. These improvements include the increase of the receptive field, the use of attention, feature integration like skip-connections & FPN, and post-processing like non-maximum suppression. BoF and BoS was filled with different approaches used for both, backbone and detector modules. The most of these techniches are explained in the notebook [Improving object detection](https://github.com/victorviro/Deep_learning_python/blob/master/Improving_object_detection.ipynb). The next figure show which of this methods was choosen in the design of YOLOv4:\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/XXW0R52/bag-freebies-and-specials-YOLOv4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xPiczO7QyWI",
        "colab_type": "text"
      },
      "source": [
        "### Backbone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo9p43zaQ03S",
        "colab_type": "text"
      },
      "source": [
        "We have seen the DenseNet architecture ([notebook CNN architectures](https://github.com/victorviro/Deep_learning_python/blob/master/CNN_Architectures.ipynb)) which allows us to build much deeper networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq7eqqkMXhkt",
        "colab_type": "text"
      },
      "source": [
        "#### Cross-Stage-Partial-connections (CSP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcXr0aV2XjUQ",
        "colab_type": "text"
      },
      "source": [
        "[CSPNet](https://arxiv.org/abs/1911.11929) separates the input feature maps of the DenseBlock into two parts. The first part bypasses the DenseBlock (is used directly into the concatenation with the final output of the  Transition Block (TB)) and becomes part of the input to the next transition layer. The second part will go thought the Dense block (is used as an input in the Dense Block (DB)) as below.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/fYhvgJC/Densenet-CSPDensenet.png)\n",
        "\n",
        "This new design reduces the computational complexity by separating the input into two parts, with only one going through the Dense Block (decrease the number of multiplications in Dense Block).\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/VjH5KqM/CSPDense-Net.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od1YgLFdYrqw",
        "colab_type": "text"
      },
      "source": [
        "#### CSPDarknet53"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amQd1-ZsYs9p",
        "colab_type": "text"
      },
      "source": [
        "YOLOv4 uses the CSP connections above with the Darknet-53 of YOLOv3 below as the backbone in feature extraction. The CSPDarknet53 model has higher accuracy in object detection compared with ResNet based designs even they have a better classification performance. But the classification accuracy of CSPDarknet53 can be improved with Mish and other techniques discussed later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1c_P2SmFNyg",
        "colab_type": "text"
      },
      "source": [
        "#### Mish activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmmSSl5CFQOp",
        "colab_type": "text"
      },
      "source": [
        "ReLU (Rectified Linear Unit) is one of the most famous activation functions. It’s easy differentiability and zero cost are the biggest advantages that make them the first choice. As we discussed [here](https://github.com/victorviro/Deep_learning_python/blob/master/Vanishing_Exploding_gradients_problem_DNNs.ipynb) RELU suffers from a problem known as the *dying ReLUs*.\n",
        "\n",
        "Mish is a novel activation function similar to Swish and is defined as\n",
        "\n",
        "$$f(x)=x\\text{tanh}(\\text{softplus(x)})=x\\text{tanh}(\\text{ln}(1+e^x))$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Y5UqlAHWS6",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "4e8250fa-fade-4c7f-a2dc-2595bece2a10"
      },
      "source": [
        "#@title\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def mish(z, alpha=0.01):\n",
        "    return z*np.tanh(np.log(1+np.exp(z)))\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(z, mish(z, 0.05), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
        "plt.grid(True)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.title(\"Mish activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.5, 4.2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmzEu5fKIiuZ",
        "colab_type": "text"
      },
      "source": [
        "The properties of Mish activation function are:\n",
        "\n",
        "- *Non-monotonic function*: Preserves negative values, that stabilize the network gradient flow and unlike ReLU, and almost solving Dying ReLU problem and helps to learn more expressive features.\n",
        "\n",
        "- *Unboundedness and Bounded Below*: Former helps to remove the saturation problem of the output neurons and the latter helps in better regularization of networks.\n",
        "\n",
        "- *Infinite Order of Continuity*: Unbiased towards initialization of weights and learning rate due to the smoothness of a function helping for the better generalizations.\n",
        "\n",
        "- High compute function but increases accuracy: Although being high-cost function, it has proven itself better in deep layers in comparison to ReLU.\n",
        "\n",
        "- *Scalar Gating*: Scalar Gating is an important property of this function and so it becomes logical and can easily replace the pointwise functions like ReLU.\n",
        "\n",
        "\n",
        "Using Mish as an activation function in YOLOv4 showed decent amount of accuracy gains. Mish Activation + CSPDarknet53 combo gave the best results in the study mentioned in the paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpCaUz-TExTZ",
        "colab_type": "text"
      },
      "source": [
        "#### Bag of freebies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz7-ak8ZEz2E",
        "colab_type": "text"
      },
      "source": [
        "The bag of freebies used in the backbone are class label smothing, Mosaic and Cutmix data augmentation techniques, and Dropblock regularization. These methods are explained in the notebook [Improving object detection](https://github.com/victorviro/Deep_learning_python/blob/master/Improving_object_detection.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsHHkPIRZTW8",
        "colab_type": "text"
      },
      "source": [
        "### Neck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mruxYU-InCfP",
        "colab_type": "text"
      },
      "source": [
        "#### SPP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6ICdFQ9nGDP",
        "colab_type": "text"
      },
      "source": [
        "Several modules can be used to enhance receptive field like Spatial Pyramid Pooling ([SPP](https://arxiv.org/abs/1406.4729)), Atrous Spatial Pyramid Pooling ([ASPP](https://arxiv.org/abs/1606.00915)), and Receptive Field Block Net ([RFB](https://arxiv.org/abs/1711.07767)). This modules are explained in the notebook [Improving object detection](https://github.com/victorviro/Deep_learning_python/blob/master/Improving_object_detection.ipynb).\n",
        "\n",
        "YOLOv4 uses SPP. However, conventional SPP generates 1-D output, which cannot be used in FCNs like YOLO. Therefore, before passing the output feature map to YOLO-Head, it is passed to different convolutional blocks of kernel size $1\\times1$, $3\\times3$, $9\\times9$, and $13\\times13$ to increase the receptive field and capture different object patterns at different scales. These are concatenated and then passed to the final prediction stage or YOLO Head.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/85pW2C4/yolov4-SPP.png)\n",
        "\n",
        "In YOLOv4, SPP is used as an extra module to increase the receptive field. Backbone is comprised CSPDarknet53. So this module is used after the ((DB+TB)+TB) blocks i.e after partial connections as discussed in previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7caoMlcnMM3",
        "colab_type": "text"
      },
      "source": [
        "#### PANet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXZo-45avcSU",
        "colab_type": "text"
      },
      "source": [
        "Collecting feature maps from different stages can help to recognize objects at different scales. *Feature Pyramid Network* ([FPN](https://arxiv.org/abs/1612.03144)) or *Path Aggregation Network* ([PAN](https://arxiv.org/abs/1803.01534)) are modules specifically designed to this purpose. This modules are explained in the notebook [Improving object detection](https://github.com/victorviro/Deep_learning_python/blob/master/Improving_object_detection.ipynb)..\n",
        "\n",
        "In YOLOv4, a modified version of PAN is used for feature aggregation. Instead of elementwise addition, concatenation approach is used between every bottom-up layer. This helps in conserving the missed out or FPN+Bottom Up features at the same time. Of course, this will increase the computational power.\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/THZF8ZF/PAN-vs-PAN-YOLOv4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNlovMO41pzm",
        "colab_type": "text"
      },
      "source": [
        "### Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "360XIlBn1r3y",
        "colab_type": "text"
      },
      "source": [
        "The head of YOLOv4 is the same as in YOLOv3.\n",
        "\n",
        "- The bag of freebies used in the detector are CIoU loss, cross Minibatch normalization (CmBN), DropBlock regularization, mosaic data augmentation and self adversarial training.\n",
        "\n",
        "- The bag os specials used in the the detector are the mish activation function, Spatial Pyramid Pooling, Spatial Attention Module, Path Aggregation Networks and DIoU-NMS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMwnS1wOn-rf",
        "colab_type": "text"
      },
      "source": [
        "## PP-YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDRc4IpCoAVf",
        "colab_type": "text"
      },
      "source": [
        "In August 2020 researches of company Baidu have published a paper which intruce a new design of this popular architecture called [PP-YOLO](https://arxiv.org/abs/2007.12099). PP is short for PaddlePaddle, a deep learning framework written by Baidu.\n",
        "\n",
        "\n",
        "\n",
        "I encourage you to go ahead and read the paper and previous ones: they are quite pleasant to read, and it is an excellent example of how Deep Learning systems can be incrementally improved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk-WuyB0iZ3C",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ElD51Iib_P",
        "colab_type": "text"
      },
      "source": [
        "- [YOLO Paper](https://arxiv.org/abs/1506.02640), [YOLOv2 Paper](https://arxiv.org/abs/1612.08242), [YOLOv3 Paper](https://arxiv.org/abs/1804.02767), [YOLOv4 Paper](https://arxiv.org/abs/2004.10934), [PP-YOLO Paper](https://arxiv.org/abs/2007.12099)\n",
        "\n",
        "- [Course CNN deeplearning.ai.](https://www.youtube.com/watch?v=9s_FpMpdYW8&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=30)\n",
        "\n",
        "- [Yolo Object Detectors](https://medium.com/oracledevs/final-layers-and-loss-functions-of-single-stage-detectors-part-1-4abbfa9aa71c)\n",
        "\n",
        "- [Real-time Object Detection](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n",
        "\n",
        "- [What’s new in YOLO v3?](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)\n",
        "\n",
        "- [BoF and BoS in YOLOv4](https://medium.com/@jonathan_hui/yolov4-c9901eaa8e61?source=---------4------------------)\n",
        "\n",
        "\n",
        "- [YOLOv4 introduction](https://medium.com/visionwizard/yolov4-version-0-introduction-90514b413ccf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}