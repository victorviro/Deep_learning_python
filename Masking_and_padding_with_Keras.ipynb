{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masking and padding with Keras.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Masking_and_padding_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EBOoEdK9xfGx"
      },
      "source": [
        "# Masking and padding with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GvYwBa0gxuFE"
      },
      "source": [
        "\n",
        "\n",
        "**Masking** is a way to tell sequence-processing layers that certain timesteps\n",
        "in an input are missing, and thus should be skipped when processing the data.\n",
        "\n",
        "**Padding** is a special form of masking where the masked steps are at the start or at\n",
        "the beginning of a sequence. Padding comes from the need to encode sequence data into\n",
        "contiguous batches: in order to make all sequences in a batch fit a given standard\n",
        "length, it is necessary to pad or truncate some sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS_ywm4fhFv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d2yedquhANr",
        "colab_type": "text"
      },
      "source": [
        "## Padding sequence data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjh9hFUChA2y",
        "colab_type": "text"
      },
      "source": [
        "When processing sequence data, it is very common for individual samples to have different lengths. Consider the following example (text tokenized as words):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k59thP0ghGlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "  [\"Hello\", \"world\", \"!\"],\n",
        "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
        "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-jkyhF9hWKp",
        "colab_type": "text"
      },
      "source": [
        "Let's vectorized the data as integers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SZUANqlUhKuZ",
        "colab": {}
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sentences_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "sentences_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v4OQF72TIQ5Z"
      },
      "source": [
        "The data is a nested list where individual samples have length 3, 5, and 6, respectively. Since the input data for a deep learning model must be a single tensor (of shape e.g. `(batch_size, 6, vocab_size)` in this case), samples that are shorter than the longest item need to be padded with some value (alternatively,\n",
        "one might also truncate long samples before padding short samples).\n",
        "\n",
        "Keras provides a utility function to truncate and pad Python lists to a common length:\n",
        "`tf.keras.preprocessing.sequence.pad_sequences`. By default, this will pad using 0s althoug it is configurable via the `value` parameter. Note that we could \"pre\" padding (at the beginning) or\n",
        "\"post\" padding (at the end). Keras recommends using \"post\" padding when working with RNN layers\n",
        "(in order to be able to use the CuDNN implementation of the layers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRSRhyWOTjEe",
        "colab": {}
      },
      "source": [
        "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sentences_encoded, padding=\"post\")\n",
        "print(padded_inputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wzpJh73Dt4Q7"
      },
      "source": [
        "## Masking\n",
        "\n",
        "Now that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is **masking**.\n",
        "\n",
        "There are three ways to introduce input masks in Keras models:\n",
        "\n",
        "- Add a `keras.layers.Masking` layer.\n",
        "- Configure a `keras.layers.Embedding` layer with `mask_zero=True`.\n",
        "- Pass a `mask` argument manually when calling layers that support this argument (e.g.\n",
        "RNN layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNU8i2f5nohw",
        "colab_type": "text"
      },
      "source": [
        "## Mask-generating layers: `Embedding` and `Masking`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gy7L4s70ZqpQ"
      },
      "source": [
        "Under the hood, these layers will create a mask tensor (2D tensor with shape `(batch_size, sequence_length)`), and attach it to the tensor output returned by the `Masking` or `Embedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0o8DBr9Q95D",
        "colab": {}
      },
      "source": [
        "vocab_size=5000\n",
        "embedding_dim = 16\n",
        "embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)\n",
        "masked_output = embedding(padded_inputs)\n",
        "print(masked_output._keras_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITbiIiPKnzFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "masking_layer = layers.Masking()\n",
        "# Simulate the embedding lookup by expanding the 2D input to 3D, with embedding dimension of 10.\n",
        "unmasked_embedding = tf.cast(\n",
        "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32)\n",
        "masked_embedding = masking_layer(unmasked_embedding)\n",
        "print(masked_embedding._keras_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lBPHoQeUnNBk"
      },
      "source": [
        "As we can see from the printed result, the mask is a 2D boolean tensor with shape `(batch_size, sequence_length)`, where each individual `False` entry indicates that the corresponding timestep should be ignored during processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXccsIhOootT",
        "colab_type": "text"
      },
      "source": [
        "## Mask propagation in the Functional API and Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mPhq3RfDAtPB"
      },
      "source": [
        "When using the Functional API or the Sequential API, a mask generated by an `Embedding` or `Masking` layer will be propagated through the network for any layer that is capable of using them (for example, RNN layers). Keras will automatically fetch the\n",
        "mask corresponding to an input and pass it to any layer that knows how to use it.\n",
        "\n",
        "For instance, in the following Sequential model, the `LSTM` layer will automatically receive a mask, which means it will ignore padded values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4dLZUGISFVAW",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True), layers.LSTM(32),]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cJ0sneMbeyDy"
      },
      "source": [
        "This is also the case for the following Functional API model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sRIRwwVv8R8D",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "outputs = layers.LSTM(32)(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t439AapvLGbE"
      },
      "source": [
        "## Passing mask tensors directly to layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UB8jymZONw2m"
      },
      "source": [
        "Layers that can handle masks (such as the `LSTM` layer) have a `mask` argument in their `__call__` method.\n",
        "\n",
        "Meanwhile, layers that produce a mask (e.g. `Embedding`) expose a `compute_mask(input, previous_mask)` method which we can call.\n",
        "\n",
        "Thus, we can pass the output of the `compute_mask()` method of a mask-producing layer to the `__call__` method of a mask-consuming layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R5pIXpXd8BFp",
        "colab": {}
      },
      "source": [
        "class MyLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyLayer, self).__init__(**kwargs)\n",
        "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)\n",
        "        self.lstm = layers.LSTM(32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        # Note that you could also prepare a `mask` tensor manually.\n",
        "        # It only needs to be a boolean tensor\n",
        "        # with the right shape, i.e. (batch_size, timesteps).\n",
        "        mask = self.embedding.compute_mask(inputs)\n",
        "        output = self.lstm(x, mask=mask)  # The layer will ignore the masked values\n",
        "        return output\n",
        "\n",
        "\n",
        "layer = MyLayer()\n",
        "x = np.random.random((32, 10)) * 100\n",
        "x = x.astype(\"int32\")\n",
        "layer(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqVNyHM6fmRU",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "- [pad_sequences method in Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)\n",
        "\n",
        "- [Masking layer in Keras](https://keras.io/api/layers/core_layers/masking/)\n",
        "\n",
        "- [Masking and padding with Keras](https://www.tensorflow.org/guide/keras/masking_and_padding)\n",
        "\n",
        "- [Padding and masking sequence data Coursera](https://www.coursera.org/lecture/customising-models-tensorflow2/coding-tutorial-padding-and-masking-sequence-data-4cbXR)\n",
        "\n",
        "- [Python keras.layers.Masking() Examples](https://www.programcreek.com/python/example/89671/keras.layers.Masking)"
      ]
    }
  ]
}