{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanishing/Exploding_gradients_problem_DNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+4Cv9CGe9Eye3WZDcSYAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Vanishing_Exploding_gradients_problem_DNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbPQg63Gl1uC",
        "colab_type": "text"
      },
      "source": [
        "# Training deep neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZAK0DeJl1w3",
        "colab_type": "text"
      },
      "source": [
        "[Here](https://github.com/victorviro/Deep_learning_python/blob/master/Introduction_artificial_neural_networks_keras.ipynb) we introduced artificial neural networks and trained our first deep neural networks. But they were shallow nets, with just a few hidden layers. What if you need to tackle a complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN, perhaps with 10 layers or many more, each containing hundreds of neurons, linked by hundreds of thousands of connections. Training a deep DNN isn’t a walk in the park. Here are some of the problems you could run into:\n",
        "\n",
        "- You may be faced with the tricky *vanishing gradients* problem or the related *exploding gradients* problem. This is when the gradients grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training. Both of these problems make lower layers very hard to train.\n",
        "\n",
        "- You might not have enough training data for such a large network, or it might be too costly to label.\n",
        "\n",
        "- Training may be extremely slow.\n",
        "\n",
        "- A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances or if they are too noisy.\n",
        "\n",
        "In this notebook, we will explore the vanishing and exploding gradients problems and some of their most popular solutions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pKNU-N4l10Q",
        "colab_type": "text"
      },
      "source": [
        "## The vanishing/exploding gradients problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Lc4rC0l1ra",
        "colab_type": "text"
      },
      "source": [
        "The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
        "\n",
        "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers connection weights virtually unchanged, and training never converges to a good solution. We call this the *vanishing gradients* problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the *exploding gradients* problem, which surfaces in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
        "\n",
        "This unfortunate behavior was observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a [paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by Xavier Glorot and Yoshua Bengio in 2010. The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks).\n",
        "\n",
        "Looking at the logistic activation function in the next figure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB-jIeJ8LvE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "cellView": "form",
        "outputId": "f623962e-3729-4b04-f682-12513037cdc2"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def logit(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [1, 1], 'k--')\n",
        "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
        "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
        "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.2, 1.2])\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TokNHHFRMXmn",
        "colab_type": "text"
      },
      "source": [
        "You can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDyU8JpfBgsR",
        "colab_type": "text"
      },
      "source": [
        "### Glorot and He initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_8ZkalSBh9B",
        "colab_type": "text"
      },
      "source": [
        "In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction. It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the *fan-in* and *fan-out* of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in the next equation:\n",
        "\n",
        "Normal distribution with mean 0 and variance $\\sigma^2=\\frac{1}{\\text{fan}_{\\text{avg}}}$\n",
        "\n",
        "Or a uniform distribution between $−r$ and $+r$, with $r=\\sqrt{\\frac{3}{\\text{fan}_{\\text{avg}}}}$\n",
        "\n",
        "where $\\text{fan}_{\\text{avg}}=\\frac{(\\text{fan}_{\\text{in}}+\\text{fan}_{\\text{out}})}{2}$.\n",
        "\n",
        "This initialization strategy is called *Xavier/Glorot initialization*.\n",
        "\n",
        "If you replace $\\text{fan}_{\\text{avg}}$ with $\\text{fan}_{\\text{in}}$ in the equation, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it *LeCun initialization*. This initialization is equivalent to Glorot initialization when $\\text{fan}_{\\text{in}}=\\text{fan}_{\\text{out}}$. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning (it is usually used with the logistic, tanh and softmax activation functions).\n",
        "\n",
        "But where did those formulas come from?\n",
        "Suppose we have an input $X$ with $n$ components and a linear neuron with random weights $W$ that spits out a number $Y$. What’s the variance of Y? Well, we can write\n",
        "$$Y=W_1X_1+...+W_nX_n$$\n",
        "\n",
        "If the two variables $X_i$ and $W_i$ are independent, the variance of their product is given by\n",
        "\n",
        "$$\\text{Var}(W_iX_i)=E[X_i]^2\\text{Var}(W_i)+E[W_i]^2\\text{Var}(X_i)+\\text{Var}(W_i)\\text{Var}(X_i)$$\n",
        "\n",
        "Now if our inputs and weights both have mean $0$, that simplifies to\n",
        "\n",
        "$$\\text{Var}(W_iX_i)=\\text{Var}(W_i)\\text{Var}(X_i)$$\n",
        "\n",
        "Then if we make a further assumption that the $X_i$ and $W_i$ are all independent and identically distributed, then\n",
        "\n",
        "$$\\text{Var}(Y)=\\text{Var}(\\sum_{i=1}^{n}W_iX_i)=n\\text{Var}(W_i)\\text{Var}(X_i)$$\n",
        "\n",
        "This is, the variance of the output is the variance of the input, but scaled by $n\\text{Var}(W_i)$. So if we want the variance of the input and output to be the same, that means $n\\text{Var}(W_i)$ should be 1. Which means the variance of the weights should be\n",
        "\n",
        "$$\\text{Var}(W_i)=\\frac{1}{n}=\\frac{1}{\\text{fan}_{in}}$$\n",
        "\n",
        "This is the *LeCun initialization*.\n",
        "\n",
        "The *Glorot & Bengio's* formula needs a tiny bit more work. If you go through the same steps for the backpropagated signal, you find that you need\n",
        "\n",
        "$$\\text{Var}(W_i)=\\frac{1}{\\text{fan}_{out}}$$\n",
        "\n",
        "to keep the variance of the input gradient and the output gradient the same. These two constraints can only be satisfied simultaneously if $\\text{fan}_{in}=\\text{fan}_{out}$, so as a compromise, Glorot & Bengio take the average of the two:\n",
        "\n",
        "$$\\text{Var}(W_i)=\\frac{2}{\\text{fan}_{in}+\\text{fan}_{out}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXutkI6zNzuj",
        "colab_type": "text"
      },
      "source": [
        "Some [papers](https://arxiv.org/pdf/1502.01852.pdf) have provided similar strategies for different activation functions. These strategies differ only by the scale of the variance and whether they use $\\text{fan}_{\\text{avg}}$ or $\\text{fan}_{\\text{in}}$.\n",
        "\n",
        "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called *He initialization*, $\\sigma^2=\\frac{2}{\\text{fan}_{\\text{in}}}$\n",
        ". The SELU activation function will be explained later in this chapter. It should be used with LeCun initialization (preferably with a normal distribution).\n",
        "\n",
        "By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"` like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkgPSOKcGH0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce049bfb-19cb-4fc7-bf31-e461a64974d2"
      },
      "source": [
        "import keras\n",
        "\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.Dense at 0x7fd16f1164a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5URqpz8Bh_-",
        "colab_type": "text"
      },
      "source": [
        "If you want He initialization with a uniform distribution but based on $\\text{fan}_{\\text{avg}}$ rather than $\\text{fan}_{\\text{in}}$ , you can use the `VarianceScaling` initializer like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v7GE645Gyuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f30eef50-70e2-43df-bafb-c7c123197c06"
      },
      "source": [
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
        "                                                 distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.Dense at 0x7fd16f116a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpowODeLNaJ6",
        "colab_type": "text"
      },
      "source": [
        "The following built-in initializers are available as part of the [`keras.initializers`](https://keras.io/api/layers/initializers/) module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgvLIbXxNaRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "174a9edc-db95-4eb4-d478-54445154fd13"
      },
      "source": [
        "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Constant',\n",
              " 'Identity',\n",
              " 'Initializer',\n",
              " 'K',\n",
              " 'Ones',\n",
              " 'Orthogonal',\n",
              " 'RandomNormal',\n",
              " 'RandomUniform',\n",
              " 'TruncatedNormal',\n",
              " 'VarianceScaling',\n",
              " 'Zeros',\n",
              " 'absolute_import',\n",
              " 'constant',\n",
              " 'deserialize',\n",
              " 'deserialize_keras_object',\n",
              " 'division',\n",
              " 'get',\n",
              " 'glorot_normal',\n",
              " 'glorot_uniform',\n",
              " 'he_normal',\n",
              " 'he_uniform',\n",
              " 'identity',\n",
              " 'lecun_normal',\n",
              " 'lecun_uniform',\n",
              " 'normal',\n",
              " 'np',\n",
              " 'one',\n",
              " 'ones',\n",
              " 'orthogonal',\n",
              " 'print_function',\n",
              " 'random_normal',\n",
              " 'random_uniform',\n",
              " 'serialize',\n",
              " 'serialize_keras_object',\n",
              " 'six',\n",
              " 'truncated_normal',\n",
              " 'uniform',\n",
              " 'zero',\n",
              " 'zeros']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5aosan9BiCg",
        "colab_type": "text"
      },
      "source": [
        "### Nonsaturating activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akhldAW_BiFS",
        "colab_type": "text"
      },
      "source": [
        "One of the insights in the paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of the activation function. Until then most people had assumed that the sigmoid activation function was an excellent choice. But it turns out that other activation functions behave much better in deep neural networks. In particular, the ReLU activation function, mostly because it does not saturate for positive values (and because it is fast to compute).\n",
        "\n",
        "Remember that a function $f$ is non-saturating iff $|\\lim_{z\\to-\\infty} f(z)|=+\\infty$ and $|\\lim_{z\\to\\infty} f(z)|=+\\infty$, and $f$ is saturating iff $f$ is not non-saturating. The sigmoid activation function (previous figure) is saturating, because it squashes real numbers to range between [0,1].\n",
        "\n",
        "\n",
        "The following built-in activations are available as part of the `keras.activations` module:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amwLCdF8ZxD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a4728829-71d2-4861-c828-acbb95c008af"
      },
      "source": [
        "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['K',\n",
              " 'Layer',\n",
              " 'absolute_import',\n",
              " 'deserialize',\n",
              " 'deserialize_keras_object',\n",
              " 'division',\n",
              " 'elu',\n",
              " 'exponential',\n",
              " 'get',\n",
              " 'hard_sigmoid',\n",
              " 'linear',\n",
              " 'print_function',\n",
              " 'relu',\n",
              " 'selu',\n",
              " 'serialize',\n",
              " 'sigmoid',\n",
              " 'six',\n",
              " 'softmax',\n",
              " 'softplus',\n",
              " 'softsign',\n",
              " 'tanh',\n",
              " 'warnings']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SADzorheSDkV",
        "colab_type": "text"
      },
      "source": [
        "#### RELU and variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqch946QSF_9",
        "colab_type": "text"
      },
      "source": [
        "The Rectified Linear Unit (ReLU) activation function, which is defined as $f(z)=\\text{max}(0,z)$ is non-saturating because $\\lim_{z\\to\\infty} f(z)=+\\infty$.\n",
        "\n",
        "\n",
        "\n",
        "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the *dying ReLUs*: during training, some neurons effectively “die”, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.\n",
        "\n",
        "To solve this problem, you may want to use a variant of the ReLU function, such as the *leaky ReLU*. This function is defined as $\\text{LeakyReLU}_{\\alpha}(z) = \\text{max}(\\alpha z, z)$ (see the next figure). The hyperparameter $\\alpha$ defines how much the function “leaks”: it is the slope of the function for $z < 0$ and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. A [paper](https://arxiv.org/abs/1505.00853) in 2015 compared several variants of the ReLU activation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting $\\alpha = 0.2$ (a huge leak) seemed to result in better performance than $\\alpha = 0.01$ (a small leak). The paper also evaluated the *randomized leaky ReLU* (RReLU), where $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing. RReLU also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set). Finally, the paper evaluated the *parametric leaky* ReLU (PReLU), where α is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijzUIs17Oklw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "cellView": "form",
        "outputId": "87430dd3-3aaa-4d8c-b9e5-28dd3bf1deff"
      },
      "source": [
        "#@title\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.maximum(alpha*z, z)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
        "plt.grid(True)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
        "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.5, 4.2])\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8gr9x_bOkx1",
        "colab_type": "text"
      },
      "source": [
        "To use the leaky ReLU activation function, create a `LeakyReLU` layer and add it to your model just after the layer you want to apply it.\n",
        "\n",
        "Let's train a neural network on [Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist) dataset using the Leaky ReLU/PRELU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lit7trdCTPwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, shuffle= True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GyZDXu6T_ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(), #keras.layers.PReLU()\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(), #keras.layers.PReLU()\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4tJjIxAUCh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JaoHmiUERC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a9755f62-eb7d-4ca1-8760-0cfa4873be81"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/10\n",
            "54000/54000 [==============================] - 4s 72us/step - loss: 1.3223 - accuracy: 0.6001 - val_loss: 0.9120 - val_accuracy: 0.7040\n",
            "Epoch 2/10\n",
            "54000/54000 [==============================] - 4s 77us/step - loss: 0.7917 - accuracy: 0.7411 - val_loss: 0.7281 - val_accuracy: 0.7640\n",
            "Epoch 3/10\n",
            "54000/54000 [==============================] - 4s 70us/step - loss: 0.6755 - accuracy: 0.7767 - val_loss: 0.6504 - val_accuracy: 0.7870\n",
            "Epoch 4/10\n",
            "54000/54000 [==============================] - 4s 68us/step - loss: 0.6172 - accuracy: 0.7959 - val_loss: 0.6032 - val_accuracy: 0.8025\n",
            "Epoch 5/10\n",
            "54000/54000 [==============================] - 4s 68us/step - loss: 0.5800 - accuracy: 0.8069 - val_loss: 0.5726 - val_accuracy: 0.8107\n",
            "Epoch 6/10\n",
            "54000/54000 [==============================] - 4s 70us/step - loss: 0.5532 - accuracy: 0.8147 - val_loss: 0.5505 - val_accuracy: 0.8185\n",
            "Epoch 7/10\n",
            "54000/54000 [==============================] - 4s 69us/step - loss: 0.5334 - accuracy: 0.8201 - val_loss: 0.5328 - val_accuracy: 0.8248\n",
            "Epoch 8/10\n",
            "54000/54000 [==============================] - 4s 69us/step - loss: 0.5174 - accuracy: 0.8239 - val_loss: 0.5202 - val_accuracy: 0.8273\n",
            "Epoch 9/10\n",
            "54000/54000 [==============================] - 4s 70us/step - loss: 0.5046 - accuracy: 0.8284 - val_loss: 0.5141 - val_accuracy: 0.8282\n",
            "Epoch 10/10\n",
            "54000/54000 [==============================] - 4s 70us/step - loss: 0.4936 - accuracy: 0.8316 - val_loss: 0.4992 - val_accuracy: 0.8342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tq2LRe5Z-Wh",
        "colab_type": "text"
      },
      "source": [
        "The following built-in variants of RELU activations are available as part of the `keras.layers` module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrY138bYZ-jp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "471f46fa-034d-4340-ae98-d89966cb4824"
      },
      "source": [
        "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVXxuq_lSWkX",
        "colab_type": "text"
      },
      "source": [
        "#### ELU and variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvXXvipkJebj",
        "colab_type": "text"
      },
      "source": [
        "A [paper](https://arxiv.org/abs/1511.07289) by Djork-Arné Clevert in 2015 proposed a new activation function called the *exponential linear unit* (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced, and the neural network performed better on the test set. The next equation shows its definition.\n",
        "\n",
        "$$\\text{ELU}_{\\alpha}(z) = \\begin{cases}\n",
        "              \\alpha(e^{z}-1) & \\text{if} z < 0,\\\\\n",
        "              z & \\text{if } z \\geq 0.\n",
        "\\end{cases}$$\n",
        "\n",
        "The next figure graphs the function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CjLFNjdU3io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "cellView": "form",
        "outputId": "a1bd5926-09cd-4ae7-a249-5d71540f63a7"
      },
      "source": [
        "#@title\n",
        "def elu(z, alpha=1):\n",
        "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1, -1], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSY8xaNYU3tk",
        "colab_type": "text"
      },
      "source": [
        "The ELU activation function looks a lot like the ReLU function, with a few major differences:\n",
        "\n",
        "- It takes on negative values when $z < 0$, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when $z$ is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter.\n",
        "\n",
        "- It has a nonzero gradient for $z < 0$, which avoids the dead neurons problem.\n",
        "\n",
        "- If $\\alpha=1$ then the function is smooth everywhere, including around $z = 0$, which helps speed up Gradient Descent since it does not bounce as much to the left and right of $z = 0$.\n",
        "\n",
        "The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network.\n",
        "\n",
        "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mx9e3YzVTVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54f4aced-948c-49f5-c824-5f51ba20d244"
      },
      "source": [
        "keras.layers.Dense(10, activation=\"elu\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.Dense at 0x7fd1d014b470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTS1jBA2BiIa",
        "colab_type": "text"
      },
      "source": [
        "Then, a [paper](https://arxiv.org/abs/1706.02515) by Günter Klambauer in 2017 introduced the Scaled ELU (SELU) activation function: as its name suggests, it is a scaled variant of the ELU activation function. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will *self-normalize*: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones). There are, however, a few conditions for self-normalization to happen (see the paper for the mathematical justification):\n",
        "\n",
        "- The input features must be standardized (mean 0 and standard deviation 1).\n",
        "\n",
        "- Every hidden layer’s weights must be initialized with LeCun normal initialization. In Keras, this means setting `kernel_initializer=\"lecun_normal\"`.\n",
        "\n",
        "- The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
        "\n",
        "- The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well.\n",
        "\n",
        "Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use $l_1$ or $l_2$ regularization, regular dropout, max-norm, skip connections, or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice, it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions.\n",
        "\n",
        "The next figure graphs the function.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjdhGKQbWbc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "cellView": "form",
        "outputId": "b8477420-d410-49d7-b252-d3e0099cc5d7"
      },
      "source": [
        "#@title\n",
        "from scipy.special import erfc\n",
        "\n",
        "# alpha and scale to self normalize with mean 0 and standard deviation 1 (equation 14 in the paper):\n",
        "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
        "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n",
        "\n",
        "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
        "    return scale * elu(z, alpha)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.title(\"SELU activation function\", fontsize=14)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF_ewL9rW3_X",
        "colab_type": "text"
      },
      "source": [
        "By default, the SELU hyperparameters (scale and alpha) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xDauaV0W7DQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f56a864a-d082-48e6-b889-6d47e071c4e1"
      },
      "source": [
        "np.random.seed(42)\n",
        "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
        "for layer in range(1000):\n",
        "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
        "    Z = selu(np.dot(Z, W))\n",
        "    means = np.mean(Z, axis=0).mean()\n",
        "    stds = np.std(Z, axis=0).mean()\n",
        "    if layer % 100 == 0:\n",
        "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 0: mean -0.00, std deviation 1.00\n",
            "Layer 100: mean 0.02, std deviation 0.96\n",
            "Layer 200: mean 0.01, std deviation 0.90\n",
            "Layer 300: mean -0.02, std deviation 0.92\n",
            "Layer 400: mean 0.05, std deviation 0.89\n",
            "Layer 500: mean 0.01, std deviation 0.93\n",
            "Layer 600: mean 0.02, std deviation 0.92\n",
            "Layer 700: mean -0.02, std deviation 0.90\n",
            "Layer 800: mean 0.05, std deviation 0.83\n",
            "Layer 900: mean 0.02, std deviation 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quv6Q-SXWbku",
        "colab_type": "text"
      },
      "source": [
        "To use the SELU activation, set `activation=\"selu\"` and `kernel_initializer=\"lecun_normal\"` when creating a layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze7GQOC0XE6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d4b4ea9-aafa-4d36-c00e-a9fa14d83e82"
      },
      "source": [
        "keras.layers.Dense(10, activation=\"selu\",\n",
        "                   kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.Dense at 0x7fd16f08c5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9BUIISWXKgZ",
        "colab_type": "text"
      },
      "source": [
        "Let's create a neural net for the Fashion MNIST dataset with 100 hidden layers, using the SELU activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTUvpHKCXMh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
        "                             kernel_initializer=\"lecun_normal\"))\n",
        "for layer in range(99):\n",
        "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
        "                                 kernel_initializer=\"lecun_normal\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhQPfggCXRbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0FraejxXVMW",
        "colab_type": "text"
      },
      "source": [
        "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP7YcrluXW0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6d1ffedd-fc8e-4e90-de23-a87f36cadc11"
      },
      "source": [
        "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/5\n",
            "54000/54000 [==============================] - 36s 670us/step - loss: 1.1956 - accuracy: 0.5432 - val_loss: 0.7920 - val_accuracy: 0.6907\n",
            "Epoch 2/5\n",
            "54000/54000 [==============================] - 34s 623us/step - loss: 0.7471 - accuracy: 0.7182 - val_loss: 0.7962 - val_accuracy: 0.7222\n",
            "Epoch 3/5\n",
            "54000/54000 [==============================] - 34s 621us/step - loss: 0.7100 - accuracy: 0.7417 - val_loss: 0.6099 - val_accuracy: 0.7767\n",
            "Epoch 4/5\n",
            "54000/54000 [==============================] - 34s 621us/step - loss: 0.5829 - accuracy: 0.7862 - val_loss: 0.5884 - val_accuracy: 0.7803\n",
            "Epoch 5/5\n",
            "54000/54000 [==============================] - 33s 614us/step - loss: 0.5251 - accuracy: 0.8147 - val_loss: 0.5080 - val_accuracy: 0.8180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaVxiMerXrDV",
        "colab_type": "text"
      },
      "source": [
        "Now look at what happens if we try to use the ReLU activation function instead:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN3z8h89Xbxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1bf808f4-e748-41ee-a7e5-2cbedd332e6c"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "\n",
        "for layer in range(99):\n",
        "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/5\n",
            "54000/54000 [==============================] - 23s 426us/step - loss: 1.8492 - accuracy: 0.2554 - val_loss: 1.5562 - val_accuracy: 0.3985\n",
            "Epoch 2/5\n",
            "54000/54000 [==============================] - 22s 400us/step - loss: 1.3452 - accuracy: 0.4344 - val_loss: 0.9714 - val_accuracy: 0.5927\n",
            "Epoch 3/5\n",
            "54000/54000 [==============================] - 22s 408us/step - loss: 1.0464 - accuracy: 0.5687 - val_loss: 0.8770 - val_accuracy: 0.6633\n",
            "Epoch 4/5\n",
            "54000/54000 [==============================] - 21s 395us/step - loss: 0.8961 - accuracy: 0.6373 - val_loss: 0.8327 - val_accuracy: 0.6588\n",
            "Epoch 5/5\n",
            "54000/54000 [==============================] - 21s 398us/step - loss: 0.7600 - accuracy: 0.6954 - val_loss: 0.7094 - val_accuracy: 0.7182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2sIODXUX1C6",
        "colab_type": "text"
      },
      "source": [
        "Not great at all, we suffered from the vanishing/exploding gradients problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HB7DD9PBiRq",
        "colab_type": "text"
      },
      "source": [
        "### Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6yP-035BiUl",
        "colab_type": "text"
      },
      "source": [
        "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
        "\n",
        "In a [paper](https://arxiv.org/abs/1502.03167) in 2015, Sergey Ioffe and Christian Szegedy proposed a technique called *Batch Normalization* (BN) that addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a `StandardScaler`); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).\n",
        "\n",
        "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”). The whole operation is summarized step by step in the next equation.\n",
        "\n",
        "\\begin{cases}\n",
        "\\boldsymbol{\\mu}_{B}=\\frac{1}{m_B}\\sum_{i=1}^{m_B}\\boldsymbol{x}^{(i)}\\\\\\\\\n",
        "\\boldsymbol{\\sigma}_{B}^2=\\frac{1}{m_B}\\sum_{i=1}^{m_B}(\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_B)^2\\\\\\\\\n",
        "\\boldsymbol{\\hat{x}}^{(i)}=\\frac{\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_B}{\\sqrt{\\boldsymbol{\\sigma}_{B}^2+\\epsilon}}\\\\\\\\\n",
        "\\boldsymbol{z}^{(i)}=\\boldsymbol{\\gamma} \\otimes \\boldsymbol{\\hat{x}}^{(i)} + \\boldsymbol{\\beta}\n",
        "\\end{cases}\n",
        "\n",
        "\n",
        "- $\\boldsymbol{\\mu}_{B}$ is the vector of input means, evaluated over the whole mini-batch B (it contains one mean per input).\n",
        "\n",
        "- $\\boldsymbol{\\sigma}_{B}$ is the vector of input standard deviations, also evaluated over the whole mini-batch (it contains one standard deviation per input).\n",
        "\n",
        "- $m_B$ is the number of instances in the mini-batch.\n",
        "\n",
        "- $\\boldsymbol{\\hat{x}}^{(i)}$ is the vector of zero-centered and normalized inputs for instance $i$.\n",
        "\n",
        "- $\\boldsymbol{\\gamma}$ is the output scale parameter vector for the layer (it contains one scale parameter per input).\n",
        "\n",
        "- $\\otimes$ represents element-wise multiplication (each input is multiplied by its corresponding output scale parameter).\n",
        "\n",
        "- $\\boldsymbol{\\beta}$ is the output shift (offset) parameter vector for the layer (it contains one offset parameter per input). Each input is offset by its corresponding shift parameter.\n",
        "\n",
        "- $\\epsilon$ is a tiny number that avoids division by zero (typically $10^{-5}$). This is called a smoothing term.\n",
        "\n",
        "- $\\boldsymbol{z}^{(i)}$ is the output of the BN operation. It is a rescaled and shifted version of the inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOV0k5kDuthl",
        "colab_type": "text"
      },
      "source": [
        "So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it’s not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable. One solution could be to wait until the end of the training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions. However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer’s input means and standard deviations. This is what Keras does automatically when you use the `BatchNormalization` layer. To sum up, four parameter vectors are learned in each batch-normalized layer: $\\boldsymbol{\\gamma}$ (the output scale vector) and $\\boldsymbol{\\beta}$ (the output offset vector) are learned through regular backpropagation, and $\\boldsymbol{\\mu}$ (the final input mean vector) and $\\boldsymbol{\\sigma}$ (the final input standard deviation vector) are estimated using an exponential moving average. Note that $\\boldsymbol{\\mu}$ and \\boldsymbol{\\sigma} are estimated during training, but they are used only after training (to replace the batch input means and standard deviations in the third equation).\n",
        "\n",
        "Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all the deep neural networks they experimented with, leading to a huge improvement in the ImageNet classification task (ImageNet is a large database of images classified into many classes, commonly used to evaluate computer vision systems). The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function. The networks were also much less sensitive to the weight initialization. The authors were able to use much larger learning rates, significantly speeding up the learning process. Specifically, they note that, applying to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Finally, like a gift that keeps on giving, Batch Normalization acts like a regularizer, reducing the need for other regularization techniques (such as dropout).\n",
        "\n",
        "Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. So if you need predictions to be lightning-fast, you may want to check how well plain ELU + He initialization perform before playing with Batch Normalization.\n",
        "\n",
        "**Note**: You may find that training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhJq2qiLvROq",
        "colab_type": "text"
      },
      "source": [
        "#### Implementing batch normalization in Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXUMm77QvRRT",
        "colab_type": "text"
      },
      "source": [
        "As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a `BatchNormalization` layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model. For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJBmr7AuvbjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyCqcERjvbq9",
        "colab_type": "text"
      },
      "source": [
        "In this tiny example with just two hidden layers, it’s unlikely that Batch Normalization will have a very positive impact; but for deeper networks, it can make a tremendous difference.\n",
        "\n",
        "Let’s display the model summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxaPl2crviFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8e2ab8f7-e6c5-4ed1-9b27-45591f208933"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_9 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_425 (Dense)            (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_426 (Dense)            (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_427 (Dense)            (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQvMktA-vm65",
        "colab_type": "text"
      },
      "source": [
        "As you can see, each BN layer adds four parameters per input: $\\boldsymbol{\\gamma}$, $\\boldsymbol{\\beta}$, $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$ (for example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two parameters, $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable” (if you count the total number of BN parameters, 3136 + 1200 + 400, and divide by 2, you get 2368, which is the total number of non-trainable parameters in this model).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyKMrzgShWxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJU7mkdThh65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b1cd52c9-890d-43bd-c177-2201771b96d7"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/10\n",
            "54000/54000 [==============================] - 7s 133us/step - loss: 0.9048 - accuracy: 0.6944 - val_loss: 0.6060 - val_accuracy: 0.7923\n",
            "Epoch 2/10\n",
            "54000/54000 [==============================] - 7s 128us/step - loss: 0.5942 - accuracy: 0.7956 - val_loss: 0.5293 - val_accuracy: 0.8167\n",
            "Epoch 3/10\n",
            "54000/54000 [==============================] - 7s 129us/step - loss: 0.5382 - accuracy: 0.8121 - val_loss: 0.4963 - val_accuracy: 0.8273\n",
            "Epoch 4/10\n",
            "54000/54000 [==============================] - 7s 129us/step - loss: 0.5082 - accuracy: 0.8229 - val_loss: 0.4705 - val_accuracy: 0.8372\n",
            "Epoch 5/10\n",
            "54000/54000 [==============================] - 7s 128us/step - loss: 0.4881 - accuracy: 0.8301 - val_loss: 0.4551 - val_accuracy: 0.8425\n",
            "Epoch 6/10\n",
            "54000/54000 [==============================] - 7s 128us/step - loss: 0.4691 - accuracy: 0.8356 - val_loss: 0.4434 - val_accuracy: 0.8450\n",
            "Epoch 7/10\n",
            "54000/54000 [==============================] - 7s 129us/step - loss: 0.4588 - accuracy: 0.8393 - val_loss: 0.4351 - val_accuracy: 0.8485\n",
            "Epoch 8/10\n",
            "54000/54000 [==============================] - 7s 128us/step - loss: 0.4451 - accuracy: 0.8454 - val_loss: 0.4245 - val_accuracy: 0.8513\n",
            "Epoch 9/10\n",
            "54000/54000 [==============================] - 7s 128us/step - loss: 0.4378 - accuracy: 0.8457 - val_loss: 0.4182 - val_accuracy: 0.8533\n",
            "Epoch 10/10\n",
            "54000/54000 [==============================] - 7s 130us/step - loss: 0.4295 - accuracy: 0.8493 - val_loss: 0.4119 - val_accuracy: 0.8557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBc_o_cSvwHr",
        "colab_type": "text"
      },
      "source": [
        "The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after (as we just did). There is some debate about this, as which is preferable seems to depend on the task, you can experiment with this too to see which option works best on your dataset. To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass `use_bias=False` when creating it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdQtnjymv2F3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm2DNPJPhdP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fUBaBkPheqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e3b85a83-c39a-4e1b-f219-8891f1480aa3"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/10\n",
            "54000/54000 [==============================] - 7s 131us/step - loss: 0.9399 - accuracy: 0.6890 - val_loss: 0.6535 - val_accuracy: 0.7882\n",
            "Epoch 2/10\n",
            "54000/54000 [==============================] - 7s 126us/step - loss: 0.6391 - accuracy: 0.7834 - val_loss: 0.5642 - val_accuracy: 0.8137\n",
            "Epoch 3/10\n",
            "54000/54000 [==============================] - 7s 126us/step - loss: 0.5735 - accuracy: 0.8037 - val_loss: 0.5205 - val_accuracy: 0.8240\n",
            "Epoch 4/10\n",
            "54000/54000 [==============================] - 7s 124us/step - loss: 0.5404 - accuracy: 0.8138 - val_loss: 0.4952 - val_accuracy: 0.8287\n",
            "Epoch 5/10\n",
            "54000/54000 [==============================] - 7s 125us/step - loss: 0.5155 - accuracy: 0.8209 - val_loss: 0.4780 - val_accuracy: 0.8352\n",
            "Epoch 6/10\n",
            "54000/54000 [==============================] - 7s 125us/step - loss: 0.4971 - accuracy: 0.8284 - val_loss: 0.4635 - val_accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "54000/54000 [==============================] - 7s 130us/step - loss: 0.4828 - accuracy: 0.8320 - val_loss: 0.4516 - val_accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "54000/54000 [==============================] - 7s 125us/step - loss: 0.4724 - accuracy: 0.8356 - val_loss: 0.4437 - val_accuracy: 0.8467\n",
            "Epoch 9/10\n",
            "54000/54000 [==============================] - 7s 125us/step - loss: 0.4621 - accuracy: 0.8384 - val_loss: 0.4357 - val_accuracy: 0.8497\n",
            "Epoch 10/10\n",
            "54000/54000 [==============================] - 7s 126us/step - loss: 0.4558 - accuracy: 0.8411 - val_loss: 0.4279 - val_accuracy: 0.8535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dm7zU6rv25d",
        "colab_type": "text"
      },
      "source": [
        "The `BatchNormalization` class has quite a few hyperparameters you can tweak. The defaults will usually be fine, but you may occasionally need to tweak the `momentum`. This hyperparameter allows you to control how much of the statistics from the previous mini-batch to include when the update is calculated. It is used by the `BatchNormalization` layer when it updates the exponential moving averages; given a new value $\\boldsymbol{v}$ (i.e., a new vector of input means or standard deviations computed over the current batch), the layer updates the running average $\\boldsymbol{\\hat{v}}$ using the following equation:\n",
        "\n",
        "$$\\boldsymbol{\\hat{v}} \\leftarrow  \\boldsymbol{\\hat{v}} \\times\\text{momentum}+\\boldsymbol{v} \\times (1-\\text{momentum})$$\n",
        "\n",
        "\n",
        "A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and smaller mini-batches). By default, this is kept high with a value of 0.99. This can be set to 0.0 to only use statistics from the current mini-batch\n",
        "\n",
        "Another important hyperparameter is `axis`: it determines which axis should be normalized. It defaults to -1, meaning that by default it will normalize the last axis (using the means and standard deviations computed across the *other* axes). When the input batch is 2D (i.e., the batch shape is [*batch size, features*]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. For example, the first BN layer in the previous code example will independently normalize (and rescale and shift) each of the 784 input features. If we move the first BN layer before the `Flatten` layer, then the input batches will be 3D, with shape [*batch size, height, width*]; therefore, the BN layer will compute 28 means and 28 standard deviations (1 per column of pixels, computed across all instances in the batch and across all rows in the column), and it will normalize all pixels in a given column using the same mean and standard deviation. There will also be just 28 scale parameters and 28 shift parameters. If instead you still want to treat each of the 784 pixels independently, then you should set `axis=[1, 2]`.\n",
        "\n",
        "Notice that the BN layer does not perform the same computation during training and after training: it uses batch statistics during training and the “final” statistics after training (i.e., the final values of the moving averages).\n",
        "\n",
        "\n",
        "`BatchNormalization` has become one of the most-used layers in deep neural networks, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. But a recent [paper](https://arxiv.org/abs/1901.09321) by Hongyi Zhang et al. may change this assumption: by using a novel *fixed-update* (fixup) weight initialization technique, the authors managed to train a very deep neural network (10000 layers!) without BN, achieving *state-of-the-art* performance on complex image classification tasks. As this is bleeding-edge research, however, you may want to wait for additional research to confirm this finding before you drop Batch Normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0E9UJsuwCLI",
        "colab_type": "text"
      },
      "source": [
        "### Gradient clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IcZXe1LwM8W",
        "colab_type": "text"
      },
      "source": [
        "Neural networks are trained using the stochastic gradient descent optimization algorithm. This requires first the estimation of the loss on one or more training examples, then the calculation of the derivative of the loss, which is propagated backward through the network in order to update the weights. It is possible for the updates to the weights to be so large that the weights either overflow or underflow their numerical precision. In practice, the weights can take on the value of an “NaN” or “Inf” when they overflow or underflow and for practical purposes the network will be useless from that point forward, forever predicting NaN values as signals flow through the invalid weights.\n",
        "\n",
        "The problem of exploding gradients is more common with recurrent neural networks, such as LSTMs given the accumulation of gradients unrolled over hundreds of input time steps.\n",
        "\n",
        "A common solution to exploding gradients is to change the derivative of the error before propagating it backward through the network and using it to update the weights. By rescaling the error derivative, the updates to the weights will also be rescaled, dramatically decreasing the likelihood of an overflow or underflow.\n",
        "\n",
        "There are two main methods for updating the error derivative; they are:\n",
        "\n",
        "- Gradient Scaling.\n",
        "- Gradient Clipping.\n",
        "\n",
        "*Gradient scaling* involves normalizing the error gradient vector such that vector norm (magnitude) equals a defined value, such as 1.0. *Gradient clipping*  involves forcing the gradient values (element-wise) to a specific threshold (a minimum or maximum value) if the gradient exceeded an expected range. Together, these methods are often simply referred to as *gradient clipping*.\n",
        "\n",
        "\n",
        "It is common to use the same gradient clipping configuration for all layers in the network. Nevertheless, there are examples where a larger range of error gradients are permitted in the output layer compared to hidden layers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In Keras, two types of gradient clipping can be used: gradient norm scaling and gradient value clipping.\n",
        "\n",
        "*Gradient norm scaling* involves changing the derivatives of the loss function to have a given vector norm when the $L_2$ vector norm (sum of the squared values) of the gradient vector exceeds a threshold value. For example, we could specify a norm of 1.0, meaning that if the vector norm for a gradient exceeds 1.0, then the values in the vector will be rescaled so that the norm of the vector equals 1.0.\n",
        "\n",
        "This can be used in Keras by specifying the `clipnorm` argument on the optimizer; for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q8Tlv40lH_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure sgd with gradient norm clipping\n",
        "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvBo0fuAlA_J",
        "colab_type": "text"
      },
      "source": [
        "*Gradient value clipping* involves clipping the derivatives of the loss function to have a given value if a gradient value is less than a negative threshold or more than the positive threshold. For example, we could specify a norm of 01.0, meaning that if a gradient value was less than -1.0, it is set to -1.0 and if it is more than 1.0, then it will be set to 1.0.\n",
        "\n",
        "This can be used in Keras by specifying the `clipvalue`  argument on the optimizer, for example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mKs3ug_lBOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure sgd with gradient value clipping\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvv2vsu4wQoY",
        "colab_type": "text"
      },
      "source": [
        "This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter you can tune. Note that it may change the orientation of the gradient vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of `clipvalue`. This will clip the whole gradient if its $l_2$ norm is greater than the threshold you picked. For example, if you set `clipnorm=1.0`, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component. If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set."
      ]
    }
  ]
}