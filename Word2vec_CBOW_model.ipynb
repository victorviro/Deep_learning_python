{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec_CBOW_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PCbSISKxEejJ",
        "gOwXvMVxEl--",
        "kQaviP63EqMu",
        "IPeEkt5OlxGV",
        "0-nH18AFGXEO",
        "8M79b4AYJBcd",
        "2jQmiweSPhjH"
      ],
      "mount_file_id": "1xtItdw4_7GaJo2ojYqdRzlVivNBswOVc",
      "authorship_tag": "ABX9TyP5Wshp6OEJ4WrUlxjXDsWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Word2vec_CBOW_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khLWesUEEXMk",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjnTn9LEb4p",
        "colab_type": "text"
      },
      "source": [
        "[Word2Vec](https://arxiv.org/abs/1301.3781) is a statistical method for efficiently learning a standalone word embedding from a text corpus. It was developed by Google in 2013.\n",
        "\n",
        "Additionally, the work involved analysis of the learned vectors and the exploration of vector math on the representations of words. The word vectors can be reduced to two-dimensions using a dimensionality reduction technique to explore relationships between certain words.\n",
        "\n",
        "![](https://i.ibb.co/8dHdyKk/wor2vec-relatiuonships.png)\n",
        "\n",
        "An interesting tool to visualize these relationships can be seen in a [tensorflow projection](https://projector.tensorflow.org/) where the word vectors are reduced to a three-dimensional space.\n",
        "\n",
        "The trick is to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer (these weights are actually the *word vectors* that we’re trying to learn).\n",
        "\n",
        "Two different learning models were introduced that can be used as part of the Word2Vec approach to learn the word embedding:\n",
        "\n",
        "-  Continuous Bag-of-Words, or CBOW model.\n",
        "-  Continuous Skip-Gram Model.\n",
        "\n",
        "In this notebook, we are going to deep into the CBOW model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCbSISKxEejJ",
        "colab_type": "text"
      },
      "source": [
        "## CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDKvb9hwg-F4",
        "colab_type": "text"
      },
      "source": [
        "The **CBOW** model learns the embedding by predicting the current target word (the center word) based on the context words (its surrounding words).\n",
        "\n",
        "![](https://i.ibb.co/XbLDFZB/CBOW-word2vec.png)\n",
        "\n",
        "While the Word2Vec family of models is unsupervised (we just give it a corpus without additional labels and it can construct dense word embeddings from the corpus), we will still need to leverage a supervised, classification methodology once we have this corpus to get these embeddings. Although we can do that from within the corpus itself, without any auxiliary information. We can model this CBOW architecture now as a classification model such that we take in the context words as our input, `X`, and try to predict the target word, `Y`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgSP19neEf9X",
        "colab_type": "text"
      },
      "source": [
        "Considering a simple sentence, \"deep learning is very hard and fun\". If we consider a context window of size 2, we will take 2 words behind the word and 2 words after the word. Hence for each word, we will get 4 words associated with it. We will do this for every word in the data and collect the word pairs. Let’s visualize this.\n",
        "\n",
        "![](https://i.ibb.co/ggCDLSn/context-and-target-word-word2vec.png)\n",
        "\n",
        " \n",
        "\n",
        "Note that for the words near the edges, we have lesser words to consider and hence the number of context words will be lesser than 4. In the practice, padding is used so that the number of words will be always 4.\n",
        "\n",
        "As we are passing the context window through the text data, we find all word pairs of target and context words to form a dataset in the format of (*context_window, target_word*). For the sentence above, it will look like this:\n",
        "\n",
        "- $1^{\\text{st}}$ window: ([\"learning\", \"is\"],\"deep\")\n",
        "- $2^{\\text{st}}$ window: ([\"deep\", \"is\", \"very\"],\"learning\")\n",
        "- $3^{\\text{st}}$ window: ([\"deep\", \"learning\", \"very\", \"hard\"],\"is\")\n",
        "\n",
        "And so on. This can be considered as our \"training data\" for word2vec.\n",
        "\n",
        "It should be noted that the order of the input words is not considered in the model (hence the concept of the bag of words in its name).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOwXvMVxEl--",
        "colab_type": "text"
      },
      "source": [
        "## Implementing CBOW model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQaviP63EqMu",
        "colab_type": "text"
      },
      "source": [
        "#### Load the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGUsGbIF6uz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(13)\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "#import gensim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzZbF7EeFB8R",
        "colab_type": "text"
      },
      "source": [
        "#### Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDtcmnlW6uwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a44ab0e2-c2df-409b-8b85-725d06fd28c0"
      },
      "source": [
        "path = get_file('alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
        "corpus = open(path).readlines()\n",
        "print(f'First 10 sentences of the corpus: {corpus[0:10]}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 sentences of the corpus: ['\\ufeffThe Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n', '\\n', 'This eBook is for the use of anyone anywhere at no cost and with\\n', 'almost no restrictions whatsoever.  You may copy it, give it away or\\n', 're-use it under the terms of the Project Gutenberg License included\\n', 'with this eBook or online at www.gutenberg.org\\n', '\\n', '\\n', 'Title: Alice’s Adventures in Wonderland\\n', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX6kpEdwFAS5",
        "colab_type": "text"
      },
      "source": [
        "#### Build the corpus vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7FCrkAzoZNe",
        "colab_type": "text"
      },
      "source": [
        "We will first build our corpus vocabulary where we extract out each unique word from our vocabulary and map a unique numeric identifier to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl8aBY4r6uu3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "181a628c-bfb9-4c80-f518-d06f30158af5"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "corpus_ids = tokenizer.texts_to_sequences(corpus)\n",
        "word_id_dict = tokenizer.word_index\n",
        "word_id_dict['PAD'] = 0\n",
        "id_word_dict = {v:k for k, v in word_id_dict.items()}\n",
        "print(f'First 10 sentences of the corpus by ids: {corpus_ids[0:10]}')\n",
        "#number_samples = sum(len(s) for s in corpus_ids)\n",
        "vocab_size = len(word_id_dict)\n",
        "print(f'Vocabulary size: {vocab_size}')\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 sentences of the corpus by ids: [[1838, 46, 42, 379, 6, 237, 380, 10, 487, 47, 820, 821], [], [22, 379, 34, 24, 1, 147, 6, 699, 996, 19, 49, 822, 3, 18], [488, 49, 1295, 1296, 11, 174, 344, 8, 311, 8, 165, 27], [1297, 147, 8, 203, 1, 204, 6, 1, 46, 42, 259, 997], [18, 22, 379, 27, 823, 19, 408, 42, 381], [], [], [1839, 237, 380, 10, 487], []]\n",
            "Vocabulary size: 3552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QVKj_Vs-rMn",
        "colab_type": "text"
      },
      "source": [
        "The `PAD` term is typically used to pad context words to a fixed length if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv-LkzgtFNQ0",
        "colab_type": "text"
      },
      "source": [
        "#### Build a CBOW (context, target) generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPo6vIRuo6DG",
        "colab_type": "text"
      },
      "source": [
        "We need pairs that consist of a target center word and surround context words. In our implementation, the surrounding context is composed of `2*window_size` words (except for words near the edges) where we take `window_size` words before and after the target word in our corpus. This will become clearer with the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo9AOqMAIp0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data(corpus, window_size, vocab_size):\n",
        "    maxlen = window_size*2\n",
        "    for words in corpus:\n",
        "        L = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            contexts = []\n",
        "            labels   = []            \n",
        "            s = index - window_size\n",
        "            e = index + window_size + 1\n",
        "            \n",
        "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
        "            labels.append(word)\n",
        "\n",
        "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
        "            # Convert the target word to one-hot encoding\n",
        "            y = np_utils.to_categorical(labels, vocab_size)\n",
        "            yield (x, y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIGnIM_h-UU",
        "colab_type": "text"
      },
      "source": [
        "We define the dimension of the embedding to 100, and the window size to 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvLkh036iOLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim_embedding = 100\n",
        "window_size = 2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3t72EqqiQuz",
        "colab_type": "text"
      },
      "source": [
        "Let's see some examples of training batches: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ekb0ZR7rmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "629ee773-b802-46b1-f936-8cc1cd2973e3"
      },
      "source": [
        "# Test this out for some samples\n",
        "i = 0\n",
        "for x, y in generate_data(corpus_ids[80:100], window_size, vocab_size):\n",
        "    if x[0][0] != 0:\n",
        "        print('_'*60)\n",
        "        #print('x:', x)\n",
        "        #print('y:', y)\n",
        "        print('Context (X):', [id_word_dict[w] for w in x[0]], '-> Target (Y):', id_word_dict[np.argwhere(y[0])[0][0]])\n",
        "        i+=1\n",
        "        if i==5:\n",
        "            break\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "____________________________________________________________\n",
            "Context (X): ['large', 'rabbit', 'under', 'the'] -> Target (Y): hole\n",
            "____________________________________________________________\n",
            "Context (X): ['rabbit', 'hole', 'the', 'hedge'] -> Target (Y): under\n",
            "____________________________________________________________\n",
            "Context (X): ['in', 'another', 'down', 'went'] -> Target (Y): moment\n",
            "____________________________________________________________\n",
            "Context (X): ['another', 'moment', 'went', 'alice'] -> Target (Y): down\n",
            "____________________________________________________________\n",
            "Context (X): ['moment', 'down', 'alice', 'after'] -> Target (Y): went\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY1kl00EpvG9",
        "colab_type": "text"
      },
      "source": [
        "This output give us more perspective of how `X` forms our context words and we are trying to predict the target center word `Y` based on its context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbXiPVymFsoo",
        "colab_type": "text"
      },
      "source": [
        "#### Build the CBOW model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qFl_CkVi2VK",
        "colab_type": "text"
      },
      "source": [
        "We now leverage Keras to build our deep learning architecture for the CBOW model. For this, our inputs will be our context words which are passed to an embedding layer (initialized with random weights). The word embeddings are propagated to a lambda layer where we average out the word embeddings (hence called CBOW because we don’t really consider the order or sequence in the context words when averaged) and then we pass this averaged context embedding to a dense softmax layer which predicts our target word. We match this with the actual target word, compute the loss by leveraging the `categorical_crossentropy` loss ([explanatory video](https://www.coursera.org/lecture/probabilistic-models-in-nlp/training-a-cbow-model-cost-function-N1pEX)) and perform backpropagation with each epoch to update the embedding layer in the process. The following code shows us our model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIvE6tw0_T6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "6d8cd09d-8689-4d85-9c43-19f791e97ab7"
      },
      "source": [
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=dim_embedding, input_length=window_size*2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim_embedding,)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "cbow.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 100)            355200    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3552)              358752    \n",
            "=================================================================\n",
            "Total params: 713,952\n",
            "Trainable params: 713,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52n-JSMMF6Wi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "d2723115-db0c-4e33-ae46-0f7f909d274c"
      },
      "source": [
        "# visualize model structure\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
        "                 rankdir='TB', dpi=65).create(prog='dot', format='svg'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"274pt\" viewBox=\"0.00 0.00 227.00 304.00\" width=\"205pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9028 .9028) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 223,-300 223,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139729596439744 -->\n<g class=\"node\" id=\"node1\">\n<title>139729596439744</title>\n<polygon fill=\"none\" points=\"12.5,-249.5 12.5,-295.5 206.5,-295.5 206.5,-249.5 12.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.5\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"92.5,-249.5 92.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"92.5,-272.5 150.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"150.5,-249.5 150.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-280.3\">[(?, 4)]</text>\n<polyline fill=\"none\" points=\"150.5,-272.5 206.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-257.3\">[(?, 4)]</text>\n</g>\n<!-- 139729597156656 -->\n<g class=\"node\" id=\"node2\">\n<title>139729597156656</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 219,-212.5 219,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-185.8\">Embedding</text>\n<polyline fill=\"none\" points=\"84,-166.5 84,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"84,-189.5 142,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"142,-166.5 142,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.5\" y=\"-197.3\">(?, 4)</text>\n<polyline fill=\"none\" points=\"142,-189.5 219,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.5\" y=\"-174.3\">(?, 4, 100)</text>\n</g>\n<!-- 139729596439744&#45;&gt;139729597156656 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139729596439744-&gt;139729597156656</title>\n<path d=\"M109.5,-249.3799C109.5,-241.1745 109.5,-231.7679 109.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"113.0001,-222.784 109.5,-212.784 106.0001,-222.784 113.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139729596439240 -->\n<g class=\"node\" id=\"node3\">\n<title>139729596439240</title>\n<polygon fill=\"none\" points=\"10,-83.5 10,-129.5 209,-129.5 209,-83.5 10,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-102.8\">Lambda</text>\n<polyline fill=\"none\" points=\"74,-83.5 74,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"74,-106.5 132,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"132,-83.5 132,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-114.3\">(?, 4, 100)</text>\n<polyline fill=\"none\" points=\"132,-106.5 209,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-91.3\">(?, 100)</text>\n</g>\n<!-- 139729597156656&#45;&gt;139729596439240 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139729597156656-&gt;139729596439240</title>\n<path d=\"M109.5,-166.3799C109.5,-158.1745 109.5,-148.7679 109.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"113.0001,-139.784 109.5,-129.784 106.0001,-139.784 113.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139729490305264 -->\n<g class=\"node\" id=\"node4\">\n<title>139729490305264</title>\n<polygon fill=\"none\" points=\"20,-.5 20,-46.5 199,-46.5 199,-.5 20,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"46\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"72,-.5 72,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"72,-23.5 130,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"130,-.5 130,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-31.3\">(?, 100)</text>\n<polyline fill=\"none\" points=\"130,-23.5 199,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-8.3\">(?, 3552)</text>\n</g>\n<!-- 139729596439240&#45;&gt;139729490305264 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139729596439240-&gt;139729490305264</title>\n<path d=\"M109.5,-83.3799C109.5,-75.1745 109.5,-65.7679 109.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"113.0001,-56.784 109.5,-46.784 106.0001,-56.784 113.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WHj9EZc_X6X",
        "colab_type": "text"
      },
      "source": [
        "We have input context words of dimensions ($2 \\times \\text{window_size}$), we will pass them to an embedding layer of size ($\\text{vocab_size}  \\times \\text{dim_embedding}$) which will give us dense word embeddings for each of these context words ($1 \\times \\text{dim_embedding}$ for each word). Next up we use a lambda layer to average out these embeddings and get an average dense embedding ($1 \\times \\text{dim_embedding}$) which is sent to the dense softmax layer which outputs the most likely target word. We compare this with the actual target word, compute the loss, backpropagate the errors to adjust the weights (in the embedding layer), and repeat this process for all (context, target) pairs for multiple epochs. The following figure tries to explain the same.\n",
        "\n",
        "![](https://i.ibb.co/f4bJF8k/cbow-model.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYCsRYj_7HWF",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to train this model on our corpus using our data generator to feed in (*context, target_word*) pairs (explanatory [video](https://www.coursera.org/lecture/probabilistic-models-in-nlp/training-a-cbow-model-forward-propagation-Vphwi) of the training process)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdlKHn0y_YvX",
        "colab_type": "text"
      },
      "source": [
        "#### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGG5rBhp_Y4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8bd684d-344b-431a-c4d1-4e6b9074a5e4"
      },
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    loss = 0.\n",
        "    for x, y in generate_data(corpus_ids, window_size, vocab_size):\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "\n",
        "    print(f'Epoch: {epoch}, loss: {loss}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 224614.19338440895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q1KFbb3_kYW",
        "colab_type": "text"
      },
      "source": [
        "**Note**: Running this model is computationally expensive and works better if we train it using a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpLOPA5f_nKg",
        "colab_type": "text"
      },
      "source": [
        "#### Get Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfR3jfw0_AdI",
        "colab_type": "text"
      },
      "source": [
        "To get word embeddings for our entire vocabulary, we can extract out the same from our embedding layer ([explanatory video](https://www.coursera.org/lecture/probabilistic-models-in-nlp/extracting-word-embedding-vectors-Py1VW)). We don’t take the embedding at position 0 since it belongs to the padding (`PAD`) term which is not really a word of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zP6tKpl_nRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "78b69ef1-45fc-4a08-edac-4f456c643d96"
      },
      "source": [
        "vectors = cbow.get_weights()[0]\n",
        "vectors = vectors[1:]\n",
        "print(vectors.shape)\n",
        "\n",
        "pd.DataFrame(vectors, index=list(id_word_dict.values())[1:]).head()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3551, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>”</th>\n",
              "      <td>-0.029552</td>\n",
              "      <td>-0.128178</td>\n",
              "      <td>-0.071386</td>\n",
              "      <td>-0.059629</td>\n",
              "      <td>-0.140173</td>\n",
              "      <td>-0.123153</td>\n",
              "      <td>-0.035233</td>\n",
              "      <td>-0.160361</td>\n",
              "      <td>-0.063094</td>\n",
              "      <td>0.139186</td>\n",
              "      <td>0.058532</td>\n",
              "      <td>0.134008</td>\n",
              "      <td>0.117726</td>\n",
              "      <td>0.016573</td>\n",
              "      <td>-0.054235</td>\n",
              "      <td>0.121164</td>\n",
              "      <td>0.013266</td>\n",
              "      <td>0.035842</td>\n",
              "      <td>0.214127</td>\n",
              "      <td>0.008796</td>\n",
              "      <td>-0.081649</td>\n",
              "      <td>0.017369</td>\n",
              "      <td>-0.005718</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>-0.041746</td>\n",
              "      <td>0.030866</td>\n",
              "      <td>-0.116763</td>\n",
              "      <td>0.075086</td>\n",
              "      <td>0.121884</td>\n",
              "      <td>0.003918</td>\n",
              "      <td>-0.057648</td>\n",
              "      <td>-0.005558</td>\n",
              "      <td>0.058962</td>\n",
              "      <td>0.018233</td>\n",
              "      <td>0.074702</td>\n",
              "      <td>0.146103</td>\n",
              "      <td>-0.185930</td>\n",
              "      <td>-0.176789</td>\n",
              "      <td>-0.035013</td>\n",
              "      <td>-0.017268</td>\n",
              "      <td>...</td>\n",
              "      <td>0.110904</td>\n",
              "      <td>-0.258919</td>\n",
              "      <td>-0.035852</td>\n",
              "      <td>0.035824</td>\n",
              "      <td>0.007883</td>\n",
              "      <td>0.014758</td>\n",
              "      <td>0.035187</td>\n",
              "      <td>0.133310</td>\n",
              "      <td>0.032935</td>\n",
              "      <td>0.190954</td>\n",
              "      <td>-0.111632</td>\n",
              "      <td>0.045459</td>\n",
              "      <td>0.066904</td>\n",
              "      <td>-0.064270</td>\n",
              "      <td>0.059865</td>\n",
              "      <td>0.158340</td>\n",
              "      <td>-0.010454</td>\n",
              "      <td>0.192761</td>\n",
              "      <td>0.027730</td>\n",
              "      <td>-0.115010</td>\n",
              "      <td>0.040528</td>\n",
              "      <td>-0.159061</td>\n",
              "      <td>0.044026</td>\n",
              "      <td>-0.358830</td>\n",
              "      <td>0.071318</td>\n",
              "      <td>0.039884</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>0.114593</td>\n",
              "      <td>-0.019946</td>\n",
              "      <td>0.040310</td>\n",
              "      <td>0.038064</td>\n",
              "      <td>0.125629</td>\n",
              "      <td>-0.122487</td>\n",
              "      <td>-0.110907</td>\n",
              "      <td>0.188948</td>\n",
              "      <td>-0.005223</td>\n",
              "      <td>-0.053620</td>\n",
              "      <td>0.036097</td>\n",
              "      <td>0.062824</td>\n",
              "      <td>0.012555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.117233</td>\n",
              "      <td>0.016544</td>\n",
              "      <td>0.101689</td>\n",
              "      <td>0.021456</td>\n",
              "      <td>-0.030479</td>\n",
              "      <td>0.026200</td>\n",
              "      <td>0.008636</td>\n",
              "      <td>-0.126505</td>\n",
              "      <td>0.072622</td>\n",
              "      <td>0.095569</td>\n",
              "      <td>-0.023006</td>\n",
              "      <td>0.008287</td>\n",
              "      <td>0.135392</td>\n",
              "      <td>0.027950</td>\n",
              "      <td>-0.025980</td>\n",
              "      <td>0.109073</td>\n",
              "      <td>0.016646</td>\n",
              "      <td>0.189715</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.021837</td>\n",
              "      <td>-0.040788</td>\n",
              "      <td>0.008121</td>\n",
              "      <td>0.093361</td>\n",
              "      <td>-0.048606</td>\n",
              "      <td>0.008493</td>\n",
              "      <td>-0.066345</td>\n",
              "      <td>-0.060154</td>\n",
              "      <td>-0.039572</td>\n",
              "      <td>-0.160502</td>\n",
              "      <td>-0.099071</td>\n",
              "      <td>-0.153187</td>\n",
              "      <td>-0.047099</td>\n",
              "      <td>-0.074506</td>\n",
              "      <td>0.035228</td>\n",
              "      <td>-0.034128</td>\n",
              "      <td>-0.046756</td>\n",
              "      <td>-0.014728</td>\n",
              "      <td>-0.028115</td>\n",
              "      <td>0.033962</td>\n",
              "      <td>-0.020422</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.115893</td>\n",
              "      <td>-0.102126</td>\n",
              "      <td>0.011516</td>\n",
              "      <td>-0.099970</td>\n",
              "      <td>0.036222</td>\n",
              "      <td>0.085219</td>\n",
              "      <td>0.047850</td>\n",
              "      <td>0.063395</td>\n",
              "      <td>-0.089816</td>\n",
              "      <td>0.155253</td>\n",
              "      <td>-0.071693</td>\n",
              "      <td>-0.033219</td>\n",
              "      <td>-0.076548</td>\n",
              "      <td>0.075853</td>\n",
              "      <td>0.032205</td>\n",
              "      <td>0.108184</td>\n",
              "      <td>0.077354</td>\n",
              "      <td>0.106102</td>\n",
              "      <td>0.109509</td>\n",
              "      <td>-0.117673</td>\n",
              "      <td>0.053231</td>\n",
              "      <td>-0.159656</td>\n",
              "      <td>0.043299</td>\n",
              "      <td>-0.057019</td>\n",
              "      <td>0.015827</td>\n",
              "      <td>0.136566</td>\n",
              "      <td>-0.056677</td>\n",
              "      <td>0.136419</td>\n",
              "      <td>0.136370</td>\n",
              "      <td>0.007497</td>\n",
              "      <td>-0.002596</td>\n",
              "      <td>0.084508</td>\n",
              "      <td>-0.080427</td>\n",
              "      <td>0.048260</td>\n",
              "      <td>0.076598</td>\n",
              "      <td>-0.039146</td>\n",
              "      <td>-0.007155</td>\n",
              "      <td>0.028067</td>\n",
              "      <td>-0.002938</td>\n",
              "      <td>0.018143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.035484</td>\n",
              "      <td>-0.066146</td>\n",
              "      <td>-0.034149</td>\n",
              "      <td>0.007305</td>\n",
              "      <td>0.032073</td>\n",
              "      <td>-0.054643</td>\n",
              "      <td>-0.062519</td>\n",
              "      <td>-0.056999</td>\n",
              "      <td>-0.054246</td>\n",
              "      <td>0.062831</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.034841</td>\n",
              "      <td>-0.042727</td>\n",
              "      <td>-0.004102</td>\n",
              "      <td>0.057236</td>\n",
              "      <td>0.010382</td>\n",
              "      <td>0.069961</td>\n",
              "      <td>0.020618</td>\n",
              "      <td>0.033784</td>\n",
              "      <td>-0.004615</td>\n",
              "      <td>0.044146</td>\n",
              "      <td>0.035211</td>\n",
              "      <td>-0.008897</td>\n",
              "      <td>-0.033554</td>\n",
              "      <td>0.037463</td>\n",
              "      <td>0.024240</td>\n",
              "      <td>-0.037834</td>\n",
              "      <td>-0.053897</td>\n",
              "      <td>0.019957</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.009200</td>\n",
              "      <td>-0.015458</td>\n",
              "      <td>-0.030871</td>\n",
              "      <td>0.014882</td>\n",
              "      <td>-0.022580</td>\n",
              "      <td>-0.024039</td>\n",
              "      <td>0.021216</td>\n",
              "      <td>-0.042906</td>\n",
              "      <td>-0.010084</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004377</td>\n",
              "      <td>-0.033147</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>-0.027348</td>\n",
              "      <td>0.022194</td>\n",
              "      <td>0.050563</td>\n",
              "      <td>0.048612</td>\n",
              "      <td>-0.015250</td>\n",
              "      <td>-0.006647</td>\n",
              "      <td>0.022340</td>\n",
              "      <td>0.010508</td>\n",
              "      <td>-0.042597</td>\n",
              "      <td>-0.031571</td>\n",
              "      <td>0.035349</td>\n",
              "      <td>0.007185</td>\n",
              "      <td>-0.022979</td>\n",
              "      <td>-0.017612</td>\n",
              "      <td>0.030991</td>\n",
              "      <td>0.013430</td>\n",
              "      <td>0.007389</td>\n",
              "      <td>0.042958</td>\n",
              "      <td>-0.033196</td>\n",
              "      <td>0.044754</td>\n",
              "      <td>-0.038929</td>\n",
              "      <td>0.037473</td>\n",
              "      <td>-0.022918</td>\n",
              "      <td>0.054604</td>\n",
              "      <td>0.040514</td>\n",
              "      <td>0.034267</td>\n",
              "      <td>-0.030979</td>\n",
              "      <td>0.009439</td>\n",
              "      <td>-0.003423</td>\n",
              "      <td>0.005517</td>\n",
              "      <td>0.018388</td>\n",
              "      <td>-0.003103</td>\n",
              "      <td>-0.002675</td>\n",
              "      <td>0.059223</td>\n",
              "      <td>0.029142</td>\n",
              "      <td>-0.043972</td>\n",
              "      <td>-0.007106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.027848</td>\n",
              "      <td>-0.004527</td>\n",
              "      <td>0.046239</td>\n",
              "      <td>0.039621</td>\n",
              "      <td>0.007900</td>\n",
              "      <td>0.007746</td>\n",
              "      <td>0.004744</td>\n",
              "      <td>-0.018295</td>\n",
              "      <td>-0.028796</td>\n",
              "      <td>-0.033999</td>\n",
              "      <td>-0.064780</td>\n",
              "      <td>0.021124</td>\n",
              "      <td>0.022901</td>\n",
              "      <td>-0.062046</td>\n",
              "      <td>0.010096</td>\n",
              "      <td>-0.012618</td>\n",
              "      <td>-0.009146</td>\n",
              "      <td>0.059334</td>\n",
              "      <td>0.043705</td>\n",
              "      <td>0.039751</td>\n",
              "      <td>-0.019967</td>\n",
              "      <td>0.008316</td>\n",
              "      <td>0.018225</td>\n",
              "      <td>0.033478</td>\n",
              "      <td>0.002128</td>\n",
              "      <td>-0.046766</td>\n",
              "      <td>-0.055147</td>\n",
              "      <td>0.005716</td>\n",
              "      <td>0.033384</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>-0.084112</td>\n",
              "      <td>0.020987</td>\n",
              "      <td>-0.048410</td>\n",
              "      <td>-0.003907</td>\n",
              "      <td>-0.020562</td>\n",
              "      <td>0.019196</td>\n",
              "      <td>0.003229</td>\n",
              "      <td>0.011947</td>\n",
              "      <td>-0.045300</td>\n",
              "      <td>0.045124</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028531</td>\n",
              "      <td>0.026697</td>\n",
              "      <td>-0.023553</td>\n",
              "      <td>-0.050060</td>\n",
              "      <td>0.048695</td>\n",
              "      <td>0.063267</td>\n",
              "      <td>-0.012563</td>\n",
              "      <td>0.009970</td>\n",
              "      <td>-0.058870</td>\n",
              "      <td>0.032463</td>\n",
              "      <td>0.003328</td>\n",
              "      <td>-0.015966</td>\n",
              "      <td>-0.039934</td>\n",
              "      <td>-0.032318</td>\n",
              "      <td>-0.004884</td>\n",
              "      <td>0.044362</td>\n",
              "      <td>0.024514</td>\n",
              "      <td>0.057996</td>\n",
              "      <td>0.002856</td>\n",
              "      <td>-0.031226</td>\n",
              "      <td>0.039527</td>\n",
              "      <td>0.015314</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.053099</td>\n",
              "      <td>0.045402</td>\n",
              "      <td>-0.002491</td>\n",
              "      <td>0.024632</td>\n",
              "      <td>0.049172</td>\n",
              "      <td>-0.005550</td>\n",
              "      <td>0.036584</td>\n",
              "      <td>0.006994</td>\n",
              "      <td>0.020529</td>\n",
              "      <td>-0.038625</td>\n",
              "      <td>-0.039016</td>\n",
              "      <td>-0.019945</td>\n",
              "      <td>0.005725</td>\n",
              "      <td>0.026745</td>\n",
              "      <td>0.013703</td>\n",
              "      <td>-0.038166</td>\n",
              "      <td>0.044844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.012924</td>\n",
              "      <td>-0.087829</td>\n",
              "      <td>-0.035109</td>\n",
              "      <td>-0.028797</td>\n",
              "      <td>0.006452</td>\n",
              "      <td>-0.012191</td>\n",
              "      <td>-0.027451</td>\n",
              "      <td>-0.041174</td>\n",
              "      <td>-0.009586</td>\n",
              "      <td>-0.013946</td>\n",
              "      <td>0.013780</td>\n",
              "      <td>-0.026399</td>\n",
              "      <td>0.021071</td>\n",
              "      <td>-0.054858</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.032470</td>\n",
              "      <td>-0.011261</td>\n",
              "      <td>0.040684</td>\n",
              "      <td>0.036073</td>\n",
              "      <td>0.028575</td>\n",
              "      <td>-0.030963</td>\n",
              "      <td>0.040903</td>\n",
              "      <td>0.002826</td>\n",
              "      <td>0.006051</td>\n",
              "      <td>-0.055422</td>\n",
              "      <td>-0.035734</td>\n",
              "      <td>-0.076959</td>\n",
              "      <td>0.047616</td>\n",
              "      <td>-0.024174</td>\n",
              "      <td>-0.017770</td>\n",
              "      <td>0.012372</td>\n",
              "      <td>0.052224</td>\n",
              "      <td>-0.017336</td>\n",
              "      <td>-0.011984</td>\n",
              "      <td>0.058699</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>-0.058143</td>\n",
              "      <td>-0.049876</td>\n",
              "      <td>-0.046568</td>\n",
              "      <td>0.027824</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>-0.035797</td>\n",
              "      <td>0.049714</td>\n",
              "      <td>-0.042115</td>\n",
              "      <td>0.030208</td>\n",
              "      <td>0.016677</td>\n",
              "      <td>0.033671</td>\n",
              "      <td>-0.026652</td>\n",
              "      <td>-0.024872</td>\n",
              "      <td>0.065304</td>\n",
              "      <td>-0.035745</td>\n",
              "      <td>0.051818</td>\n",
              "      <td>0.022686</td>\n",
              "      <td>-0.022643</td>\n",
              "      <td>-0.056507</td>\n",
              "      <td>0.058899</td>\n",
              "      <td>-0.036939</td>\n",
              "      <td>0.066628</td>\n",
              "      <td>-0.035605</td>\n",
              "      <td>0.010036</td>\n",
              "      <td>0.013141</td>\n",
              "      <td>-0.019055</td>\n",
              "      <td>0.022245</td>\n",
              "      <td>-0.036076</td>\n",
              "      <td>0.030963</td>\n",
              "      <td>-0.004276</td>\n",
              "      <td>0.072773</td>\n",
              "      <td>-0.006362</td>\n",
              "      <td>0.035447</td>\n",
              "      <td>0.006468</td>\n",
              "      <td>0.025585</td>\n",
              "      <td>-0.021107</td>\n",
              "      <td>0.001213</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.019565</td>\n",
              "      <td>0.018664</td>\n",
              "      <td>0.024765</td>\n",
              "      <td>0.007464</td>\n",
              "      <td>-0.015115</td>\n",
              "      <td>-0.015356</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2   ...        97        98        99\n",
              "”   -0.029552 -0.128178 -0.071386  ...  0.036097  0.062824  0.012555\n",
              "and -0.117233  0.016544  0.101689  ...  0.028067 -0.002938  0.018143\n",
              "to  -0.035484 -0.066146 -0.034149  ...  0.029142 -0.043972 -0.007106\n",
              "a    0.027848 -0.004527  0.046239  ...  0.013703 -0.038166  0.044844\n",
              "of  -0.012924 -0.087829 -0.035109  ...  0.007464 -0.015115 -0.015356\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jZ53oUB9OCE",
        "colab_type": "text"
      },
      "source": [
        "We can see that each word has a dense embedding of size ($1 \\times 100$) as depicted in the preceding output. Let’s try and find out some contextually similar words for specific words of interest based on these embeddings. For this, we build out a pairwise distance matrix amongst all the words in our vocabulary based on the dense embedding vectors and then find out the n-nearest neighbors of each word of interest based on the shortest (euclidean) distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1WTUWGL9Gp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "73918b22-1ab3-4030-afa1-76acfb352c3b"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# compute pairwise distance matrix\n",
        "distance_matrix = euclidean_distances(vectors)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "# view contextually similar words\n",
        "similar_words = {search_term: [id_word_dict[idx] for idx in distance_matrix[word_id_dict[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['walk', 'surprised', 'slowly', 'remarkable', 'alice']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3551, 3551)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alice': ['cart', 'she', 'is—oh', 'thought', 'mock'],\n",
              " 'remarkable': ['problem', 'deeply', 'prizes', 'sluggard', 'examining'],\n",
              " 'slowly': ['paused', 'child', 'cry', 'one—but', 'shaped'],\n",
              " 'surprised': ['quickly', 'kneel', 'absence', 'late', 'bank—the'],\n",
              " 'walk': ['began', 'resource', 'dressed', 'editions', 'enough—i']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9VwCGUWkrVR",
        "colab_type": "text"
      },
      "source": [
        "We can use Cosine distance to evaluate vector correlations.\n",
        "\n",
        "![](https://i.ibb.co/C1YxjH2/cosine-similarity.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI0XYc8Mkett",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "1a4daafa-c49b-4048-e442-39fbf2e35363"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# compute pairwise distance matrix\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "print(similarity_matrix.shape)\n",
        "\n",
        "# view contextually similar words\n",
        "similar_words = {search_term: [id_word_dict[idx] for idx in similarity_matrix[word_id_dict[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['walk', 'surprised', 'slowly', 'remarkable', 'alice']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3551, 3551)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alice': ['remembering', 'teases', 'telescope', 'advise', 'kettle'],\n",
              " 'remarkable': ['row', 'city', 'improve', 'short', 'nonsense'],\n",
              " 'slowly': ['“i’m', 'quiet', 'morals', 'procession', '“serpent'],\n",
              " 'surprised': ['days', 'gloves—that', 'grumbled', 'beheading', 'wildly'],\n",
              " 'walk': ['b', 'such', 'along—“catch', 'credit', '“first']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPeEkt5OlxGV",
        "colab_type": "text"
      },
      "source": [
        "## Subsampling of frequent words and negative sampling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPyMQ6H2l08F",
        "colab_type": "text"
      },
      "source": [
        "When we have created the CBOW model for Word2Vec, you may have noticed something. It’s a huge neural network!\n",
        "\n",
        "Suppose we had word vectors with 300 components and a vocabulary of 10000 words. Recall that the neural network had two weight matrices, a hidden layer, and an output layer. Both of these layers would have a weight matrix with $300 \\times 10000 = 3$ million weights each!\n",
        "\n",
        "Running gradient descent on a neural network that large is going to be slow. And to make matters worse, we need a huge amount of training data to tune that many weights and avoid over-fitting.\n",
        "\n",
        "The authors of Word2Vec addressed these issues in their second [paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with the following two innovations:\n",
        "\n",
        "- Subsampling frequent words to decrease the number of training examples.\n",
        "\n",
        "- Modifying the optimization objective with a technique they called *Negative Sampling*, which causes each training sample to update only a small percentage of the model’s weights.\n",
        "\n",
        "These techniques not only reduced the compute burden of the training process but also improved the quality of their resulting word vectors as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10O5tRMOq3Ma",
        "colab_type": "text"
      },
      "source": [
        "### Subsampling Frequent Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8zrD5uhq4OQ",
        "colab_type": "text"
      },
      "source": [
        "We have already seen how to create training samples from the source text. We found all word pairs of target and context words to form a dataset in the format of (*context_window, target_word*).\n",
        "\n",
        "![](https://i.ibb.co/ggCDLSn/context-and-target-word-word2vec.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwSrRy7EtqT2",
        "colab_type": "text"
      },
      "source": [
        "There are two \"problems\" with common words like \"is\" or \"and\":\n",
        "\n",
        "- When looking at the $5^{\\text{st}}$ window ([\"is\", \"very\", \"and\", \"fun\"],\"hard\"). The words \"is\" or \"and\" don’t tell us much about the meaning of “hard” and they appear in the context of pretty much every word.\n",
        "\n",
        "- We will have many more samples of (“is”,…) than we need to learn a good vector for “is”.\n",
        "\n",
        "Word2Vec implements a “subsampling” scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word’s frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLzGdb8hJDjf",
        "colab_type": "text"
      },
      "source": [
        "#### Sampling rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_coluqvJQVE",
        "colab_type": "text"
      },
      "source": [
        "The word2vec C code implements an equation for calculating a probability with which to keep a given word in the vocabulary.\n",
        "\n",
        "Given a word $w_i$, we define $z(w_i)$ as the fraction of the total words in the corpus that are that word. For example, if the word \"and\" occurs 1000 times in a 1 million word corpus, then $z(\\text{\"and\"}) = \\frac{1000}{1000000}$. A new parameter, called *sample* controls how much subsampling occurs, and the default value is 0.001. Smaller values of *sample* mean words are less likely to be kept. The probability of keeping the word $w_i$, $P(w_i)$, is given by the equation:\n",
        "\n",
        "$$P(w_i)=(\\sqrt{\\frac{z(w_i)}{\\text{sample}}}+1)\\frac{\\text{sample}}{z(w_i)}$$\n",
        "\n",
        "Thre are some interesting points in this function (we use the default sample value of 0.001).\n",
        "\n",
        "- $P(w_i)=1.0$  (100% chance of word being kept) when $z(w_i)\\leq 0.0026$. This means that only words which represent more than 0.26% of the total words will be subsampled.\n",
        "\n",
        "- $P(w_i)=0.5$ (50% chance of word being kept) when $z(w_i)=0.00746$.\n",
        "\n",
        "- $P(w_i)=0.033$ (3.3% chance of being kept) when $z(w_i)=1.0$.\n",
        "That is, if the corpus consisted entirely of word $w_i$, which is ridiculous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZJliel9q4iO",
        "colab_type": "text"
      },
      "source": [
        "### Negative sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zfYB_8l3HU",
        "colab_type": "text"
      },
      "source": [
        "In the word2vec models, each training sample will tweak all of the weights in the neural network.\n",
        "\n",
        "A big size of our word vocabulary means that our model has a tremendous number of weights, all of which would be updated slightly by every one of training samples! *Negative sampling* addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. \n",
        "\n",
        "When training the network on the window ([\"is\", \"very\", \"and\", \"fun\"],\"hard\"), recall that the \"label\" or \"correct output\" of the network is a one-hot vector. That is, for the output neuron corresponding to \"hard\" to output a 1, and for **all** of the other thousands of output neurons to output a 0.\n",
        "\n",
        "With negative sampling, we are instead going to randomly select just a small number of \"negative\" words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our \"positive\" word (which is the word \"hard\" in our current example).\n",
        "\n",
        "**Note**: The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.\n",
        "\n",
        "If we have a vocabulary size of 10000 words and we use a embedding dimension of 300, the output layer of our model will have a weight matrix that’s $300 \\times 10000$. So we will just be updating the weights for our positive word (\"hard\"), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1800 weight values total. That’s only 0.06% of the 3M weights in the output layer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0lYeC89N4kG",
        "colab_type": "text"
      },
      "source": [
        "#### Selecting negative samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz3W3oBaN9Mq",
        "colab_type": "text"
      },
      "source": [
        "The *negative samples* (that is, the 5 output words that we’ll train to output 0) are selected using a *unigram distribution*, where more frequent words are more likely to be selected as negative samples.\n",
        "\n",
        "For instance, suppose we had our entire training corpus as a list of words, and we chose our 5 negative samples by picking randomly from the list. In this case, the probability for picking a particular word would be equal to the number of times that this word appears in the corpus, divided by the sum of the occurrences of each other words in the corpus. \n",
        "\n",
        "$$P(w_i)=\\frac{f(w_i)}{\\sum_{j=0}^{n}(f(w_j))}$$\n",
        "\n",
        "The authors state in their paper that they tried several variations on this equation, and the one which performed best was to raise the word counts to the $\\frac{3}{4}$ power:\n",
        "\n",
        "$$P(w_i)=\\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{j=0}^{n}(f(w_j)^{\\frac{3}{4}})}$$\n",
        "\n",
        "Compared to the simpler equation, this one tends to increase the probability for less frequent words and decrease the probability for more frequent words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-nH18AFGXEO",
        "colab_type": "text"
      },
      "source": [
        " ## Implementing CBOW model with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9nxmvjqIMc1",
        "colab_type": "text"
      },
      "source": [
        "[Gensim](https://github.com/RaRe-Technologies/gensim) is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. This library provides the `Word2Vec` class for working with a Word2Vec model. Learning a word embedding from text involves loading and organizing the text into sentences and providing them to the constructor of a new `Word2Vec()` instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukawyq5FFALt",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSUE8ldeIUpE",
        "colab_type": "text"
      },
      "source": [
        "Each sentence must be tokenized (divided into words). The sentences could be text loaded into memory or an iterator that progressively loads the text, required for very large text corpora. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABhM71EVFFii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c7d70c70-1c28-4429-9818-a6eb7d6265be"
      },
      "source": [
        "print(corpus[0:10])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffThe Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n', '\\n', 'This eBook is for the use of anyone anywhere at no cost and with\\n', 'almost no restrictions whatsoever.  You may copy it, give it away or\\n', 're-use it under the terms of the Project Gutenberg License included\\n', 'with this eBook or online at www.gutenberg.org\\n', '\\n', '\\n', 'Title: Alice’s Adventures in Wonderland\\n', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly-KvWGzerbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "be596cf4-b23b-4a5b-9b08-1620654c5751"
      },
      "source": [
        "sentences = [sentence.split() for sentence in corpus]\n",
        "print(sentences[0:10])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Alice’s', 'Adventures', 'in', 'Wonderland,', 'by', 'Lewis', 'Carroll'], [], ['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with'], ['almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or'], ['re-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included'], ['with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org'], [], [], ['Title:', 'Alice’s', 'Adventures', 'in', 'Wonderland'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSWeBfurIY6Z",
        "colab_type": "text"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgNVBQSjPt5q",
        "colab_type": "text"
      },
      "source": [
        "We prefer to separate the training process in 3 distinctive steps for clarity and monitoring:\n",
        "\n",
        "1-  `Word2Vec()`: In this first step, we set up the parameters of the model one-by-one.\n",
        "We do not supply the sentences, and therefore leave the model uninitialized, purposefully. There are many parameters on this constructor:\n",
        "\n",
        "- `size`: (default 100) The number of dimensions of the embedding, that is the length of the dense vector to represent each token (word).\n",
        "\n",
        "- `window`: (default 5) The maximum distance between a target word and words around the target word.\n",
        "\n",
        "- `min_count`: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
        "\n",
        "- `negative`: If > 0, negative sampling will be used, the integer value specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used.\n",
        "\n",
        "- `sample`: (default 1e-5) The threshold for configuring which higher-frequency words are randomly downsampled.\n",
        "\n",
        "- `workers`: (default 3) The number of threads to use while training.\n",
        "\n",
        "- `sg`: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip-gram (1).\n",
        "\n",
        "- `alpha`: The initial learning rate.\n",
        "\n",
        "- `min_alpha`: Learning rate will linearly drop to min_alpha as training progresses.\n",
        "\n",
        "2- `.build_vocab()`: It builds the vocabulary from a sequence of sentences and thus initialized the model. With the loggings, we can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. These two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for more accurate and easier management of their influence.\n",
        "\n",
        "3.- `.train()`: Finally, trains the model. The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously. The parameters of the training are:\n",
        "- `total_examples`: Count of sentences.\n",
        "- `epochs`: Number of iterations (epochs) over the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4XQGNMBRdVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fbafbbe-5997-42e8-f5e6-fecd77be45dd"
      },
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8US6I_t7fAVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     size=100,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=cores-1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4NiHWRCCXJP",
        "colab_type": "text"
      },
      "source": [
        "We build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVxxZQwHRjuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fb25129-7ba3-44e0-8519-c76bd83187f8"
      },
      "source": [
        "from time import time\n",
        "\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=100)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.0 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhQ44CtgCMfV",
        "colab_type": "text"
      },
      "source": [
        "We can print the vocabulary of tokens (words):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR0JK8pMBszV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6744a2f2-4f03-4019-fa67-f73800b7b588"
      },
      "source": [
        "words = list(w2v_model.wv.vocab)\n",
        "print(words)\n",
        "len(words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Project', 'Gutenberg', 'of', 'in', 'by', 'This', 'is', 'for', 'the', 'use', 'at', 'no', 'and', 'with', 'may', 'it,', 'it', 'or', 'under', 'terms', 'this', 'set', 'CHAPTER', 'The', 'a', 'Rabbit', 'from', 'Mock', 'Alice', 'was', 'to', 'get', 'very', 'her', 'on', 'nothing', 'she', 'had', 'into', 'but', '“and', 'what', 'thought', 'So', 'well', 'as', 'made', 'would', 'be', 'getting', 'up', 'when', 'White', 'There', 'so', 'did', 'think', 'much', 'out', 'way', 'say', 'I', 'shall', 'over', 'that', 'have', 'time', 'all', 'seemed', 'quite', 'its', 'looked', 'then', 'on,', 'never', 'before', 'after', 'just', 'see', 'down', 'large', 'another', 'moment', 'went', 'how', 'like', 'some', 'not', 'about', 'herself', 'found', 'look', 'going', 'make', 'too', 'they', 'were', 'there', 'upon', 'She', 'took', 'one', 'great', 'put', 'herself,', 'such', 'me', 'if', 'off', 'come', 'an', '“I', 'I’ve', 'said', 'must', 'you', 'things', 'good', 'right', 'got', 'began', 'their', 'rather', 'them', 'could', 'little', 'do', 'soon', 'should', 'my', 'wish', 'are', 'I’m', 'might', 'And', 'which', 'felt', 'tell', 'came', 'long', 'it’s', 'round', 'been', 'first', 'any', 'than', 'head', 'go', 'poor', 'Alice,', 'without', 'only', 'back', 'half', 'find', 'I’ll', 'said,', 'who', 'other', 'will', 'your', 'you,', '*', '“What', 'now', 'more', 'thing', 'two', '“Well,', 'can', 'don’t', 'same', 'won’t', '“You', 'heard', 'he', 'next', 'can’t', 'know', 'his', 'again,', 'them,', 'we', 'last', 'him', 'Alice.', 'looking', 'added', '“It', 'three', 'work', 'Queen', 'March', 'Hatter', 'Dormouse', 'King', 'Turtle', 'Gryphon', 'works', 'Gutenberg-tm', 'electronic']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVImHiW7Cdhm",
        "colab_type": "text"
      },
      "source": [
        "We train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Z0Hb_tRlt7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd9401fd-e7db-4c4d-a59f-1ae2207659e4"
      },
      "source": [
        "t = time()\n",
        "\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to train the model: 0.01 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PziqjbZFhcH",
        "colab_type": "text"
      },
      "source": [
        "#### Exploring the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaAs8xjvgF3w",
        "colab_type": "text"
      },
      "source": [
        "As we do not plan to train the model any further, we are calling `init_sims()`, which will make the model much more memory-efficient:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bip06ksUgIEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model.init_sims(replace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jek6eh76gPfy",
        "colab_type": "text"
      },
      "source": [
        "Here, we will ask our model to find the word most similar to some words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7HqX-egYhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f2602c1a-9535-4fc4-9f23-a79ea164d78a"
      },
      "source": [
        "w2v_model.wv.most_similar(positive=[\"Alice\"])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('moment', 0.9998019933700562),\n",
              " ('think', 0.9997971057891846),\n",
              " ('So', 0.9997895956039429),\n",
              " ('added', 0.9997886419296265),\n",
              " ('on', 0.9997850656509399),\n",
              " ('got', 0.9997814893722534),\n",
              " ('heard', 0.9997798204421997),\n",
              " ('them', 0.9997791051864624),\n",
              " ('into', 0.9997789263725281),\n",
              " ('said,', 0.9997769594192505)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ygBWwNQgpkn",
        "colab_type": "text"
      },
      "source": [
        "We can see how similar are two words to each other:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZyNynUhgqd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6eb5e863-854a-47b9-abed-2625ed6689f4"
      },
      "source": [
        "w2v_model.wv.similarity(\"the\", 'for')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99965125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUX7a1nFIyyG",
        "colab_type": "text"
      },
      "source": [
        "We can review the embedded vector for a specific token as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do6zzT6kI0-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "497bff52-fb38-410f-8b2e-34763e47f93c"
      },
      "source": [
        "print(w2v_model['the'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.21375056  0.16120857 -0.11373315  0.11415086  0.06541886 -0.07892276\n",
            " -0.06338476  0.1286254  -0.00990151 -0.12290736  0.16195273  0.00729052\n",
            "  0.09874725 -0.20527722 -0.12823749  0.05746158 -0.05147931 -0.15502775\n",
            "  0.03582692  0.06739356  0.02578196 -0.16577996 -0.01026801  0.01780486\n",
            "  0.01364862  0.03532826 -0.09602528  0.20533538 -0.17952584 -0.0897965\n",
            " -0.10752189  0.11784374 -0.01617444 -0.0311919  -0.09223859 -0.06196856\n",
            "  0.00785304  0.0197102  -0.04045342 -0.10091233  0.12190437  0.09393188\n",
            " -0.07813845 -0.07392438  0.05129271  0.04714946  0.01501277  0.04121938\n",
            " -0.17472431  0.14947602 -0.12645413 -0.00199613 -0.01401774 -0.14616165\n",
            " -0.05297752 -0.07084643  0.11410096  0.01783177  0.01221659  0.00405719\n",
            "  0.18528333  0.1244683   0.09965014 -0.02793783  0.01055666 -0.06053714\n",
            "  0.12303355  0.08367602  0.02736081  0.12617749 -0.02532187 -0.05227293\n",
            "  0.09856843  0.11244963 -0.13938023 -0.12979284 -0.16459724 -0.00673519\n",
            "  0.0708756  -0.11254862 -0.09267356 -0.14684954  0.01325268 -0.0866816\n",
            " -0.03522303 -0.12610011 -0.02110455  0.05894635  0.03735592  0.06252424\n",
            "  0.08392242  0.07418985  0.1062441   0.02574546 -0.03568689 -0.11813497\n",
            " -0.08741714 -0.1492942  -0.17814015  0.15274765]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqAY-1r9Fn8q",
        "colab_type": "text"
      },
      "source": [
        "#### Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oocgkHgI38C",
        "colab_type": "text"
      },
      "source": [
        "Finally, a trained model can then be saved to file by calling the `save_word2vec_format()` function on the word vector model. By default, the model is saved in a binary format to save space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cezlz8Q0I4cQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d8f53857-4580-463d-8f26-fc412764b814"
      },
      "source": [
        "w2v_model.wv.save_word2vec_format('model.bin')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhbQpNKdI7nd",
        "colab_type": "text"
      },
      "source": [
        "When getting started, we can save the learned model in ASCII format and review the contents. We can do this by setting `binary=False` when calling the `save_word2vec_format()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmhbgXrHI9H0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "15a18faa-d748-4534-dd88-66c59cee8707"
      },
      "source": [
        "w2v_model.wv.save_word2vec_format('model.txt', binary=False)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRdUAWZ5I-zp",
        "colab_type": "text"
      },
      "source": [
        "The saved model can then be loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18rjLwCTBdeP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b5bbf3a5-1752-4f0e-e0a9-3acafdcc5b6f"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format('/content/model.bin', binary=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0qIgxl0JQHG",
        "colab_type": "text"
      },
      "source": [
        "We can see that with a little work to prepare your text document, you can create your own word embedding very easily with Gensim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M79b4AYJBcd",
        "colab_type": "text"
      },
      "source": [
        "## Load Google’s Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZoF-xVfF3UU",
        "colab_type": "text"
      },
      "source": [
        "Training our own word vectors may be the best approach for a given NLP problem. But it can take a long time, a fast computer with a lot of RAM and disk space, and perhaps some expertise in finessing the input data and training algorithm. An alternative is to simply use an existing pre-trained word embedding. \n",
        "\n",
        "Along with the paper and code for Word2Vec, Google also published a pre-trained Word2Vec model on the Word2Vec Google Code Project.\n",
        "A pre-trained model is nothing more than a file containing tokens and their associated word vectors. The pre-trained Google Word2Vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors. It is a 1.53 Gigabyte file. You can download it from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp9z2SNKJgQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6092cf70-518d-4791-a29a-3e4e4912f1bc"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "filename = '/content/drive/My Drive/GoogleNews-vectors-negative300.bin'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg47x_92_sAm",
        "colab_type": "text"
      },
      "source": [
        "**Note**: This example may require a workstation with 8 or more Gigabytes of RAM to execute.\n",
        "\n",
        "An interesting thing that you can do is do a little linear algebra arithmetic with words. For example, a popular example\n",
        "described in lectures and introduction papers is:\n",
        "\n",
        "$$\\text{queen} = (king - man) + woman$$\n",
        "\n",
        "That is the word \"queen\" is the closest word given the subtraction of the notion of \"man\" from \"king\" and adding the word \"woman\". The man-ness in \"king\" is replaced with woman-ness to give us \"queen\". A very cool concept. Gensim provides an interface for performing these types of operations in the `most_similar()` function on the trained or loaded model. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HZ6e-ZhAa1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e7436049-c6f3-4864-e3c5-f535684b2a5c"
      },
      "source": [
        "result = model.most_similar(positive=['woman','king'],\n",
        "                            negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('queen', 0.7118192911148071)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jQmiweSPhjH",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC0pFkcZPi9X",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
        "\n",
        "- [Distributed Representations of Words and Phrases\n",
        "and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
        "\n",
        "- https://github.com/nzw0301/keras-examples/blob/master/CBoW.ipynb\n",
        "\n",
        "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
        "\n",
        "- https://www.coursera.org/lecture/probabilistic-models-in-nlp/overview-4k0An\n",
        "\n",
        "- https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html\n",
        "\n",
        "- http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
        "\n",
        "- Deep Learning for Natural Language Processing, Jason Brownlee"
      ]
    }
  ]
}