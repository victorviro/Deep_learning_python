{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Vectorization NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMz7O1LITvCm43nN98cFVzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Text_Vectorization_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A_L2V8hYNpO"
      },
      "source": [
        "## Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxpQ2rdpVNWk"
      },
      "source": [
        "\n",
        "1. [Introduction to text vectorization](#1)\n",
        "2. [Label encoding and One-hot encoding](#2)\n",
        "3. [Bag-of-words](#3)\n",
        "    1. [Bag-of-words variations](#3.1)\n",
        "    2. [Design the vocabulary of words](#3.2)\n",
        "    3. [Limitations of bag-of-words](#3.3)\n",
        "4. [One-hot encoding ordered](#4)\n",
        "5. [Word embeddings](#5)\n",
        "    1. [Embedding layer](#5.1)\n",
        "    2. [Word2Vec](#5.2)\n",
        "    3. [GloVe](#5.3)\n",
        "    4. [FastText](#5.4)\n",
        "    5. [Contextualized word embeddings](#5.5)\n",
        "6. [Paragraph embeddings](#6)\n",
        "7. [References](#7)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbBJJu5ZdRmM"
      },
      "source": [
        "#  Introduction to text vectorization <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld-6PshKqfmM"
      },
      "source": [
        "Text data requires special preparation before we can start using it for predictive modeling. Besides cleaning the text, we cannot work with the text directly when using machine learning algorithms because they are well suited to manage numbers. For example, images are matrices that contain numeric values where each number depicts the intensity of a pixel. Or when fitting a linear regression model where we have numerical and categorical features we also need to transform the categorical variables into numerical variables. Therefore, we need to convert the text to numbers. This process is called *feature extraction* (or *vectorization*).\n",
        "\n",
        "Suppose we have a dataset which contains the text of film reviews and another variable which indicates if a particular review has a positive or negative sentiment (a binary variable where the value 1 means that the review is positive and 0 otherwise). In this classification task, we want to train a model to predict the sentiment of new reviews do not still labeled. In the NLP context, each raw text is usually called *document* (in this case each review is a document). The set of all row text is usually known as *corpus* (in this case the corpus is the set of all reviews). The *vocabulary* of the corpus is the set of all different words in the documents. \n",
        "\n",
        "**Note**: We can encode sequences of words (sentences or documents), words, characters, or even sub-words. The choice depends on the problem we want to solve and how we want to focus on. For example, encoding words is easier to generate realistic text. Although if we encode characters, the algorithm could even generate words that do not exist. Each of these units which form our sequences of data are called *tokens*. The process of divide the sequences into these blocks is called *tokenization*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PkEalYXIAYC"
      },
      "source": [
        "# Label encoding and One-hot encoding <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPYOMn47IDa8"
      },
      "source": [
        "A first idea to encode sentences or documents of words is to assign each word of the vocabulary with a unique number (an integer value).\n",
        "\n",
        "![](https://i.ibb.co/zs0yb2J/integer-encoding.png)\n",
        "\n",
        "This method preserves the order of the words as they occur in the sentences. It is usually used to encode the output target in classification tasks. In the example of sentiment analysis, the sentiment of the review is encoded as 1 positive and 0 negative. In this context, this method is known as *label encoding* or *integer encoding*. \n",
        "\n",
        "However this method is inefficient for encoding documents or when we transform categorical features in numerical because the integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship, although, for [ordinal variables](https://en.wikipedia.org/wiki/Ordinal_data), this may be enough. Imagine a dataset about people where there are columns like \"nationality\", \"age\", etc. A training algorithm could understand that a person who is 30 years old is twice as old as a person who is 15 years old. Suppose we transform the categorical variable \"nationality\" into a numerical variable using this encoding ( 0->Spanish, 1->English and 2->German). The algorithm could understand that a person from Germany has twice as a \"nationality\" as a person from England!. \n",
        "\n",
        "To solve this, the *one-hot* encoded is used. Instead of assigning a single number to each label, we assign a vector. A binary variable is added for each category.\n",
        "\n",
        "![](https://i.ibb.co/zxGhQkB/one-hot-encoding.png)\n",
        "\n",
        "\n",
        "In this case, the distance between all encoded-words is the same so we do not have the problem we saw previously. A very similar method called *dummy variables*  is commonly used for transforming categorical features to numerical in many machine learning tasks. \n",
        "\n",
        "Let's see one of the most traditional methods to encode sequences of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzPkf0IQII3f"
      },
      "source": [
        "# Bag of words <a name=\"3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Fqm-8lUSzr"
      },
      "source": [
        "A bag-of-words model, or BoW for short, is a way of extracting features from sentences for use in modeling, such as with machine learning algorithms. It is called a \"bag\" of words because any information about the order or structure of words in the document is discarded. The intuition is that documents are similar if they have similar content.\n",
        "\n",
        "We represent every document from the corpus as a fixed-length vector whose length is equal to the vocabulary size of the corpus. \n",
        "\n",
        "![](https://i.ibb.co/d03jmGM/encoding-documents-as-vectors.png)\n",
        "\n",
        "\n",
        "The bag-of-words can be as simple or complex as we like. The complexity comes both in deciding how to design the vocabulary of known words (bag of $N$-grams) and how to score the presence of known words. We will look at three types regarding how we score the presence of words in the document: one-hot, frequency, and TF–IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvQZiDDBdPdo"
      },
      "source": [
        "## Baf of words variations <a name=\"3.1\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GwFgc6PdRDc"
      },
      "source": [
        "**One-hot document vectorization (not ordered)**\n",
        "\n",
        "In this method, each document then can be encoded as a fixed-length binary vector with the length of the vocabulary of known words where 1 encodes the presence of the word and 0 its absence. \n",
        "\n",
        "![](https://i.ibb.co/XkX8tyG/one-hot-document-not-ordered2.png)\n",
        "\n",
        "**Frequency-based document vectorization**\n",
        "\n",
        "A variation of the previous method uses the frequency or count of each word in the document instead of just its presence/absence. This variation is called *frequency-based* vectorization.\n",
        "\n",
        "![](https://i.ibb.co/8X3HJxq/frequence-based-vectorization.png)\n",
        "\n",
        "This representation can either be a straight count (integer) encoding as shown in the previous figure or a normalized encoding where each word is weighted by the total number of words in the document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjP9QSKIctjS"
      },
      "source": [
        "**TF-IDF**\n",
        "\n",
        "A problem with the previous technique is that highly frequent words start to dominate in the document (larger score), but may not contain as much “informational content” to the model as rarer but perhaps domain-specific words.\n",
        "\n",
        "One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized.\n",
        "\n",
        "This approach to scoring is called *Term Frequency-Inverse Document Frequency (TF-IDF)*. \n",
        "\n",
        "- *Term Frequency*: is a scoring of the frequency of the word in the current document.\n",
        "- *Inverse Document Frequency*: is a scoring of how rare the word is across all documents.\n",
        "\n",
        "We interpret the score to mean that the closer the TF–IDF score of a term is to 1, the more informative that term is to that document. The closer the score is to zero, the less informative that term is. In the next figure, the token \"studio\" has higher relevance to this document since it appears with less frequency in the rest of the documents.\n",
        "\n",
        "![](https://i.ibb.co/6P9wFJ5/tf-idf.png)\n",
        "\n",
        "\n",
        "TF–IDF is computed on a per-term basis, such that the relevance of a token to a document is measured by the scaled frequency of the appearance of the term in the document, normalized by the inverse of the scaled frequency of the term in the entire corpus.\n",
        "\n",
        "The term frequency of a term given a document, $\\text{tf}(t,d)$, can be the boolean frequency (as in one-hot encoding, 1 if $t$ occurs in $d$  and 0 otherwise), or the count. However, generally both the term frequency and inverse document frequency are scaled logarithmically to prevent bias of longer documents or terms that appear much more frequently relative to other terms:\n",
        "$\\text{tf}(t,d)=1+\\text{log}(f_{t,d})$.\n",
        "\n",
        "Similarly, the inverse document frequency of a term given the set of documents can be logarithmically scaled as follows: $\\text{idf}(t,D)=\\text{log}(1)+N \\cdot n_t$\n",
        ", where $N$ is the number of documents and $n_t$ is the number of occurrences of the term $t$ in all documents. TF–IDF is then computed completely as $\\text{tfidf }(t,d,D)=\\text{tf}(t,d) \\cdot \\text{idf}(t,D)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r3W8OWWVoVq"
      },
      "source": [
        "The scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of our text data. The class `CountVectorizer()` can be used for frequency-based document vectorization and the  class, and `TfidfVectorizer` for TF-IDF. An example using the TF-IDF vectorizer for a sentiment analysis task can be found [here](https://www.kaggle.com/viroviro/sentiment-analysis-tf-idf-logistic-regression). Alternatively we can use Gensim or NLTK (see implementation [here](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knzl0nJ1mDUL"
      },
      "source": [
        "## Design the vocabulary of words <a name=\"3.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bR-UFVxmM1h"
      },
      "source": [
        "Since every document from the corpus is represented as a fixed-length vector whose length is equal to the vocabulary size of the corpus, if the vocabulary size increases, so the vector representation of documents does. For a very large corpus, the length of the vector might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary. This results in a vector with lots of zero scores, which is known as sparse vector or *sparse representation*. These vectors require more computational resources when modeling and can make the modeling process very challenging for traditional algorithms (note that for some algorithms like a naive Bayes classifier can scale well with high dimensionality while for others is a major issue).\n",
        "\n",
        "As such, there is pressure to decrease the size of the vocabulary when using a bag-of-words model. There are text cleaning techniques that, depending on the problem at hand, can be used as a first step, such as:\n",
        "\n",
        "- Ignoring punctuation\n",
        "- Ignoring frequent words that don’t contain much information, called *stop words*, like \"a\", \"of\", etc.\n",
        "- Fixing misspelled words.\n",
        "- Reducing words to their stem (e.g. \"play\" from \"playing\") is called  *Stemming*. Reducing words to their lemma (e.g. \"be\" from \"was\") is called *Lemmatization*.\n",
        "\n",
        "It is also common to limit the number of most frequent words from a vocabulary (e.g. 2000) and represent documents as frequencies of only those selected terms.\n",
        "\n",
        "A more sophisticated approach is to create a vocabulary of grouped words. This changes the vocabulary size and allows the bag-of-words to capture a little more meaning from the document. In this approach, each word or token is called a \"gram\". Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams. The general approach is called the $n$-gram model, where $n$ refers to the number of grouped words.\n",
        "\n",
        "![](https://i.ibb.co/r7m59h8/n-grams.png)\n",
        "\n",
        "For example, the sentences \"the movie was good, not bad at all\" and \"the movie was bad, not good at all\" have the opposite meaning. However, their encoding will be equal if we use a unigram model since both sentences have the same words. We can solve this issue using a bigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yagvKEHgkCFf"
      },
      "source": [
        "## Limitations of Bag-of-Words <a name=\"3.3\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIM6qjT3l2qW"
      },
      "source": [
        "The bag-of-words model is simple, easy to understand, and has been used with success on prediction problems like document classification. Nevertheless, it suffers from some shortcomings, such as:\n",
        "\n",
        "- **Vocabulary**: The vocabulary requires careful design to manage its size.\n",
        "\n",
        "- **Sparse representations**: Each document may contain very few of the known words in the vocabulary resulting in sparse representations.\n",
        "\n",
        "- **Meaning**: Discarding word order ignores the context and in turn, the meaning of words in the document (semantics). Context and meaning help to tackle polysemous words, synonyms, and much more. On the other hand, some more primitive NLP techniques and machine learning algorithms might not make use of the word order anyway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUFchjuZ3nE"
      },
      "source": [
        "# One-Hot encoding ordered <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUXS88DCaewm"
      },
      "source": [
        "Some machine learning algorithms can build an internal representation of items in a sequence, like time series or ordered words in a sentence. Recurrent Neural Networks (RNNs), for example, can exploit the sequence order for better classification results (see notebook [Introduction to RNNs](https://nbviewer.jupyter.org/github/victorviro/Deep_learning_python/blob/master/Introduction_to_RNNs.ipynb)).\n",
        "\n",
        "The encoded-words are fed sequentially into the model. Note that the sentences can have a variable number of words, but RNNs can handle variable-length inputs.\n",
        "\n",
        "![](https://i.ibb.co/3pQxWCf/one-hot-ordered.png)\n",
        "\n",
        "Remember that with this technique, each word is represented as a fixed-length vector whose length is equal to the number of words of the vocabulary.\n",
        "Therefore, when we encode characters, this method can work well since the vocabulary size is not too large. But, when encoding words, the vocabulary size increases considerably, leading to sparse representations of the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmwKrHHicQ9k"
      },
      "source": [
        "Furthermore, the distance or similarity between all encoded words will be the same (vectors are orthogonal). For example, the tokens \"cat\" and \"dog\" (both animals) will be equally distant from unrelated tokens like \"bicycle\".\n",
        " \n",
        "![](https://i.ibb.co/1fFJqPd/3d-plo.png)\n",
        "\n",
        "**Note**: The inner product of two vectors $\\boldsymbol{a}$ and $\\boldsymbol{b}$ serves as a similarity measure. Often the inner product is normalized by vectors lengths to make the measure independent of them. This way we obtain **cosine similarity**:\n",
        "$$\\text{sim}(\\boldsymbol{a}, \\boldsymbol{b}) = \\frac{\\boldsymbol{a}^T\\boldsymbol{b}}{|\\boldsymbol{a}||\\boldsymbol{b}|}$$\n",
        "\n",
        "\n",
        "Can we use the similarity of the vectors to encode the relationship between these words?\n",
        "\n",
        "![](https://i.ibb.co/kXyf7ZG/3d-plo2.png)\n",
        "\n",
        "Yes, we can do it. Let's see how word embeddings work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daeIyT7ZAfv"
      },
      "source": [
        "# Word embeddings <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbP2zE_gYBBR"
      },
      "source": [
        "*Word embeddings* are distributed representations of words that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging NLP problems.\n",
        "\n",
        "One of the benefits of using dense and low-dimensional vectors is computational: the majority of neural machine learning algorithms do not play well with very high-dimensional, sparse vectors. Words are represented as real-valued vectors in a predefined vector space, often hundreds of dimensions. This is contrasted to the thousands or millions of dimensions required for sparse word representations, such as a one-hot encoding.\n",
        "\n",
        "The learning process is either joint with the neural network model on some task, such as document classification, or is an unsupervised process, using document statistics. Embeddings are learned based on the usage of words which allow capturing semantic and syntactic relationships between them (words with similar meaning have a similar representation, i.e. vectors having low cosine distance).\n",
        "\n",
        "![](https://i.ibb.co/8dHdyKk/wor2vec-relatiuonships.png)\n",
        "\n",
        "Let's see the principal word embedding algorithms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpbSy1K3ZdbA"
      },
      "source": [
        "## Embedding Layer <a name=\"5.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VWPs2x_Zpg_"
      },
      "source": [
        "An embedding layer is a word embedding that is learned jointly with a neural network model on a specific NLP task, such as language modeling or document classification. It usually requires that document text be cleaned and prepared such that each word is one-hot encoded. The size of the vector space is specified as part of the model, such as 100, or 300 dimensions. The vectors are initialized with small random numbers. The embedding layer is used on the front end of a neural network and is fit in a supervised way using the Backpropagation algorithm.\n",
        "\n",
        "In a RNN, each word may be taken as one input in a sequence. This approach of learning an embedding layer requires a lot of training data and can be slow,\n",
        "but will learn an embedding both targeted to the specific text data and the NLP task.\n",
        "\n",
        "Keras offers a flexible `Embedding` layer that can be used for neural networks on text data in a variety of ways. A text classifier with RNNs using the `Embedding` layer is available in a [TensorFlow tutorial](https://www.tensorflow.org/tutorials/text/text_classification_rnn). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFCmaweIZ4-J"
      },
      "source": [
        "## Word2Vec <a name=\"5.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxs7brJPpXAI"
      },
      "source": [
        "[Word2Vec](https://arxiv.org/abs/1301.3781) is a statistical method developed by Google in 2013, for efficiently learning word embeddings from a text corpus given their local usage context.\n",
        "\n",
        "Word2vec implements two separate embedding algorithms. They are conceptually different, but similar from a computational point of view.\n",
        "\n",
        "- **Continuous Bag-of-Words (CBOW) model**: learns the embedding by predicting the current target word (the center word) based on its context words (its surrounding words).\n",
        "\n",
        "- **Continuous Skip-Gram model**: learns by predicting the surrounding words given a current word.\n",
        "\n",
        "Both word2vec models are implemented as simple neural networks\n",
        "with just one hidden layer and two weight matrices.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Metin_Bilgin2/publication/320829283/figure/fig2/AS:759096500318213@1557994117366/Word2Vec-CBOW-and-Skip-gram-There-are-two-different-methods-in-the-Word2Vec-algorithm.png)\n",
        "\n",
        "The context is a parameter defined by a window of neighboring words (in the image above the window size is 2).\n",
        "\n",
        "An interesting tool to visualize word relationships can be seen in a [tensorflow projection](https://projector.tensorflow.org/) where the word vectors are reduced to a three-dimensional space.\n",
        "\n",
        "The key benefit of the approach is that high-quality word embeddings can be learned efficiently (low space and time complexity), allowing embeddings to be learned from much larger corpora of text (billions of words).\n",
        "\n",
        "While the Word2Vec family of models is unsupervised (additional labels are not needed), a supervised classification methodology is prepared to train the model. We can model the CBOW architecture as a classification model such that we take the context words as our input and the target word as our output or ground truth. \n",
        "\n",
        "\n",
        "Embeddings in the word2Vec models are trained as multinomial or multiclass classification. Therefore, the loss function compares the probability distributions over center words (CBOW) or context words (skip-gram) with a given one-hot encoding of the ground truth. But, using a softmax for predicting a target word is computationally demanding. In a follow-up [paper](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf), authors suggested using one of the two approximate cost functions: \n",
        "\n",
        "- *Hierarchical softmax* builds a binary tree where leaves are all the words from the\n",
        "vocabulary. To estimate the probability of a given word, one traverses the tree from the root to a leaf.\n",
        "\n",
        "- With *negative sampling*, each training sample only modifies a small percentage of the weights, rather than all of them. The negative samples are selected using a unigram distribution, where more frequent words are less likely to be selected as negative samples.\n",
        "\n",
        "The authors also introduced a technique called *subsampling of frequent words* to decrease the number of training examples. The probability that a word is discarded is related to the word’s frequency.\n",
        "\n",
        "The [Gensim](https://github.com/RaRe-Technologies/gensim) package provides an implementation of this algorithm. An example implementing the CBOW model using Keras and using Gensim is available [here](https://nbviewer.jupyter.org/github/victorviro/Deep_learning_python/blob/master/Word2vec_CBOW_model.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfaXyIoWbZ2c"
      },
      "source": [
        "## GloVe <a name=\"5.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh7IN7mCe5ce"
      },
      "source": [
        "While word2vec is a predictive model (a feed-forward neural network that learns vectors to improve the predictive ability), Global Vectors for Word Representation, or [GloVe](https://nlp.stanford.edu/projects/glove/), is a count-based model for efficiently learning word vectors, developed by Pennington, et al. at Stanford. Classical vector space model representations of words were developed using matrix factorization techniques such as [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis) (LSA) that do a good job of using global text statistics but are not as good as the learned methods like Word2Vec at capturing meaning and demonstrating it on tasks like calculating analogies. The gloVe is an approach to marry both the global statistics of matrix factorization techniques like LSA with the local context-based learning in Word2Vec. Rather than using a window to define local context, GloVe constructs a word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may result in generally better word embeddings. GloVe is a global log-bilinear regression model with a weighted least-squares objective for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAMioMOHcuHB"
      },
      "source": [
        "The model is based on a simple idea that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning which can be encoded as vector differences. Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ depict the number of times word $j$ occurs in the context of word $i$. An example co-occurrence matrix might look as follows.\n",
        "\n",
        "![](https://i.ibb.co/c1VRfqv/co-ocurrence-matrix.png)\n",
        "\n",
        "\n",
        "\n",
        "Let $X_i=\\sum_{k} X_{ik}$ be the number of times any word appears in the context of word $i$. Finally, let $P_{ij}=P(j|i)=\\frac{X_{ij}}{X_i}$ be the probability that word $j$ appear in the context of word $i$\n",
        "\n",
        "Suppose we are interested in the concept of thermodynamic phase, for which we might take $i=\\text{ice}$ and $j=\\text{steam}$.The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$. For words $k$ related to ice but not steam, say $k=\\text{solid}$, \n",
        "$P_{ik}=P(\\text{solid}|\\text{ice})$ will be relatively high, and\n",
        "$P_{jk}=P(\\text{solid}|\\text{steam})$ will be relatively low, so\n",
        "the ratio $\\frac{P_{ik}}{P_{jk}}$ will be large. Similarly, for words $k$ related to steam but not ice, say $k=\\text{gas}$,\n",
        " $P_{ik}$ will be relativaly low, and $P_{jk}$ will be relatively high, so the ratio should be small. For words $k$ like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one.\n",
        "\n",
        " ![](https://i.ibb.co/jLq6GJq/Selection-742.png)\n",
        "\n",
        "\n",
        "\n",
        "The ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words. So, if we can find a way to incorporate $\\frac{P_{ik}}{P_{jk}}$ to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh11k39TmrgQ"
      },
      "source": [
        "So, if we can find a way to incorporate $\\frac{P_{ik}}{P_{jk}}$ to computing word vectors we will be achieving the goal of using global statistics when learning word vectors. The most general model takes the form\n",
        "\n",
        "$$F(w_i,w_j,\\tilde{w}_k)=\\frac{P_{ik}}{P_{jk}}$$\n",
        "\n",
        "that is a function $F$ which takes in word vectors of $i$, $j$, and $k$ and outputs the ratio we’re interested in. Note that $w$ and $\\tilde{w}$ are two embedding layers. The paper says, often both these layers will perform equivalently and will only differ by the different random initialization. However, having two layers help the model to reduce overfitting.\n",
        "\n",
        "Word vectors are linear systems. For example, we can perform arithmetic in embedding space, such as $w_{\\text{king}}-w_{\\text{male}}+w_{\\text{female}}=w_{\\text{queen}}$. With this aim, we can restrict our consideration to those functions $F$ that depend only on the difference of the two target words, modifying the above equation to \n",
        "\n",
        "$$F(w_i-w_j,\\tilde{w}_k)=\\frac{P_{ik}}{P_{jk}}$$\n",
        "\n",
        "\n",
        "Word vectors are high-dimensional vectors, however $\\frac{P_{ik}}{P_{jk}}$ is a scalar. We can introduce a transpose and a dot product between the two entities to prevent $F$ from mixing the vector dimensions in undesirable ways.\n",
        "\n",
        "$$F((w_i-w_j)^T\\tilde{w}_k)=\\frac{P_{ik}}{P_{jk}}$$\n",
        "\n",
        "So, if we suposse a word vector as a $D \\times 1$ matrix, $(w_i-w_j)^T$ will be $1 \\times D$ shaped which gives a scalar when multiplied with $\\tilde{w}_k$.\n",
        "\n",
        "Next, if we assume $F$ has a certain property (i.e. homomorphism between the additive group and the multiplicative group) which gives,\n",
        "\n",
        "$$F((w_i-w_j)^T\\tilde{w}_k)=\\frac{F(w_i^T\\tilde{w}_k)}{F(w_j^T\\tilde{w}_k)}=\\frac{P_{ik}}{P_{jk}}$$\n",
        "\n",
        "In other words, this particular homomorphism ensures that the subtraction $F(A-B)$ can also be represented as a division $\\frac{F(A)}{F(B)}$ and get the same result. Therefore,\n",
        "\n",
        "$$F(w_i^T\\tilde{w}_k)=P_{ik}=\\frac{X_{ik}}{X_{i}}$$\n",
        "\n",
        "If $F$ is the exponential function then\n",
        "\n",
        "$$w_i^T\\tilde{w}_k=\\text{log}(P_{ik})=\\text{log}(\\frac{X_{ik}}{X_{i}})=\\text{log}(X_{ik})-\\text{log}(X_{i})$$\n",
        "\n",
        "The term $\\text{log}(X_{i})$ is independent of $k$ so it can be absorbed into a bias $b_i$ for $w_i$. Finally, an additional bias $\\tilde{b}_k$ is added for $\\tilde{w}_k$.\n",
        "\n",
        "$$w_i^T\\tilde{w}_k + b_i + \\tilde{b}_k = \\text{log}(X_{ik})$$\n",
        "\n",
        "In an ideal setting, where we have perfect word vectors, the above expression will be zero. So we will be setting the above expression as our cost function.\n",
        "\n",
        "$$J(w_i,w_j)=(w_i^T\\tilde{w}_j + b_i + \\tilde{b}_j-\\text{log}(X_{ij}))^2$$\n",
        "\n",
        "Note that the square makes this a mean square cost function. Also, $k$ has been replaced with $j$.\n",
        "\n",
        "But, if $X_{ik}=0$ then $\\text{log}(X_{ik})=\\text{log}(0)$ which is undefined. The easy fix would be to use $\\text{log}(1+X_{ik})$ known as Laplacian smoothing. But the GloVe paper proposes a sleeker way of doing this. That is to introduce a weighting function.\n",
        "\n",
        "$$J(w_i,w_j)=f(X_{ij})(w_i^T\\tilde{w}_j + b_i + \\tilde{b}_j-\\text{log}(X_{ij}))^2$$\n",
        "\n",
        "where \n",
        "$$\n",
        "f(x)=\\begin{cases}\n",
        "              (\\frac{x}{x_{\\text{max}}})^{\\alpha} & \\text{if }x< x_{\\text{max}},\\\\\n",
        "              o & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "The final cost function gives us the model\n",
        "\n",
        "$$J(w_i,w_j)=\\sum_{i,j=1}^{V}f(X_{ij})(w_i^T\\tilde{w}_k + b_i + \\tilde{b}_j-\\text{log}(X_{ij}))^2$$\n",
        "\n",
        "where $V$ is the size of the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U7Oca8eUzZc"
      },
      "source": [
        "To train GloVe word vectors on our own corpus or download pre-trained word vectors on Common Crawl and Wikipedia we can use the [stanfordnlp project](https://github.com/stanfordnlp/glove). Pre-trained GloVe word vectors trained on Common Crawl are also available in the python package [Spacy](https://spacy.io/models/en#en_core_web_md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bkAM4VbMVBO"
      },
      "source": [
        "## FastText <a name=\"5.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgQ-pK4FMYuo"
      },
      "source": [
        "One major drawback of word-embedding techniques like word2vec and glove was its inability to deal with out of corpus words. These embedding techniques treat words as the minimal entity and try to learn their respective embedding vector. Hence in the case that there is a word that does not appear in the training corpus, word2vec or glove fails to get their vectorized representation. This is a limitation, especially for languages with large vocabularies and many rare words. Another weakness of these models is that for words with the same radicals such as \"eat\" and \"eaten\", Word2Vec doesn’t do any parameter sharing. Each word is learned uniquely based on the context it appears in. Thus, there is scope for utilizing the internal structure of the word to make the process more efficient.\n",
        "\n",
        "[FastText](https://arxiv.org/abs/1607.04606) is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several $n$-grams (sub-words). Taking the word *where* and $n=3$ as an example, it will be represented by the character $n$-grams: `<wh, whe, her, ere, re>`. The symbols `<` and `>` are special symbols which are appended to show the start and end of the token, allowing to distinguish prefixes and suffixes from other character sequences (the tri-gram `her` from the word *where* is different from the tri-gram `<her>` from the word *her*).\n",
        "\n",
        "Each word is represented by the sum of the vector representations of its $n$-grams.\n",
        "\n",
        "After training the Neural Network, we will have word embeddings for all the $n$-grams given the training dataset. Words that do not appear in the training corpus can now be properly represented since it is highly likely that their n-grams also appears in other words.\n",
        "\n",
        "One of the major drawbacks of this model is the high memory requirement. Since this model creates word-embedding from its characters and not from words. We can control the number of character embeddings by applying hashing. Instead of learning an embedding for each unique $n$-gram, we learn total $K$ embeddings where $K$ denotes the bucket size. Each character $n$-gram is mapped to an integer between 1 to K. Though this could result in collisions, it helps control the vocabulary size. The paper uses the FNV-1a variant of the Fowler-Noll-Vo hashing function to hash character sequences to integer values.\n",
        "\n",
        "To train our own embeddings, we can either use the official [tool](https://fasttext.cc/) created by Facebook (see [tutorial](https://fasttext.cc/docs/en/unsupervised-tutorial.html)) or we can use the [fastText implementation](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html) available in gensim (see [tutorial](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb)).\n",
        "\n",
        "Pre-trained word vectors trained on Common Crawl and Wikipedia for 157 languages are available [here](https://fasttext.cc/docs/en/crawl-vectors.html) and variants of English word vectors are available [here](https://fasttext.cc/docs/en/english-vectors.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn_-I6IEy436"
      },
      "source": [
        "## Contextualized word embeddings <a name=\"5.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9uCqzO_zA3l"
      },
      "source": [
        "Consider the sentences \"The man was accused of robbing a bank\" and \"The man went fishing by the bank of the river\". The word \"bank\" has a different meaning in these two sentences (polysemy) but models like Word2Vec, GloVe, or FastText would produce the same word embedding for this word because these models are context-independent (output just one embedding for each word, combining all the different senses of the word into one vector). Recent models like [ELMo](https://arxiv.org/abs/1802.05365) or [BERT](https://arxiv.org/abs/1810.04805) can generate different embeddings for a word that captures the **context** of a word, that is its position in a sentence. For instance, for the same example above, the word \"bank\" from the first sentence (financial establishment), would be closer to words like \"finance\", \"investment\", \"fund\" etc. whereas the word \"bank\" from the second sentence (the land alongside to a river or lake) would be closer to words like \"river\", \"lake\", \"edge\" etc.\n",
        "\n",
        "The main difference is the fact Word2vec or Glove do not take into account word order in their training while ELMo and BERT take into account word order. A practical implication of this difference is that we can use word2vec and Glove vectors trained on a large corpus directly for downstream tasks. All we need is the vectors for the words. There is no need for the model itself that was used to train these vectors. However, in the case of ELMo and BERT, since they are context-dependent, we need the model that was used to train the vectors even after training, since the models generate the vectors for a word based on context.\n",
        "\n",
        "A notebook explaining recent language model techniques for transfer learning, including ELMo and BERT, is available [here](https://nbviewer.jupyter.org/github/victorviro/Deep_learning_python/blob/master/Transfer_learning_in_NLP.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMekWiVGtXTy"
      },
      "source": [
        "# Paragraph and document embeddings <a name=\"6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTb__oYItYzp"
      },
      "source": [
        "We have discussed distributed representations of words. The simplest way to have distributed representations of paragraphs is to take a weighted **average** or sum of embeddings of all words that occur in a given document.\n",
        "\n",
        "[Siamese CBOW](https://arxiv.org/pdf/1606.04640.pdf) extends the Wor2vec CBOW model optimizing word embeddings directly to be averaged, by predicting, from a sentence representation, its surrounding sentences.\n",
        "\n",
        "*Sent2Vec* Presented in [Pagliardini et al, 2017](https://www.aclweb.org/anthology/N18-1049.pdf) and [Gupta et al, 2019](https://www.aclweb.org/anthology/N19-1098/) extended the classic CBOW model of word2vec to include word n-grams and adapted to optimize the word (and n-grams) embeddings to average them to yield document embeddings.\n",
        "\n",
        "\n",
        "Another option is to directly learn distributed representations of groups of words. *Paragraph Vector model* or [doc2vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf), is perhaps the first attempt to generalize word2vec to work with word sequences. The authors introduce two variants:\n",
        "\n",
        "- *Paragraph Vector Distributed Memory* (PV-DM), like CBOW, tries to predict the target word based on context words (in this case, the context words are the preceding words, not the surrounding words). Unlike CBOW, PV-DM also takes a paragraph embedding as an input, which can be either concatenated with word embeddings or averaged. PV-DM learns simultaneously both word embeddings and document embeddings.\n",
        "\n",
        "- *Paragraph Vector Distributed Bag of Words* (PV-DBOW) is a simpler model than PV-DM. Despite its name, is perhaps the parallel of word2vec’s skip-gram architecture. It predicts all words in the document based on the document embedding. Therefore, in contrast to PV-DM, PV-DBOW does not learn word embeddings, but only paragraph embeddings.\n",
        "\n",
        "![](https://i.stack.imgur.com/t7slV.png)\n",
        "\n",
        "During training, the model is fed with training data and\n",
        "document embeddings, as well as softmax weights, are modified. \n",
        "Notice that only the paragraphs in the training corpus have an embedding associated with them. At prediction time, one needs to perform an inference step to compute the paragraph vector for a new paragraph, but only embeddings are modified in the optimization process. Softmax weights are fixed in the inference phase. The inference phase is not as time-consuming as the training phase. Nonetheless, the need to iteratively optimize the cost in order to obtain embeddings for new documents is one of the weaknesses of Paragraph Vector. The paragraph vector takes into consideration the ordering of words within a narrow context, similar to an n-gram model but it is much more effective because it generalizes better and has a lower dimensionality but still is of a fixed length so it can be used in common machine learning algorithms.\n",
        "The method has a Python [implementation](https://radimrehurek.com/gensim/models/doc2vec.html) in the Gensim package.\n",
        "\n",
        "\n",
        "Another attempt to generalize word2vec, specifically the skip-gram architecture, is [Skip-thought vectors](https://arxiv.org/abs/1506.06726). Instead of using a word to predict its surrounding context, it encodes a sentence to predict the sentences around it. They use a RNN encoder-decoder model where the encoder maps words to a sentence vector and a decoder is used to generate the surrounding sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emru2lGdXhbt"
      },
      "source": [
        "Due to the success of BERT in a wide range of tasks, can we use it to get sentence embeddings? A disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitation, we can pass single sentences through BERT and then derive a fixed-sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special `CLS` token. \n",
        "\n",
        "[Sentence-BERT ](https://arxiv.org/abs/1908.10084) (Sentence Embeddings using Siamese BERT-Networks) adds a pooling operation to the output of BERT to derive a fixed-sized sentence embedding. They experiment with three pooling strategies: Using the output of the `CLS`token, computing the mean of all output vectors, and computing a max-over-time of the output vectors. To fine-tune BERT, they use different objective functions to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity. The python package [sentence-transformers](https://github.com/UKPLab/sentence-transformers) provides an implementation to compute these sentence embeddings. A text classifier trained using these embeddings can be found in this [repository](https://github.com/victorviro/Text-classifier-from-BERT-embeddings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKfS42VP2oPa"
      },
      "source": [
        "# References <a name=\"7\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdU8P_18ctYd"
      },
      "source": [
        "\n",
        "\n",
        "- [Text Vectorization, Get Applied Text Analysis with Python](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html)\n",
        "\n",
        "- [Introduction bag of words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)\n",
        "\n",
        "- [Distributed Representations of Words and Phrases\n",
        "and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
        "\n",
        "- [Efficient Estimation of Word Representations in\n",
        "Vector Space](https://arxiv.org/abs/1301.3781)\n",
        "\n",
        "\n",
        "- [Introduction to word embedding Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
        "\n",
        "- [Skip-gram model explained](https://medium.com/datadriveninvestor/word2vec-skip-gram-model-explained-383fa6ddc4ae)\n",
        "\n",
        "- [Word Vectors with Word2Vec](https://deeplearningdemystified.com/article/nlp-1)\n",
        "\n",
        "- [Thorough analysis of hierarchical softmax and negative sampling loss functions in Word2vec](https://arxiv.org/abs/1411.2738).\n",
        "\n",
        "- [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "\n",
        "\n",
        "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
        "\n",
        "\n",
        "- [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
        "\n",
        "- [A gentle introduction to Doc2Vec](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)\n",
        "\n",
        "- [Text classification with doc2vec](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)\n",
        "\n",
        "- [Word Embedding & GloVe](https://medium.com/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6)\n",
        "\n",
        "- [Intuitive Guide to Understanding GloVe Embeddings](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)\n",
        "\n",
        "- [GloVe word vectors Coursera](https://www.coursera.org/lecture/nlp-sequence-models/glove-word-vectors-IxDTG)\n",
        "\n",
        "- [Global Vectors (GloVe)](https://medium.com/sciforce/word-vectors-in-natural-language-processing-global-vectors-glove-51339db89639)\n",
        "\n",
        "\n",
        "- [A Visual Guide to FastText Word Embeddings](https://amitness.com/2020/06/fasttext-embeddings/)\n",
        "\n",
        "- [Understanding fasttext](https://medium.com/@adityamohanty/understanding-fasttext-an-embedding-to-look-forward-to-3ee9aa08787)\n",
        "\n",
        "- [FastText model in gensim other (tutorial)](https://radimrehurek.com/gensim/models/fasttext.html)\n",
        "\n",
        "- [Word2Vec and FastText Word Embedding with Gensim](https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c)\n",
        "\n",
        "- [How to use ELMo to extract features from text](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)\n",
        "\n",
        "- [Vector representations of text data in deep learning](https://arxiv.org/abs/1901.01695)\n",
        "\n",
        "- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}