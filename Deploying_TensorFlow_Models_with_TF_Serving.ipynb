{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deploying TensorFlow Models with TF Serving.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3+DuJvuNU11bqavCWNGxe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Deep_learning_python/blob/master/Deploying_TensorFlow_Models_with_TF_Serving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9RL24dZxPBW"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXaQPuqX4MqE"
      },
      "source": [
        "Once we have a beautiful model that makes amazing predictions, what do we do with it? Well, we need to put it in production! This could be as simple as running the model on a batch of data and perhaps writing a script that runs this model every night. However, it is often much more involved. Various parts of our infrastructure may need to use this model on live data, in which case we probably want to wrap our model in a web service: this way, any part of our infrastructure can query our model at any time using a simple REST API (or some other protocol). But as time passes, we need to regularly retrain our model on fresh data and push the updated version to production. We must handle model versioning, gracefully transition from one model to the next, possibly roll back to the previous model in case of problems, and perhaps run multiple different models in parallel to perform A/B experiments. If our product becomes successful, our service may start to get plenty of queries per second (QPS), and it must scale up to support the load. A great solution to scale up our service, as we will see in this notebook, is to use [TF Serving](https://www.tensorflow.org/tfx/guide/serving), either on our own hardware infrastructure or via a cloud service such as Google Cloud AI Platform. It will take care of efficiently serving our model, handle graceful model transitions, and more. If we use the cloud platform, we will also get many extra features, such as powerful monitoring tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLMRMf3gxPEk"
      },
      "source": [
        "Moreover, if we have a lot of training data, and compute-intensive models, then training time may be prohibitively long. If our product needs to adapt to changes quickly, then a long training time can be a showstopper (e.g., think of a news recommendation system promoting news from last week). Perhaps even more importantly, a long training time will prevent us from experimenting with new ideas. In Machine Learning (as in many other fields), it is hard to know in advance which ideas will work, so we should try out as many as possible, as fast as possible. One way to speed up training is to use hardware accelerators such as GPUs or TPUs. To go even faster, we can train a model across multiple machines, each equipped with multiple hardware accelerators. TensorFlow’s simple yet powerful Distribution Strategies API makes this easy, as we will see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Y0xV14yDOM"
      },
      "source": [
        "In this notebook we will look at how to deploy models to TF Serving, then to Google Cloud AI Platform in the next notebook. We will also take a quick look at deploying models to mobile apps, embedded devices, and web apps in the next notebooks. We will discuss how to speed up computations using GPUs and how to train models across multiple devices and servers using the Distribution Strategies API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DulcDjYUyLQd"
      },
      "source": [
        "# Serving a TensorFlow Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRq9JpRIyNjG"
      },
      "source": [
        "Once we have trained a TensorFlow model, we can easily use it in any Python code: if it’s a tf.keras model, we just call its `predict()` method! But as our infrastructure grows, there comes a point where it is preferable to wrap our model in a small service whose sole role is to make predictions and have the rest of the infrastructure query it (e.g., via a REST or gRPC API). This decouples our model from the rest of the infrastructure, making it possible to easily switch model versions or scale the service up as needed (independently from the rest of our infrastructure), perform A/B experiments, and ensure that all our software components rely on the same model versions. It also simplifies testing and development, and more. We could create our own microservice using any technology we want (e.g., using the [Flask library](https://github.com/pallets/flask)), but why reinvent the wheel when we can just use TF Serving?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RJ9ayH1zbwH"
      },
      "source": [
        "## Using TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-yq_Q6PzdrD"
      },
      "source": [
        "TF Serving is a very efficient, battle-tested model server that’s written in C++. It can sustain a high load, serve multiple versions of our models and watch a model repository to automatically deploy the latest versions, and more (see Figure 19-1).\n",
        "\n",
        "![](https://i.ibb.co/NtKhHr0/TF-serving.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P4kA4pJniiP"
      },
      "source": [
        "### Load the data a train the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FChkPuF2z2ch"
      },
      "source": [
        "So let’s train a model in the [MNIST dataset](https://keras.io/api/datasets/mnist/) using tf.keras, and then we will deploy it to TF Serving. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhc14TqwBir7"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btrbS1e00UNb",
        "outputId": "b5a4c15f-5b89-4a37-c4a7-d1066425e8c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# import the Fashion MNIST dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# scale the values to 0.0 to 1.0\n",
        "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
        "\n",
        "# reshape for feeding into the model\n",
        "X_train_full = X_train_full.reshape(X_train_full.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "# Take 3 examples from the test data to make predictions when model is served\n",
        "#X_new = X_test[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G_xItNc0lPO",
        "outputId": "024c0ab5-0c77-47f0-c630-30525c8465bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# simple CNN\n",
        "model = keras.Sequential([\n",
        "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
        "                      strides=2, activation='relu', name='Conv1'),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
        "])\n",
        "\n",
        "# train and evaluate our model\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('\\nTest accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3884 - accuracy: 0.8915 - val_loss: 0.2506 - val_accuracy: 0.9294\n",
            "Epoch 2/2\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2333 - accuracy: 0.9332 - val_loss: 0.1946 - val_accuracy: 0.9438\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2014 - accuracy: 0.9442\n",
            "\n",
            "Test accuracy: 0.9441999793052673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7984eYbM01HV"
      },
      "source": [
        "The first thing we have to do for deploying the model to TF Serving is export this model to TensorFlow’s *SavedModel format*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlKFtW-R4TIh"
      },
      "source": [
        "### Exporting SavedModels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-crhiNUC4Wb6"
      },
      "source": [
        "TensorFlow provides a simple `tf.saved_model.save()` function to export models to the SavedModel format. All we need to do is give it the model, specifying its name and version number, and the function will save the model’s computation graph and its weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzTTm8-200pz",
        "outputId": "f45cb04f-91ef-43b3-81e0-f3a03d248313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_version = \"0001\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "print('Path where the model will be stored: {}\\n'.format(model_path))\n",
        "tf.saved_model.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path where the model will be stored: my_mnist_model/0001\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI0rYNll2Leu"
      },
      "source": [
        "Alternatively, we can just use the model’s `save()` method (`model.save(model_​path)`): as long as the file’s extension is not `.h5`, the model will be saved using the SavedModel format instead of the HDF5 format.\n",
        "\n",
        "It’s usually a good idea to include all the preprocessing layers in the final model we export so that it can ingest data in its natural form once it is deployed to production. This avoids having to take care of preprocessing separately within the application that uses the model. Bundling the preprocessing steps within the model also makes it simpler to update them later on and limits the risk of mismatch between a model and the preprocessing steps it requires.\n",
        "\n",
        "**Note**: Since a SavedModel saves the computation graph, it can only be used with models that are based exclusively on TensorFlow operations, excluding the `tf.py_function()` operation (which wraps arbitrary Python code). It also excludes dynamic tf.keras models, since these models cannot be converted to computation graphs. Dynamic models need to be served using other tools (e.g., Flask)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXi2uTcc27eW"
      },
      "source": [
        "A SavedModel represents a version of our model. It is stored as a directory containing a `saved_model.pb` file, which defines the computation graph (represented as a serialized protocol buffer), and a *variables* subdirectory containing the variable values. For models containing a large number of weights, these variable values may be split across multiple files. A SavedModel also includes an *assets* subdirectory that may contain additional data, such as vocabulary files, class names, or some example instances for this model. The directory structure is as follows (in this example, we don’t use assets):\n",
        "\n",
        "```\n",
        "my_mnist_model\n",
        "└── 0001\n",
        "    ├── assets\n",
        "    ├── saved_model.pb\n",
        "    └── variables\n",
        "        ├── variables.data-00000-of-00001\n",
        "        └── variables.index\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKH-Gc69DhBZ",
        "outputId": "0e345959-e2dc-4867-d2ec-5300a4e9ece9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for root, dirs, files in os.walk(model_name):\n",
        "    indent = '    ' * root.count(os.sep)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    for filename in files:\n",
        "        print('{}{}'.format(indent + '    ', filename))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_mnist_model/\n",
            "    0001/\n",
            "        saved_model.pb\n",
            "        assets/\n",
            "        variables/\n",
            "            variables.data-00000-of-00001\n",
            "            variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6gGjw5PDhI3"
      },
      "source": [
        "As we might expect, we can load a SavedModel using the `tf.saved_model.load()` function. However, the returned object is not a Keras model: it represents the SavedModel, including its computation graph and variable values. We can use it like a function, and it will make predictions (we make sure to pass the inputs as tensors of the appropriate type):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaXkXZ8H3Fm6"
      },
      "source": [
        "saved_model = tf.saved_model.load(model_path)\n",
        "y_pred = saved_model(tf.constant(X_test[:3], dtype=tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l6Fiydp3HJW"
      },
      "source": [
        "Alternatively, we can load this SavedModel directly to a Keras model using the `keras.models.load_model()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJtzxj_N3KMw"
      },
      "source": [
        "model = keras.models.load_model(model_path)\n",
        "y_pred = model.predict(tf.constant(X_test[:3], dtype=tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nig5hNt6D7oX"
      },
      "source": [
        "TensorFlow also comes with a small `saved_model_cli` command-line tool to inspect and examine SavedModels (see [this discussion of the SavedModel CLI](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel) in the TensorFlow Guide):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVakQqMeD8W1",
        "outputId": "4ae9c81b-8737-4420-e563-d61a42493c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli show --dir {model_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel contains the following tag-sets:\n",
            "serve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FndoAeMiEBcq",
        "outputId": "dfa48f91-4f83-4e51-b687-c6f295e8abf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
            "SignatureDef key: \"__saved_model_init_op\"\n",
            "SignatureDef key: \"serving_default\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGXvBVPrEDyD",
        "outputId": "b3d410bf-c576-466a-fb45-4800607cd652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve \\\n",
        "                      --signature_def serving_default"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['Conv1_input'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 28, 28, 1)\n",
            "      name: serving_default_Conv1_input:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['Softmax'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 10)\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s_wrXaJEF26",
        "outputId": "c94af294-6c6a-4187-c635-29f2444452b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli show --dir {model_path} --all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['Conv1_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_Conv1_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['Softmax'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1105 17:11:25.413942 139637878630272 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "\n",
            "Defined Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'Conv1_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOl-I2uo3Yok"
      },
      "source": [
        "A SavedModel contains one or more *metagraphs*. A metagraph is a computation graph plus some function signature definitions (including their input and output names, types, and shapes). Each metagraph is identified by a set of tags. For example, we may want to have a metagraph containing the full computation graph, including the training operations (this one may be tagged `\"train\"`, for example), and another metagraph containing a pruned computation graph with only the prediction operations, including some GPU-specific operations (this metagraph may be tagged `\"serve\"`, `\"gpu\"`). However, when we pass a tf.keras model to the `tf.saved_model.save()` function, by default the function saves a much simpler SavedModel: it saves a single metagraph tagged `\"serve\"`, which contains two signature definitions, an initialization function (called `__saved_model_init_op`, which we do not need to worry about) and a default serving function (called `serving_default`). When saving a tf.keras model, the default serving function corresponds to the model’s `call()` function, which of course makes predictions.\n",
        "\n",
        "The `saved_model_cli` tool can also be used to make predictions (for testing, not really for production). Suppose we have a NumPy array (`X_new`) containing three images of handwritten digits that we want to make predictions for. We first need to export them to NumPy’s `npy` format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sef2f6WV3_Yh"
      },
      "source": [
        "# Take 3 examples from the test data to make predictions \n",
        "X_new = X_test[:3]\n",
        "np.save(\"my_mnist_tests.npy\", X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaFymsX7ETB-",
        "outputId": "651d92d7-082f-41f2-d363-e5ec5dd2cd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "input_name = model.input_names[0]\n",
        "input_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Conv1_input'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFVMe5NiEXIG",
        "outputId": "fac0244a-d56a-4d13-c29d-a3321270fcea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli run --dir {model_path} --tag_set serve \\\n",
        "                     --signature_def serving_default    \\\n",
        "                     --inputs {input_name}=my_mnist_tests.npy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-05 17:11:27.491229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-05 17:11:27.495885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.496503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-05 17:11:27.496807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-05 17:11:27.498399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-05 17:11:27.499994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-05 17:11:27.500292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-05 17:11:27.501952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-05 17:11:27.502988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-05 17:11:27.506878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-05 17:11:27.507051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.508218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.509016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2020-11-05 17:11:27.516724: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-11-05 17:11:27.516981: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56057ba80840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-05 17:11:27.517014: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-05 17:11:27.625703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.626346: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56057ba80a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-05 17:11:27.626379: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-05 17:11:27.626607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.627104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-05 17:11:27.627163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-05 17:11:27.627184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-05 17:11:27.627203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-05 17:11:27.627221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-05 17:11:27.627239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-05 17:11:27.627256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-05 17:11:27.627274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-05 17:11:27.627339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.627883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.628334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
            "2020-11-05 17:11:27.630152: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-05 17:11:27.631198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-05 17:11:27.631226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
            "2020-11-05 17:11:27.631237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
            "2020-11-05 17:11:27.632089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.632692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-05 17:11:27.633256: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-05 17:11:27.633300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13587 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1105 17:11:27.634097 139792316921728 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/tools/saved_model_cli.py:420: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "2020-11-05 17:11:27.759678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-05 17:11:35.660770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "Result for output key Softmax:\n",
            "[[1.01769629e-05 4.31656350e-10 4.43657264e-05 1.76759332e-03\n",
            "  2.76432274e-07 2.70247492e-05 3.61793706e-10 9.97825503e-01\n",
            "  3.56548958e-06 3.21475003e-04]\n",
            " [1.19344925e-03 3.59798905e-05 9.35230792e-01 7.06919003e-03\n",
            "  1.87569404e-10 3.96863446e-02 1.49379345e-02 1.40170698e-13\n",
            "  1.84626156e-03 8.41693049e-10]\n",
            " [1.43477182e-05 9.81204331e-01 9.08252597e-03 1.07742776e-03\n",
            "  7.30877160e-04 1.13499307e-04 1.79149304e-03 3.04149324e-03\n",
            "  2.72172573e-03 2.22318369e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5klEs_ajreu"
      },
      "source": [
        "### Serve our model with TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7b7qoJI4eqz"
      },
      "source": [
        "#### Installing TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQGhYwA4feH"
      },
      "source": [
        "There are many ways to install TF Serving: using a Docker image, using the system’s package manager, installing from source, and more. \n",
        "\n",
        "In this example we are going to run TensorFlow Serving natively adding the TensorFlow Serving distribution URI as a package source, but we can also run it in a Docker container, which is highly recommended by the TensorFlow team as it is simple to install, it will not mess with our system, and it offers high performance (see this [repository](https://github.com/victorviro/tf_serving_mnist)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB3v_bskG7C9",
        "outputId": "5d4aa079-15a1-4e32-f3f0-357c5bc8e58c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Add TensorFlow Serving distribution URI as a package source\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2943  100  2943    0     0  45276      0 --:--:-- --:--:-- --:--:-- 45984\n",
            "OK\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [349 B]\n",
            "Get:13 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [341 B]\n",
            "Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.1 kB]\n",
            "Get:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [405 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,687 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,353 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,167 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,750 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,118 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [48.9 kB]\n",
            "Fetched 10.7 MB in 3s (4,015 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgaAVCRRjJKa",
        "outputId": "c3ae6003-7f20-4710-cf2d-24ecbd8b11dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install TensorFlow Serving\n",
        "!apt-get install tensorflow-model-server"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tensorflow-model-server\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 210 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.3.0 [210 MB]\n",
            "Fetched 210 MB in 3s (80.3 MB/s)\n",
            "Selecting previously unselected package tensorflow-model-server.\n",
            "(Reading database ... 144628 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.3.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.3.0) ...\n",
            "Setting up tensorflow-model-server (2.3.0) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns45ax2Qj738"
      },
      "source": [
        "####  Running TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50geVTUokLn5"
      },
      "source": [
        "We start running TensorFlow Serving and load our model. After it loads we can start making inference requests using REST. There are some important parameters:\n",
        "\n",
        "- `rest_api_port`: The port that we'll use for REST requests.\n",
        "- `model_name`: We'll use this in the URL of REST requests. It can be anything.\n",
        "- `model_base_path`: This is the path to the directory where we've saved our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwL-5AsfFpr-"
      },
      "source": [
        "os.environ[\"MODEL_DIR\"] = os.path.split(os.path.abspath(model_path))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdt9QQVOFq4G",
        "outputId": "d2999d17-60dd-4369-81d7-bbe6932f8f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%bash --bg\n",
        "nohup tensorflow_model_server \\\n",
        "     --rest_api_port=8501 \\\n",
        "     --model_name=my_mnist_model \\\n",
        "     --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV7pPDDuFu0C",
        "outputId": "5d00044a-7312-4f0b-b2e0-d358c15fad4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-05 17:11:53.788595: I tensorflow_serving/model_servers/server_core.cc:464] Adding/updating models.\n",
            "2020-11-05 17:11:53.788624: I tensorflow_serving/model_servers/server_core.cc:575]  (Re-)adding model: my_mnist_model\n",
            "2020-11-05 17:11:53.790293: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: my_mnist_model version: 1}\n",
            "2020-11-05 17:11:53.790333: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: my_mnist_model version: 1}\n",
            "2020-11-05 17:11:53.790347: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: my_mnist_model version: 1}\n",
            "2020-11-05 17:11:53.790411: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /content/my_mnist_model/0001\n",
            "2020-11-05 17:11:53.791948: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
            "2020-11-05 17:11:53.791986: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /content/my_mnist_model/0001\n",
            "2020-11-05 17:11:53.792059: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB_sYwyD4xkw"
      },
      "source": [
        "\n",
        "\n",
        "Now let’s go back to Python and query this server, first using the REST API, then the gRPC API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXeqGSFV5zLV"
      },
      "source": [
        "### Querying TF Serving through the REST API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR_L30lL52Ac"
      },
      "source": [
        "Let’s start by creating the query. It must contain the name of the function signature we want to call, and of course the input data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE-VzLTF533L"
      },
      "source": [
        "import json\n",
        "\n",
        "input_data_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_test[:3].tolist(),\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWdQLmJn558T"
      },
      "source": [
        "Note that the JSON format is 100% text-based, so the `X_new` NumPy array had to be converted to a Python list and then formatted as JSON:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6UGIvtv55Vu"
      },
      "source": [
        "input_data_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us_Q1GoK6A13"
      },
      "source": [
        "Now let’s send the input data to TF Serving by sending an HTTP POST request. This can be done easily using the `requests` library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDha2YkO6HFF"
      },
      "source": [
        "import requests\n",
        "\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status() # raise an exception in case of error\n",
        "response = response.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3CamsihGJFt",
        "outputId": "d3affbcb-c392-4767-e512-4f72b6e133b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "response.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['predictions'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DldCeDW6I88"
      },
      "source": [
        "The response is a dictionary containing a single `\"predictions\"` key. The corresponding value is the list of predictions. This list is a Python list, so let’s convert it to a NumPy array and round the floats it contains to the second decimal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cc8ky6w6M3M",
        "outputId": "c71b341e-72ee-4fa6-c7f2-4c91a71bbb14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_proba = np.array(response[\"predictions\"])\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.94, 0.01, 0.  , 0.04, 0.01, 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtCxJCqb6O-h"
      },
      "source": [
        "We have the predictions! The model is close to 100% confident that the first image is a 7, 99% confident that the second image is a 2, and 96% confident that the third image is a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIJ4S6Rt0RC4",
        "outputId": "ba44efc7-e2c5-4e30-b4f3-74dcef835e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "def show(index, title):\n",
        "    plt.figure()\n",
        "    plt.imshow(X_test[index].reshape(28,28))\n",
        "    plt.axis('off')\n",
        "    plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n",
        "\n",
        "show(0, 'The model thought this was a {}, and it was actually a {}'.format(\n",
        "    np.argmax(y_proba[0]), y_test[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVuSWKx3z5ff"
      },
      "source": [
        "The REST API is nice and simple, and it works well when the input and output data are not too large. Moreover, just about any client application can make REST queries without additional dependencies, whereas other protocols are not always so readily available. However, it is based on JSON, which is text-based and fairly verbose. For example, we had to convert the NumPy array to a Python list, and every float ended up represented as a string. This is very inefficient, both in terms of serialization/deserialization time (to convert all the floats to strings and back) and in terms of payload size: many floats end up being represented using over 15 characters, which translates to over 120 bits for 32-bit floats! This will result in high latency and bandwidth usage when transferring large NumPy arrays.4 So let’s use gRPC instead.\n",
        "\n",
        "**Note**: When transferring large amounts of data, it is much better to use the gRPC API (if the client supports it), as it is based on a compact binary format and an efficient communication protocol (based on HTTP/2 framing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEyqezFh6gyX"
      },
      "source": [
        "### Querying TF Serving through the gRPC API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftdKKsB56hm0"
      },
      "source": [
        "The gRPC API expects a serialized `PredictRequest` protocol buffer as input, and it outputs a serialized `PredictResponse` protocol buffer. These protobufs are part of the `tensorflow-serving-api` library, which we must install (e.g., using pip). First, let’s create the request:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbBjU0NbvT35"
      },
      "source": [
        "!pip install -q -U tensorflow-serving-api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ybw75Te6qUd"
      },
      "source": [
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "input_name = model.input_names[0]\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_test[:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73aIbAOy6r00"
      },
      "source": [
        "This code creates a `PredictRequest` protocol buffer and fills in the required fields, including the model name (defined earlier), the signature name of the function we want to call, and finally the input data, in the form of a `Tensor` protocol buffer. The `tf.make_tensor_proto()` function creates a Tensor protocol buffer based on the given tensor or NumPy array, in this case `X_new`.\n",
        "\n",
        "Next, we’ll send the request to the server and get its response (for this we will need the `grpcio` library, which we can install using pip):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXYESbC768Az"
      },
      "source": [
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "\n",
        "channel = grpc.insecure_channel('localhost:8500')\n",
        "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "response = predict_service.Predict(request, timeout=10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUFkHW4Bttri",
        "outputId": "88ed1975-90b6-4d70-ff61-f189f95b3b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "response"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "outputs {\n",
              "  key: \"Softmax\"\n",
              "  value {\n",
              "    dtype: DT_FLOAT\n",
              "    tensor_shape {\n",
              "      dim {\n",
              "        size: 3\n",
              "      }\n",
              "      dim {\n",
              "        size: 10\n",
              "      }\n",
              "    }\n",
              "    float_val: 1.0176953765039798e-05\n",
              "    float_val: 4.316563495532222e-10\n",
              "    float_val: 4.436568633536808e-05\n",
              "    float_val: 0.0017675909912213683\n",
              "    float_val: 2.764317912351544e-07\n",
              "    float_val: 2.7024723749491386e-05\n",
              "    float_val: 3.617923183263372e-10\n",
              "    float_val: 0.9978255033493042\n",
              "    float_val: 3.565489805623656e-06\n",
              "    float_val: 0.0003214747121091932\n",
              "    float_val: 0.0011934504145756364\n",
              "    float_val: 3.597992690629326e-05\n",
              "    float_val: 0.9352307319641113\n",
              "    float_val: 0.007069199811667204\n",
              "    float_val: 1.8756937669728302e-10\n",
              "    float_val: 0.039686378091573715\n",
              "    float_val: 0.014937933534383774\n",
              "    float_val: 1.4017121339506e-13\n",
              "    float_val: 0.0018462629523128271\n",
              "    float_val: 8.416913832576256e-10\n",
              "    float_val: 1.43477182064089e-05\n",
              "    float_val: 0.9812043309211731\n",
              "    float_val: 0.009082535281777382\n",
              "    float_val: 0.0010774277616292238\n",
              "    float_val: 0.0007308772183023393\n",
              "    float_val: 0.00011349941632943228\n",
              "    float_val: 0.0017914923373609781\n",
              "    float_val: 0.003041493706405163\n",
              "    float_val: 0.0027217271272093058\n",
              "    float_val: 0.0002223183837486431\n",
              "  }\n",
              "}\n",
              "model_spec {\n",
              "  name: \"my_mnist_model\"\n",
              "  version {\n",
              "    value: 1\n",
              "  }\n",
              "  signature_name: \"serving_default\"\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ddioU56-l1"
      },
      "source": [
        "The code is quite straightforward: after the imports, we create a gRPC communication channel to *localhost* on TCP port 8500, then we create a gRPC service over this channel and use it to send a request, with a 10-second timeout (not that the call is synchronous: it will block until it receives the response or the timeout period expires). In this example the channel is insecure (no encryption, no authentication), but gRPC and TensorFlow Serving also support secure channels over SSL/TLS.\n",
        "\n",
        "Next, let’s convert the PredictResponse protocol buffer to a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBh6-Q7A7EEu"
      },
      "source": [
        "output_name = model.output_names[0]\n",
        "outputs_proto = response.outputs[output_name]\n",
        "y_proba = tf.make_ndarray(outputs_proto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZU9BOmtwkX_"
      },
      "source": [
        "If we run this code and print `y_proba.numpy().round(2)`, we will get the exact same estimated class probabilities as earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK0ETA17wi-P",
        "outputId": "9ac167cc-c045-4ec2-9651-74b717d9e9c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.94, 0.01, 0.  , 0.04, 0.01, 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QamdUOSb22f_",
        "outputId": "a2a25547-ab5f-43c5-85a8-178ff85c89d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "show(0, 'The model thought this was a {}, and it was actually a {}'.format(\n",
        "    np.argmax(y_proba[0]), y_test[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkJSbTD87Fso"
      },
      "source": [
        " And that’s all there is to it: in just a few lines of code, we can now access our TensorFlow model remotely, using either REST or gRPC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX_OdbyZ7OxM"
      },
      "source": [
        "### Deploying a new model version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pOjI9If7Pnd"
      },
      "source": [
        "Now let’s create a new model version and export a SavedModel to the `my_mnist_model/0002` directory, just like earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifK5acj5w2_4",
        "outputId": "0d62f999-83a2-4c72-f3fa-2f4eb38313b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
        "                      strides=2, activation='relu', name='Conv1'),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
        "])\n",
        "\n",
        "# train and evaluate our model\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('\\nTest accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3727 - accuracy: 0.8949 - val_loss: 0.2065 - val_accuracy: 0.9412\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1784 - accuracy: 0.9485 - val_loss: 0.1466 - val_accuracy: 0.9568\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1348 - accuracy: 0.9614 - val_loss: 0.1225 - val_accuracy: 0.9644\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1128 - accuracy: 0.9666 - val_loss: 0.1048 - val_accuracy: 0.9680\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0987 - accuracy: 0.9704 - val_loss: 0.1038 - val_accuracy: 0.9700\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0901 - accuracy: 0.9725 - val_loss: 0.1011 - val_accuracy: 0.9714\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0826 - accuracy: 0.9748 - val_loss: 0.0984 - val_accuracy: 0.9726\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0760 - accuracy: 0.9765 - val_loss: 0.0949 - val_accuracy: 0.9720\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0718 - accuracy: 0.9782 - val_loss: 0.0950 - val_accuracy: 0.9728\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0670 - accuracy: 0.9794 - val_loss: 0.0942 - val_accuracy: 0.9724\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0908 - accuracy: 0.9723\n",
            "\n",
            "Test accuracy: 0.9722999930381775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_q9FKR17Tfx",
        "outputId": "fd07a632-27d6-4ade-ddb5-0dd00a227e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_version = \"0002\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "print('Path where the model will be stored: {}\\n'.format(model_path))\n",
        "tf.saved_model.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path where the model will be stored: my_mnist_model/0002\n",
            "\n",
            "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evvprltVxeXF",
        "outputId": "46a452b7-2a4a-4aff-e722-2bcf96fd0bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for root, dirs, files in os.walk(model_name):\n",
        "    indent = '    ' * root.count(os.sep)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    for filename in files:\n",
        "        print('{}{}'.format(indent + '    ', filename))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_mnist_model/\n",
            "    0002/\n",
            "        saved_model.pb\n",
            "        assets/\n",
            "        variables/\n",
            "            variables.data-00000-of-00001\n",
            "            variables.index\n",
            "    0001/\n",
            "        saved_model.pb\n",
            "        assets/\n",
            "        variables/\n",
            "            variables.data-00000-of-00001\n",
            "            variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InjvDtDs7XNL"
      },
      "source": [
        "At regular intervals (the delay is configurable), TensorFlow Serving checks for new model versions. If it finds one, it will automatically handle the transition gracefully: by default, it will answer pending requests (if any) with the previous model version, while handling new requests with the new version. As soon as every pending request has been answered, the previous model version is unloaded. \n",
        "\n",
        "**Note**: We may need to wait a minute before the new model is loaded by TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUuLDk3dxqKx"
      },
      "source": [
        "import requests\n",
        "\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
        "            \n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status()\n",
        "response = response.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnPJ7TQCxwmX",
        "outputId": "ab3a70b1-6346-443f-9a6a-81de96356061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "response.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['predictions'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "picFIesAxxxA",
        "outputId": "037ce983-319e-46cc-899e-100cdc3e929e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_proba = np.array(response[\"predictions\"])\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoaAzWatEngl",
        "outputId": "b97f8688-ec7a-4219-c318-ba7805df0a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "show(0, 'The model thought this was a {}, and it was actually a {}'.format(\n",
        "    np.argmax(y_proba[0]), y_test[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc6aGies7fRT"
      },
      "source": [
        "This approach offers a smooth transition, but it may use too much RAM (especially GPU RAM, which is generally the most limited). In this case, we can configure TF Serving so that it handles all pending requests with the previous model version and unloads it before loading and using the new model version. This configuration will avoid having two model versions loaded at the same time, but the service will be unavailable for a short period.\n",
        "\n",
        "As we can see, TF Serving makes it quite simple to deploy new models. Moreover, if we discover that version 2 does not work as well as we expected, then rolling back to version 1 is as simple as removing the `my_mnist_model/0002` directory.\n",
        "\n",
        "**Note**: Another great feature of TF Serving is its automatic batching capability, which we can activate using the `--enable_batching` option upon startup. When TF Serving receives multiple requests within a short period of time (the delay is configurable), it will automatically batch them together before using the model. This offers a significant performance boost by leveraging the power of the GPU. Once the model returns the predictions, TF Serving dispatches each prediction to the right client. We can trade a bit of latency for a greater throughput by increasing the batching delay (see the `--batching_parameters_file` option)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYYzxpMCyStK"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQxJMTiWyUPz"
      },
      "source": [
        "- [Train and serve a TensorFlow model with TensorFlow Serving](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple)\n",
        "\n",
        "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
        "\n",
        "- [Github: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://github.com/ageron/handson-ml2)\n",
        "\n",
        "- [Ease ML deployments with TensorFlow Serving](https://youtu.be/4mqFDwIdKh0)\n",
        "\n",
        "- [Building Machine Learning Pipelines](https://learning.oreilly.com/library/view/building-machine-learning/9781492053187/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmzRPvUTg1vW"
      },
      "source": [
        "file:///home/lenovo/Documents/docs/deep%20learning/hands_on_ML_htmls/models_at_scale.html\n",
        "\n",
        "https://github.com/ageron/handson-ml2/blob/master/19_training_and_deploying_at_scale.ipynb\n"
      ]
    }
  ]
}